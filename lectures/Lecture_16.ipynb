{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2d74a5-dcd7-4465-88e3-f26f9087bb55",
   "metadata": {},
   "source": [
    "# Lecture 16 \n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "We begin our formal study of reinforcement learning (RL) by examining the mathematical structure of **memory-less sequential decision algorithms**, which correspond to the class of **Markov Decision Processes (MDPs)** with **stationary policies** and **no explicit memory of past states or actions**. These systems provide a mathematically tractable and conceptually foundational framework upon which more general reinforcement learning methods are built.\n",
    "\n",
    "Let us define the concept of a *Markov Decision Process*. A Markov Decision Process is a 5-tuple\n",
    "\n",
    "$$\n",
    "\\mathcal{M} = (S, A, P, R, \\gamma),\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $S$ is a finite set of **states**.\n",
    "* $A$ is a finite set of **actions**.\n",
    "* $P: S \\times A \\times S \\to [0,1]$ is the **state transition probability function**, so that for $s, s' \\in S$ and $a \\in A$, we write\n",
    "\n",
    "  $$\n",
    "  P(s' \\mid s, a) = \\mathbb{P}(s_{t+1} = s' \\mid s_t = s, a_t = a).\n",
    "  $$\n",
    "* $R: S \\times A \\to \\mathbb{R}$ is the **reward function**, where $R(s,a)$ denotes the expected immediate reward received when action $a$ is taken in state $s$.\n",
    "* $\\gamma \\in [0,1)$ is the **discount factor**, which determines the present value of future rewards and is used to guarantee convergence of infinite-horizon reward sums.\n",
    "\n",
    "The *Markov* property asserts that the transition probability distribution of the next state depends only on the current state and action, and not on the sequence of events that preceded it. Formally, for all $t \\in \\mathbb{N}$,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(s_{t+1} = s' \\mid s_0, a_0, s_1, a_1, \\dots, s_t = s, a_t = a) = \\mathbb{P}(s_{t+1} = s' \\mid s_t = s, a_t = a).\n",
    "$$\n",
    "\n",
    "A **policy** is a rule that determines the agent’s behavior. In the memory-less (also called *stationary*) case, a policy is a function\n",
    "\n",
    "$$\n",
    "\\pi: S \\to A\n",
    "$$\n",
    "\n",
    "that maps each state deterministically to an action. More generally, one can consider stochastic policies, which are functions $\\pi: S \\times A \\to [0,1]$ satisfying $\\sum_{a \\in A} \\pi(a \\mid s) = 1$ for all $s \\in S$.\n",
    "\n",
    "The objective in this setting is to identify a policy $\\pi$ that maximizes the expected cumulative discounted reward over an infinite time horizon. That is, for a given initial state $s_0$, we define the **value function** of a policy $\\pi$ by\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\mathbb{E}_\\pi\\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, \\pi(s_t)) \\mid s_0 = s \\right],\n",
    "$$\n",
    "\n",
    "where the expectation is taken over trajectories $(s_0, s_1, s_2, \\dots)$ generated by the transition probabilities $P$ under the policy $\\pi$. In turn, we define the **optimal value function** as\n",
    "\n",
    "$$\n",
    "V^*(s) = \\sup_\\pi V^\\pi(s),\n",
    "$$\n",
    "\n",
    "and a policy $\\pi^*$ is said to be *optimal* if $V^{\\pi^*}(s) = V^*(s)$ for all $s \\in S$. The foundational recursive characterization of the value function is given by the **Bellman equation**. For a fixed policy $\\pi$, the Bellman equation reads:\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = R(s, \\pi(s)) + \\gamma \\sum_{s' \\in S} P(s' \\mid s, \\pi(s)) V^\\pi(s').\n",
    "$$\n",
    "\n",
    "This is a system of $|S|$ linear equations in the unknowns ${V^\\pi(s)}_{s \\in S}$ and has a unique solution under the assumption $\\gamma < 1$.  The optimal value function $V^*$ satisfies the **Bellman optimality equation**:\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_{a \\in A} \\left[ R(s,a) + \\gamma \\sum_{s' \\in S} P(s' \\mid s, a) V^*(s') \\right].\n",
    "$$\n",
    "\n",
    "This equation is nonlinear due to the $\\max$ operator, and its solution characterizes the optimal policy by:\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_{a \\in A} \\left[ R(s,a) + \\gamma \\sum_{s' \\in S} P(s' \\mid s, a) V^*(s') \\right].\n",
    "$$\n",
    "\n",
    "The study of memory-less policies in this context is not merely pedagogical. Under mild regularity conditions, it is known that **there exists an optimal deterministic stationary policy**, even when the state and action spaces are finite but large. This result is nontrivial and arises from the structure of the Bellman optimality operator as a contraction mapping in the sup-norm, a property which allows one to employ **value iteration** or **policy iteration** algorithms to converge to the optimal policy.\n",
    "\n",
    "This setting forms the foundation of much of modern reinforcement learning, even when explicit models of the transition probabilities and rewards are not available, leading to approximate and sample-based algorithms. Before moving on to such methods, it is essential to understand the exact, model-based solution strategies in the fully known MDP setting.\n",
    "\n",
    "\n",
    "### An Example: A Three-State Grid World\n",
    "\n",
    "Let us define a toy environment consisting of three states arranged linearly:\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\{s_1, s_2, s_3\\},\n",
    "$$\n",
    "\n",
    "with a single agent moving left or right between them. The possible actions are\n",
    "\n",
    "$$\n",
    "\\mathcal{A} = \\{\\text{Left}, \\text{Right}\\}.\n",
    "$$\n",
    "\n",
    "We assume the agent begins in state $s_2$. The environment behaves according to the following deterministic transition rules:\n",
    "\n",
    "* From $s_1$, the action *Left* keeps the agent in $s_1$, and *Right* moves it to $s_2$.\n",
    "* From $s_2$, *Left* moves the agent to $s_1$, and *Right* moves it to $s_3$.\n",
    "* From $s_3$, *Right* keeps the agent in $s_3$, and *Left* moves it to $s_2$.\n",
    "\n",
    "This defines a deterministic transition function $P : \\mathcal{S} \\times \\mathcal{A} \\to \\mathcal{S}$.\n",
    "\n",
    "We define the reward function as follows:\n",
    "\n",
    "$$\n",
    "R(s, a) = \\begin{cases}\n",
    "+1 & \\text{if } s = s_3 \\text{ and } a = \\text{Right}, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "That is, the only positive reward is obtained by choosing *Right* in $s_3$, even though this action does not change the state. All other actions yield zero reward.\n",
    "\n",
    "![A graphical representation of the MDP](../images/markov-decision.png)\n",
    "\n",
    "The goal of the agent is to find an optimal policy $\\pi^\\ast$ that maximizes the expected cumulative reward. Let us assume a discount factor $\\gamma = 0.9$. Let us fix a specific policy:\n",
    "\n",
    "$$\n",
    "\\pi(s_1) = \\text{Right},\\quad \\pi(s_2) = \\text{Right},\\quad \\pi(s_3) = \\text{Right}.\n",
    "$$\n",
    "\n",
    "That is, the agent always moves to the right if possible. Under this policy, the state transitions are:\n",
    "\n",
    "* $s_1 \\xrightarrow{\\text{Right}} s_2$\n",
    "* $s_2 \\xrightarrow{\\text{Right}} s_3$\n",
    "* $s_3 \\xrightarrow{\\text{Right}} s_3$\n",
    "\n",
    "We now solve for $V^\\pi(s)$.\n",
    "\n",
    "#### Step 1: Bellman equations\n",
    "\n",
    "Let us denote:\n",
    "\n",
    "$$\n",
    "v_1 = V^\\pi(s_1),\\quad v_2 = V^\\pi(s_2),\\quad v_3 = V^\\pi(s_3).\n",
    "$$\n",
    "\n",
    "From the policy and transition rules:\n",
    "\n",
    "* $v_1 = R(s_1,\\text{Right}) + \\gamma V^\\pi(s_2) = 0 + \\gamma v_2$\n",
    "* $v_2 = R(s_2,\\text{Right}) + \\gamma V^\\pi(s_3) = 0 + \\gamma v_3$\n",
    "* $v_3 = R(s_3,\\text{Right}) + \\gamma V^\\pi(s_3) = 1 + \\gamma v_3$\n",
    "\n",
    "#### Step 2: Solve recursively\n",
    "\n",
    "From the third equation:\n",
    "\n",
    "$$\n",
    "v_3 = 1 + \\gamma v_3 \\Rightarrow v_3(1 - \\gamma) = 1 \\Rightarrow v_3 = \\frac{1}{1 - \\gamma} = \\frac{1}{0.1} = 10.\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "v_2 = \\gamma v_3 = 0.9 \\cdot 10 = 9, \\\\\n",
    "v_1 = \\gamma v_2 = 0.9 \\cdot 9 = 8.1.\n",
    "$$\n",
    "\n",
    "Thus, under this policy $\\pi$, the value function is:\n",
    "\n",
    "$$\n",
    "V^\\pi(s_1) = 8.1,\\quad V^\\pi(s_2) = 9,\\quad V^\\pi(s_3) = 10.\n",
    "$$\n",
    "\n",
    "This illustrates how the reward structure and transition dynamics induce value across the state space, even in states where no reward is directly received, due to their proximity to a rewarding state under the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85effa62-f5ed-4fd8-bc0e-8474bdc80342",
   "metadata": {},
   "source": [
    "### A Python Implementation for the Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3406aa-e13e-462a-adc4-7a463df4f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mdptoolbox\n",
    "from collections import deque, defaultdict\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30fb9c69-3dfb-4fec-bce2-84ca7b5b04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the states and actions\n",
    "states = ['s1', 's2', 's3']\n",
    "actions = ['Left', 'Right']\n",
    "\n",
    "# Define deterministic transition function P(s, a) -> s'\n",
    "def transition(state, action):\n",
    "    if state == 's1':\n",
    "        return 's1' if action == 'Left' else 's2'\n",
    "    elif state == 's2':\n",
    "        return 's1' if action == 'Left' else 's3'\n",
    "    elif state == 's3':\n",
    "        return 's2' if action == 'Left' else 's3'\n",
    "\n",
    "# Define reward function R(s, a)\n",
    "def reward(state, action):\n",
    "    return 1 if state == 's3' and action == 'Right' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "954c4aaf-af03-42ee-b3d1-bf834a0b6683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 133 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'s1': np.float64(8.1), 's2': np.float64(9.0), 's3': np.float64(10.0)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define discount factor\n",
    "gamma = 0.9  \n",
    "\n",
    "# Define the fixed policy π(s) = 'Right' for all s\n",
    "policy = {s: 'Right' for s in states}\n",
    "\n",
    "# Initialize value function\n",
    "V = {s: 0.0 for s in states}\n",
    "\n",
    "# Iterative policy evaluation\n",
    "tolerance = 1e-6\n",
    "delta = float('inf')\n",
    "iteration = 0\n",
    "\n",
    "while delta > tolerance:\n",
    "    delta = 0.0\n",
    "    new_V = V.copy()\n",
    "    for s in states:\n",
    "        a = policy[s]\n",
    "        s_prime = transition(s, a)\n",
    "        r = reward(s, a)\n",
    "        new_V[s] = r + gamma * V[s_prime]\n",
    "        delta = max(delta, abs(new_V[s] - V[s]))\n",
    "    V = new_V\n",
    "    iteration += 1\n",
    "\n",
    "# Print results\n",
    "print(f\"Converged after {iteration} iterations\")\n",
    "{k: np.round(v,3) for k,v in V.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b2932-7291-4077-ad37-76ac61534a53",
   "metadata": {},
   "source": [
    "### Optimality\n",
    "\n",
    "One may then ask whether this policy is optimal. In this case, the only positive reward is obtained by looping in $s_3$ under action *Right*. Since $s_3$ is absorbing under *Right*, and all rewards elsewhere are zero, any policy that leads to $s_3$ and remains there taking *Right* forever will be optimal.\n",
    "\n",
    "Hence, the above policy $\\pi$ is indeed optimal. The **optimal value function** $V^\\ast$ is therefore identical to the one we computed:\n",
    "\n",
    "$$\n",
    "V^*(s_1) = 8.1,\\quad V^*(s_2) = 9,\\quad V^*(s_3) = 10.\n",
    "$$\n",
    "\n",
    "This explicit example shows how a discrete deterministic MDP with a simple reward structure and transition model can be solved analytically, and how Bellman’s equations enable recursive computation of value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ea339-3402-4bb8-ab4d-90cc3e2bb9c3",
   "metadata": {},
   "source": [
    "## Another Example: One Dimensional Solo Game\n",
    "\n",
    "Let us do another a step-by-step solution of the **Bellman equation**. This time our game is a deterministic single-player game inspired by [Solo (a variant of peg solitaire)](https://en.wikipedia.org/wiki/Peg_solitaire), but set in a **1D array of 10 bins**. We will treat this as a deterministic episodic environment terminating when no valid moves remain. The game’s state space will be a subset of ${0,1}^{10}$, encoding whether a bin is filled ($1$) or empty ($0$).\n",
    "\n",
    "### Step 1: Define the Game Environment\n",
    "\n",
    "We define the game as a deterministic, acyclic, single-agent transition system on binary strings of length $n = 10$, representing the **occupancy status** of 10 contiguous bins.\n",
    "\n",
    "Let the state space be\n",
    "\n",
    "$$\n",
    "\\mathcal{S} \\subseteq \\{0,1\\}^{10},\n",
    "$$\n",
    "\n",
    "where a state $s = (s_0, \\dots, s_9) \\in \\mathcal{S}$ represents the presence ($s_i = 1$) or absence ($s_i = 0$) of a peg at bin $i$.\n",
    "\n",
    "#### Initial State\n",
    "\n",
    "We define the **initial state** as:\n",
    "\n",
    "$$\n",
    "s^{(0)} = (1,1,1,1,0,1,1,1,1,1),\n",
    "$$\n",
    "\n",
    "i.e., all bins are filled except the middle bin (index 4).\n",
    "\n",
    "#### Legal Moves\n",
    "\n",
    "The allowed **moves** correspond to standard **peg solitaire jumps**:\n",
    "\n",
    "1. **Rightward jump:** If there exists an index $i$ such that $(s_i, s_{i+1}, s_{i+2}) = (1,1,0)$, then a jump from $i$ over $i+1$ to $i+2$ results in:\n",
    "\n",
    "   $$\n",
    "   s_i \\gets 0,\\quad s_{i+1} \\gets 0,\\quad s_{i+2} \\gets 1\n",
    "   $$\n",
    "\n",
    "2. **Leftward jump:** If $(s_i, s_{i+1}, s_{i+2}) = (0,1,1)$, then a jump from $i+2$ over $i+1$ to $i$ results in:\n",
    "\n",
    "   $$\n",
    "   s_i \\gets 1,\\quad s_{i+1} \\gets 0,\\quad s_{i+2} \\gets 0\n",
    "   $$\n",
    "\n",
    "Each legal move produces a new deterministic state $s' \\in \\mathcal{S}$. We assume that **only one jump is allowed per time step**.\n",
    "\n",
    "#### Terminal States\n",
    "\n",
    "A state $s \\in \\mathcal{S}$ is called **terminal** if **no valid jump is possible**:\n",
    "\n",
    "$$\n",
    "\\forall i \\in \\{0, \\dots, 7\\},\\quad (s_i, s_{i+1}, s_{i+2}) \\notin \\{(1,1,0), (0,1,1)\\}\n",
    "$$\n",
    "\n",
    "#### Reward Function\n",
    "\n",
    "We define a scalar reward function $R: \\mathcal{S} \\to \\mathbb{R}$ as $ R(s) = -\\sum_{i=0}^9 s_i$ if $s$ is a terminal state, and $0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459f3bc-e637-4492-abcf-6e2cc5cd6964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2562af79-5308-4ac9-9596-37a6556d28e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = Tuple[int, ...]\n",
    "\n",
    "def get_valid_moves(state: State) -> List[State]:\n",
    "    state_array = np.array(state)\n",
    "    n = len(state)\n",
    "    moves = []\n",
    "    # Rightward jumps: (1,1,0) → (0,0,1)\n",
    "    for i in range(n - 2):\n",
    "        if state[i] == 1 and state[i+1] == 1 and state[i+2] == 0:\n",
    "            new_state = state_array.copy()\n",
    "            new_state[i], new_state[i+1], new_state[i+2] = 0, 0, 1\n",
    "            moves.append(tuple(new_state))\n",
    "    # Leftward jumps: (0,1,1) → (1,0,0)\n",
    "    for i in range(n - 2):\n",
    "        if state[i] == 0 and state[i+1] == 1 and state[i+2] == 1:\n",
    "            new_state = state_array.copy()\n",
    "            new_state[i], new_state[i+1], new_state[i+2] = 1, 0, 0\n",
    "            moves.append(tuple(new_state))\n",
    "    return moves\n",
    "\n",
    "def is_terminal(state: State) -> bool:\n",
    "    return len(get_valid_moves(state)) == 0\n",
    "\n",
    "def reward(state: State) -> int:\n",
    "    if is_terminal(state):\n",
    "        return -sum(state)  # Fewer pegs → better\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def render(state: State) -> str:\n",
    "    return ''.join('●' if x else '○' for x in state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ef1d4a4-6cf6-49c2-a43c-da6499be8111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state:\n",
      "●●●●○●●●●●\n",
      "\n",
      "Valid moves from initial state:\n",
      "Move 1: ●●○○●●●●●●\n",
      "Move 2: ●●●●●○○●●●\n"
     ]
    }
   ],
   "source": [
    "initial_state: State = (1, 1, 1, 1, 0, 1, 1, 1, 1, 1)\n",
    "\n",
    "print(\"Initial state:\")\n",
    "print(render(initial_state))\n",
    "\n",
    "print(\"\\nValid moves from initial state:\")\n",
    "for i, s in enumerate(get_valid_moves(initial_state)):\n",
    "    print(f\"Move {i+1}: {render(s)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e935c25-728c-4e40-ae77-aed049141770",
   "metadata": {},
   "source": [
    "### Step 2. Build the DAG of the Game\n",
    "\n",
    "We now build the **transition graph** $\\mathcal{G} = (V, E)$ of all **reachable states** starting from the **initial state** by forward exploration of valid transitions. Since each state transition is deterministic and we assume one move per time step, the graph will be a **directed acyclic graph (DAG)** rooted at the initial state.\n",
    "\n",
    "The resulting graph will be a **dictionary** mapping each visited state to the list of its **legal successors** (reachable via one valid move), and a **set of all discovered states**, to avoid revisiting.  \n",
    "\n",
    "* We will use a **breadth-first search (BFS)** to guarantee exploration in increasing depth.\n",
    "* Each state will be represented as an immutable `tuple[int, ...]` for hashability.\n",
    "* The graph will be built incrementally: if a newly generated successor hasn’t been seen before, it is added to the exploration queue.\n",
    "* We will store for each state its **successor states** via valid jumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abe37189-bfac-420d-a0bf-6b26ae68daf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reachable states: 33\n",
      "Number of transitions: 45\n",
      "\n",
      "Sample transitions:\n",
      "●●●●○●●●●● →\n",
      "     ●●○○●●●●●●\n",
      "     ●●●●●○○●●●\n",
      "●●○○●●●●●● →\n",
      "     ○○●○●●●●●●\n",
      "     ●●○●○○●●●●\n",
      "●●●●●○○●●● →\n",
      "     ●●●○○●○●●●\n",
      "     ●●●●●○●○○●\n",
      "○○●○●●●●●● →\n",
      "     ○○●●○○●●●●\n",
      "●●○●○○●●●● →\n",
      "     ○○●●○○●●●●\n",
      "     ●●○●○●○○●●\n",
      "●●●○○●○●●● →\n",
      "     ●○○●○●○●●●\n",
      "     ●●●○○●●○○●\n",
      "●●●●●○●○○● →\n",
      "     ●●●○○●●○○●\n",
      "○○●●○○●●●● →\n",
      "     ○○○○●○●●●●\n",
      "     ○●○○○○●●●●\n",
      "     ○○●●○●○○●●\n",
      "●●○●○●○○●● →\n",
      "     ○○●●○●○○●●\n",
      "     ●●○●○●○●○○\n"
     ]
    }
   ],
   "source": [
    "transition_graph = defaultdict(list)\n",
    "\n",
    "# Set of all discovered states (to prevent revisiting)\n",
    "discovered_states = set()\n",
    "discovered_states.add(initial_state)\n",
    "\n",
    "# Queue for breadth-first traversal\n",
    "queue = deque()\n",
    "queue.append(initial_state)\n",
    "\n",
    "# Build the transition graph by exploring all reachable configurations\n",
    "while queue:\n",
    "    current_state = queue.popleft()\n",
    "    successors = get_valid_moves(current_state)\n",
    "    for next_state in successors:\n",
    "        transition_graph[current_state].append(next_state)\n",
    "        if next_state not in discovered_states:\n",
    "            discovered_states.add(next_state)\n",
    "            queue.append(next_state)\n",
    "\n",
    "print(f\"Number of reachable states: {len(discovered_states)}\")\n",
    "print(f\"Number of transitions: {sum(len(v) for v in transition_graph.values())}\")\n",
    "\n",
    "print(\"\\nSample transitions:\")\n",
    "for s in list(transition_graph.keys())[:9]:\n",
    "    print(f\"{render(s)} →\")\n",
    "    for t in transition_graph[s]:\n",
    "        print(f\"     {render(t)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f51b3-d3ee-4f10-be26-ce5c898dedc8",
   "metadata": {},
   "source": [
    "### Step 3. Identify Terminal States\n",
    "\n",
    "We now proceed to identify the **terminal states** in the transition graph $\\mathcal{G} = (V, E)$ that we constructed by forward exploration from the initial state. Recall that a **terminal state** is one with **no valid moves**, i.e., no $(i, i+1, i+2)$ subsequence of form $(1,1,0)$ or $(0,1,1)$. Equivalently, in our transition graph, a terminal state is one with **no outgoing edges**, i.e., it has no successors in the dictionary `transition_graph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae65ff0c-343c-4b0c-945b-a26d6ab2188b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terminal states: 7\n",
      "1: ●○●○○○○○○●  (Remaining pegs: 3)\n",
      "2: ○●○○○●○●○○  (Remaining pegs: 3)\n",
      "3: ○○○○○○○○●○  (Remaining pegs: 1)\n",
      "4: ○○○○○●○○○○  (Remaining pegs: 1)\n",
      "5: ●○○●○○○●○●  (Remaining pegs: 4)\n",
      "6: ○○○●○○○●○○  (Remaining pegs: 2)\n",
      "7: ●○○○○●○○○●  (Remaining pegs: 3)\n"
     ]
    }
   ],
   "source": [
    "# Extract terminal states: no successors in the graph\n",
    "terminal_states = {state for state in discovered_states if len(transition_graph[state]) == 0}\n",
    "\n",
    "print(f\"Number of terminal states: {len(terminal_states)}\")\n",
    "for i, state in enumerate(list(terminal_states)):\n",
    "    print(f\"{i+1}: {render(state)}  (Remaining pegs: {sum(state)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa407932-3803-4031-869d-9d4c09707e5f",
   "metadata": {},
   "source": [
    "The set `terminal_states` is a subset of `discovered_states` with out-degree zero. We can also precompute the **reward** for each terminal state, since it is the number of remaning pegs. We also verify which terminal states have the minimum number of pegs, i.e., the maximum reward (since our reward is negative peg count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a784fd8-20c4-4e0d-a99e-3e9c0197d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal terminal value: -1 (i.e., 1 pegs left)\n"
     ]
    }
   ],
   "source": [
    "# Reward function on terminal states\n",
    "rewards = {state: reward(state) for state in terminal_states}\n",
    "\n",
    "min_pegs = min(sum(s) for s in terminal_states)\n",
    "optimal_value = -min_pegs\n",
    "\n",
    "print(f\"\\nOptimal terminal value: {optimal_value} (i.e., {min_pegs} pegs left)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d143595-6bd6-49c8-b61c-bcc7025e9958",
   "metadata": {},
   "source": [
    "### Step 4. Solve the Bellman Equation \n",
    "\n",
    "We now proceed to implement **value iteration** to solve the **Bellman equation** over the **DAG** defined by our transition graph. Since the environment is deterministic and acyclic, and rewards are concentrated at terminal states, the value function admits a particularly elegant structure.\n",
    "\n",
    "For a general Markov Decision Process (MDP), the value function satisfies the Bellman equation:\n",
    "\n",
    "$$\n",
    "V(s) = \\max_{a \\in A(s)} \\left[ R(s,a) + \\sum_{s'} P(s' \\mid s,a) V(s') \\right].\n",
    "$$\n",
    "\n",
    "In our setting, the environment is:\n",
    "\n",
    "* **Deterministic**: $P(s' \\mid s,a) = 1$ for some $s'$.\n",
    "* **Single-action per state**: no explicit action set; the set of available \"moves\" from a state $s$ is encoded in the graph edges.\n",
    "* **Reward only at terminal states**: $R(s) = \\sum_i s_i$ if $s$ is terminal,  and $R(s) = 0$ otherwise.\n",
    "* **Transitions carry zero reward**: we can think of the \"reward per move\" as $r(s,s') = 0$.\n",
    "\n",
    "Hence, the Bellman equation simplifies to:\n",
    "\n",
    "$$\n",
    "V(s) =\n",
    "\\begin{cases}\n",
    "-\\sum_i s_i, & \\text{if } s \\text{ is terminal}, \\\\\n",
    "\\max_{s' \\in \\mathrm{Succ}(s)} V(s'), & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This is a **pure max-propagation** dynamic program: each state's value is determined by selecting the *best-valued* successor. Since the graph is a DAG, we can sort the states topologically, and propagate values backwards in a single pass. Hence, value iteration becomes equivalent to backward dynamic programming over a DAG with maximization at each node and no additive reward cost. This setup is an instance of deterministic shortest-path problems on acyclic graphs—except we are maximizing terminal reward instead of minimizing cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbfa84ec-f4a7-4402-ad07-54822c21ab08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute in-degree for each node\n",
    "in_degree = defaultdict(int)\n",
    "for s in transition_graph:\n",
    "    for s_next in transition_graph[s]:\n",
    "        in_degree[s_next] += 1\n",
    "\n",
    "# Start with nodes that have no incoming edges\n",
    "queue = deque([s for s in discovered_states if in_degree[s] == 0])\n",
    "topo_order = []\n",
    "\n",
    "while queue:\n",
    "    s = queue.popleft()\n",
    "    topo_order.append(s)\n",
    "    for s_next in transition_graph.get(s, []):\n",
    "        in_degree[s_next] -= 1\n",
    "        if in_degree[s_next] == 0:\n",
    "            queue.append(s_next)\n",
    "\n",
    "# We want reverse topological order for dynamic programming\n",
    "topo_order.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "387d2614-2274-404e-8e88-2fd171fdcf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize value function V and policy π\n",
    "V = {}\n",
    "policy = {}\n",
    "\n",
    "for s in discovered_states:\n",
    "    if s in terminal_states:\n",
    "        V[s] = rewards[s]\n",
    "        policy[s] = None\n",
    "    else:\n",
    "        V[s] = float('-inf')\n",
    "        policy[s] = None\n",
    "\n",
    "# Value iteration in reverse topological order\n",
    "for s in topo_order:\n",
    "    successors = transition_graph.get(s, [])\n",
    "    if successors:\n",
    "        best_s = max(successors, key=lambda s_next: V[s_next])\n",
    "        V[s] = V[best_s]\n",
    "        policy[s] = best_s  # store optimal successor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447df70f-21f6-4557-ae29-17a4b2633293",
   "metadata": {},
   "source": [
    "### Step 5. Calculate Optimal Trajectory\n",
    "\n",
    "We now reconstruct and render the optimal trajectory from the initial state using the stored policy map $\\pi : \\mathcal{S} \\to \\mathcal{S}$ computed during value iteration.\n",
    "\n",
    "At each state $s$, the policy tells us the best successor $s' = \\pi(s)$ to move to. We terminate the reconstruction when we reach a terminal state, i.e., when $\\pi(s) = \\text{None}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65017f07-1d1a-45a0-accb-60db733044a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(initial_state):\n",
    "    \"\"\"Reconstruct the optimal trajectory using the policy map\"\"\"\n",
    "    trajectory = [initial_state]\n",
    "    current_state = initial_state\n",
    "\n",
    "    while policy[current_state] is not None:\n",
    "        next_state = policy[current_state]\n",
    "        trajectory.append(next_state)\n",
    "        current_state = next_state\n",
    "    \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7110cc85-6839-4da4-85b9-7829d452d0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal trajectory from random initial state:\n",
      "\n",
      "Step  0: ○○●●○●○○●●   (Value: -1)\n",
      "Step  1: ○○○○●●○○●●   (Value: -1)\n",
      "Step  2: ○○○○○○●○●●   (Value: -1)\n",
      "Step  3: ○○○○○○●●○○   (Value: -1)\n",
      "Step  4: ○○○○○○○○●○   (Value: -1)\n"
     ]
    }
   ],
   "source": [
    "# Visual rendering of the trajectory\n",
    "print(\"Optimal trajectory from random initial state:\\n\")\n",
    "states = list(transition_graph.keys())\n",
    "\n",
    "N = len(states)\n",
    "trajectory = get_trajectory(states[np.random.randint(N)])\n",
    "for i, state in enumerate(trajectory):\n",
    "    print(f\"Step {i:2d}: {render(state)}   (Value: {V[state]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d86120d-1191-4056-8480-8ee0a62b2166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
