{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 09:55:52.563260: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras as ks\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation   \n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "from keras.layers import Input, Dense, Conv2D, Dropout, Flatten, MaxPooling2D, LSTM, Embedding\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,  classification_report\n",
    "from sklearn.datasets import load_iris, load_digits, fetch_20newsgroups_vectorized, fetch_olivetti_faces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10 (Neural Networks)\n",
    "\n",
    "## The Gradient Descent\n",
    "\n",
    "Consider the following problem: we have a multivariable function $F(x_1,\\ldots,x_n)$ that we want to optimize, i.e. find the point at which $F$ attains its minimum or maximum. There is an iterative algorithm called [steepest descent algorithm](https://ocw.mit.edu/courses/mathematics/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/lecture-notes/MIT18_409F09_scribe21.pdf) that we can use to find this point. The algorithm uses the [gradient](https://inst.eecs.berkeley.edu/~ee127/sp21/livebook/def_gradient.html) of the function. Recall that the gradient $\\nabla F$ at a point $x$ \n",
    "\n",
    "$$ \\nabla F = \\left(\\frac{\\partial F}{\\partial x_1},\\ldots,\\frac{\\partial F}{\\partial x_n}\\right) $$\n",
    "\n",
    "gives us the direction at which $F$ has the largest (in absolute value) derivative. The algorithm uses this information and iteratively pushes an initial guess into better and better approximations of the optimum point. Let us start with an initial guess $x^{(0)} = (x_1^{(0)},\\ldots,x_n^{(0)})$ for a minimum of $F$, and move in the opposite direction of the gradient with a small step (called **learning rate**). Then the update rule for the path we are going to follow is\n",
    "\n",
    "$$ x^{(m+1)} = x^{(m)} - \\eta \\left(\\nabla F\\right)(x_1^{(m)},\\ldots,x_n^{(m)}) $$\n",
    "\n",
    "where $\\eta$ is called *the learning rate*. \n",
    "\n",
    "Now, let try to find a solution to $ f(x) = c $ using gradient descent. We convert the root-finding problem $f(x) = c$ into an optimization problem by minimizing the squared error $(f(x) - c)^2$. This approach is particularly effective when $f$ is differentiable but not invertible. Then we need the following function is minimized at $f(x)=c$\n",
    "\n",
    "$$ F(x) = \\frac{1}{2} (f(x) - c)^2 \\quad \\Rightarrow \\quad \\nabla F(x) = (f(x)-c) \\nabla f(x) $$\n",
    "\n",
    "The gradient $\\nabla F(x)$ points in the direction of steepest increase of the function $F$. Therefore, to minimize $F$, we move in the direction $-\\nabla F(x)$. So this update rule is:\n",
    "\n",
    "$$ x^{(k+1)} = x^{(k)} - \\eta (f(x^{(k)}) - c) \\nabla f(x^{(k)}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSolve(f, c, x0, lr=0.01, h=1e-5, tol=1e-5, n=15000):\n",
    "    x0 = np.array(x0, dtype=float)\n",
    "    dim = len(x0)\n",
    "    \n",
    "    def numerical_grad(f, x):\n",
    "        grad = np.zeros_like(x)\n",
    "        for i in range(len(x)):\n",
    "            dx = np.zeros_like(x)\n",
    "            dx[i] = h\n",
    "            grad[i] = (f(x + dx) - f(x - dx)) / (2 * h)\n",
    "        return grad\n",
    "\n",
    "    for i in range(n):\n",
    "        fx = f(x0)\n",
    "        grad_f = numerical_grad(f, x0)\n",
    "        x1 = x0 - lr * (fx - c) * grad_f\n",
    "        if np.linalg.norm(x1 - x0) < tol:\n",
    "            break\n",
    "        x0 = x1\n",
    "    return [i, x1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us solve a specific example. Let $f(x,y) = x^2 + 3y^2$, let $c=0$, let initial point $(x_0,y_0)=(2,2)$ and let us set the learning rate at 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3952, array([7.93556903e-02, 1.49809119e-05])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x[0]**2 + 3 * x[1]**2\n",
    "\n",
    "MSolve(f, c=0.0, x0=[2.0, 2.0], lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started at the point $(2, 2)$ with the learning rate at set 0.01.  The red dots show the successive steps taken by the algorithm as it descends toward the minimum at the origin $(0, 0)$. This function is **convex**. The descent follows an **elliptical path**, with rapid convergence in $y$ and slower motion in $x$. The gradient is $ \\nabla f(x, y) = (2x, 6y) $. So, the updates are:\n",
    "\n",
    "$$ x^{(n+1)} = x^{(n)} - 0.01 \\cdot 2x^{(n)},\\quad y^{(n+1)} = y^{(n)} - 0.01 \\cdot 6y^{(n)} $$\n",
    "\n",
    "<center><img width=\"500px\" src=\"../images/steepest_descent.png\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "\n",
    "Perceptrons are the main building blocks of artificial neural networks. They are designed to solve binary classification problems. They take a collection of input values $x = (x_1,\\ldots,x_n)$ apply a linear combination \n",
    "\n",
    "$$\\alpha\\cdot x + \\beta = a_1 x_1 + \\cdots + a_n x_n + \\beta$$ \n",
    "\n",
    "using a collection of weights $\\alpha = (a_0,\\ldots,a_n)$ and $\\beta$ to be determined via an iterative approach. Then we apply an activation function $\\varphi(x)$ to get an output which is either 0 or 1.\n",
    "\n",
    "<center><img src=\"../images/perceptron.gif\"></center>\n",
    "\n",
    "[Source: Multilayer perceptrons from \"Nonlinear Switching State-Space Models\" by Antti Honkela](https://users.ics.aalto.fi/ahonkela/dippa/node41.html)\n",
    "\n",
    "### The Optimization Problem\n",
    "\n",
    "As in the case of logistic regression, we have a collection of data points $(x^{(i)},y^{(i)})$ that we assume satisfy a relationship of the form\n",
    "\n",
    "$$ y^{(i)} - \\varphi(\\alpha\\cdot x^{(i)} + \\beta) \\sim N(0,\\sigma) $$\n",
    "\n",
    "where $\\varphi\\colon\\mathbb{R}\\to\\mathbb{R}$ is a real valued function of a single variable, $\\alpha$ and $x^{(i)}$ are vectors in an inner product space and $\\beta$ is a scalar.  Our task is to find the best fitting pair $(\\alpha,\\beta)$ such that \n",
    "\n",
    "$$ \\sum_i (y^{(i)} - \\varphi(\\alpha\\cdot x^{(i)} + \\beta))^2 $$\n",
    "\n",
    "is minimized. This setup is a slight generalization of the [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) algorithm we covered in earlier lectures.  In the logistic regression case $\\varphi(x) = \\frac{1}{1+e^{-x}}$.  Now, we can change $\\varphi$. Here are a couple of options:\n",
    "\n",
    "- Linear: $\\varphi(z) = z$ (gives **linear regression**),\n",
    "- Sigmoid: $\\varphi(z) = \\frac{1}{1 + e^{-z}}$ (logistic regression if combined with cross-entropy loss),\n",
    "- ReLU: $\\varphi(z) = \\max(0,z)$,\n",
    "- Tanh: $\\varphi(z) = \\tanh(z)$, etc.\n",
    "\n",
    "### Minimizing MSE is Equivalent to MLE\n",
    "\n",
    "Under the Gaussian noise assumption minimizing the MSE is equivalent to finding the maximum likelihood estimation. This is because when we write the likelihood function\n",
    "\n",
    "$$\n",
    "p(y^{(i)} \\mid x^{(i)}; \\alpha, \\beta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{1}{2\\sigma^2} (y^{(i)} - \\varphi(\\alpha \\cdot x^{(i)} + \\beta))^2 \\right)\n",
    "$$\n",
    "\n",
    "We obtain the log-likelihood over all samples as\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\alpha, \\beta) = -\\frac{N}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y^{(i)} - \\varphi(\\alpha \\cdot x^{(i)} + \\beta))^2\n",
    "$$\n",
    "\n",
    "Maximizing this is equivalent to minimizing the following function\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N (y^{(i)} - \\varphi(\\alpha \\cdot x^{(i)} + \\beta))^2\n",
    "$$\n",
    "\n",
    "which justifies the choice of the squared loss.\n",
    "\n",
    "\n",
    "### The Gradient Descent Update Rule\n",
    "\n",
    "We are going to use a gradient descent method update to minimize $L(\\alpha, \\beta)$. Here we have a couple of options: We can use\n",
    "\n",
    "1. the whole dataset (ordinary gradient descent)\n",
    "2. a small random batch of points (batch gradient descent)\n",
    "3. only a single point (stochastic gradient descent)\n",
    "\n",
    "to calculate the mean square error. Below, I'll do SGD for simplicity, but you may use any of these methods. \n",
    "\n",
    "Suppose at iteration $n$, we randomly pick the data point $(x^{(n)}, y^{(n)})$, and define:\n",
    "\n",
    "$$\n",
    "\\delta^{(n)} := \\varphi(\\alpha^{(n)} \\cdot x^{(n)} + \\beta^{(n)}) - y^{(n)}\n",
    "$$\n",
    "\n",
    "We define the **per-sample loss function** $\\ell^{(n)}$ at the sample $(x^{(n)}, y^{(n)})$ as:\n",
    "\n",
    "$$\n",
    "\\ell^{(n)}(\\alpha,\\beta) := (y^{(n)} - \\varphi(\\alpha^{(n)} \\cdot x^{(n)} + \\beta^{(n)}))^2 = (\\delta^{(n)})^2\n",
    "$$\n",
    "\n",
    "We compute the gradient of $\\ell^{(n)}$ with respect to $\\alpha$ as follows: Let\n",
    "\n",
    "$$\n",
    "z^{(n)} := \\alpha^{(n)} \\cdot x^{(n)} + \\beta^{(n)} \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\ell^{(n)} = (y^{(n)} - \\varphi(z^{(n)}))^2\n",
    "$$\n",
    "\n",
    "Applying the chain rule:\n",
    "\n",
    "$$\n",
    "\\nabla_\\alpha \\ell^{(n)} = 2(y^{(n)} - \\varphi(z^{(n)})) \\cdot (-\\varphi'(z^{(n)})) \\cdot x^{(n)} = -2 \\delta^{(n)} \\varphi'(z^{(n)}) x^{(n)}\n",
    "$$\n",
    "\n",
    "If we drop the constant $2$ into the learning rate $\\eta$, the update for $\\alpha$ and $\\beta$, we obtain:\n",
    "\n",
    "$$\n",
    "\\alpha^{(n+1)} = \\alpha^{(n)} - \\eta \\cdot \\delta^{(n)} \\varphi'(z^{(n)}) x^{(n)}\n",
    "\\qquad\n",
    "\\beta^{(n+1)} = \\beta^{(n)} - \\eta \\cdot \\delta^{(n)} \\varphi'(z^{(n)})\n",
    "$$\n",
    "\n",
    "### Feed-forward and back-propagation\n",
    "\n",
    "In the feed-forward stage of the computation, we calculate the output $\\varphi(\\alpha^{(n)}\\cdot x + \\beta^{(n)})$. In the back-propagation phase, we calculate the error $y - \\varphi(\\alpha^{(n)}\\cdot x + \\beta^{(n)})$ and adjust the weights as described above to obtain the next iteration of weights $(\\alpha^{(n+1)},\\beta^{(n+1)})$.\n",
    "\n",
    "### An example\n",
    "\n",
    "For this example, we are going to use a [toy dataset](http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)) from UCI: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar = fetch_ucirepo(id=151) \n",
    "  \n",
    "sonar_X = sonar.data.features \n",
    "sonar_y_raw = sonar.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "sonar_y = encoder.fit_transform(sonar_y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(xs, ys, f, f_prime, epochs, track=10, eta=0.1, tol=1e-4):\n",
    "    # Convert to NumPy arrays and add bias term\n",
    "    X = np.hstack([np.ones((len(xs), 1)), xs])\n",
    "    N, d = X.shape\n",
    "    w = np.random.randn(d)\n",
    "    errors = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pick a random training sample\n",
    "        j = np.random.randint(N)\n",
    "\n",
    "        # Forward pass\n",
    "        z = np.dot(w, X[j])\n",
    "        a = f(z)\n",
    "        delta = a - ys[j]\n",
    "\n",
    "        # Update rule if error exceeds tolerance\n",
    "        if abs(delta) > tol:\n",
    "            grad = delta * f_prime(z) * X[j]\n",
    "            w -= eta * grad\n",
    "\n",
    "        # Track training error every 10 steps\n",
    "        if i % track == 0:\n",
    "            z_all = X @ w\n",
    "            y_hat = f(z_all)\n",
    "            mse = np.mean((y_hat - ys) ** 2)\n",
    "            errors.append(mse)\n",
    "\n",
    "    # Final prediction\n",
    "    y_pred = f(X @ w)\n",
    "    return y_pred, errors, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Train\n",
    "y_pred, errs, weights = fit_predict(sonar_X, sonar_y.reshape(-1), sigmoid, sigmoid_prime, epochs=15000, track=100, eta=0.5, tol=5e-4)\n",
    "\n",
    "# Plot error\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(errs)\n",
    "plt.xlabel(\"Epochs (every 100 steps)\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89       111\n",
      "           1       0.90      0.84      0.87        97\n",
      "\n",
      "    accuracy                           0.88       208\n",
      "   macro avg       0.88      0.88      0.88       208\n",
      "weighted avg       0.88      0.88      0.88       208\n",
      "\n",
      "Accuracy: 0.8798076923076923\n"
     ]
    }
   ],
   "source": [
    "def helper(x,th=0.5):\n",
    "    if x>=th:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "tmp = np.vectorize(helper)(y_pred).reshape(-1)\n",
    "\n",
    "print(classification_report(sonar_y, tmp))\n",
    "print(\"Accuracy:\", accuracy_score(sonar_y, tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "While perceptrons are limited in their ability to represent complex non-linear relationships, they can be composed into deeper architectures known as [neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network). \n",
    "\n",
    "Another way to describe a neural network is as a **directed computational graph** of perceptrons layered in stages:\n",
    "\n",
    "<center>\n",
    "<img width=\"450px\" src=\"../images/neural-net.png\">\n",
    "</center>\n",
    "\n",
    "Each layer applies a **linear transformation followed by a non-linear activation function**. If we denote by $x^i \\in \\mathbb{R}^{d_i}$ the vector of activations at layer $i$, and by $W^i \\in \\mathbb{R}^{d_{i+1} \\times d_i}$ the weight matrix between layers $i$ and $i+1$, then the feedforward rule is:\n",
    "\n",
    "$$\n",
    "x^{i+1} = f^i(W^i x^i + b^i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $f^i$ is an activation function (e.g., ReLU, sigmoid),\n",
    "- $b^i \\in \\mathbb{R}^{d_{i+1}}$ is a bias vector.\n",
    "\n",
    "This transformation is applied layer by layer, from the input $x^0$ to the final output $x^L$, where $L$ is the number of layers.\n",
    "\n",
    "Just as in the perceptron case, once an output is obtained, the error with respect to the true label is measured via a loss function (e.g., squared error or cross-entropy). Then, using a technique known as **backpropagation**, we compute gradients of the loss with respect to each weight via the **chain rule** of calculus.\n",
    "\n",
    "These gradients are used to iteratively update weights via **gradient descent**:\n",
    "\n",
    "$$\n",
    "W^i \\leftarrow W^i - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^i}\n",
    "$$\n",
    "\n",
    "This completes one iteration of the **training phase** of the network.\n",
    "\n",
    "Neural networks with just one hidden layer and sufficient width can approximate any continuous function on compact subsets of $\\mathbb{R}^n$, a result known as the **universal approximation theorem**. There are [many different types of neural network architectures](http://www.asimovinstitute.org/neural-network-zoo/): convolutional networks for images, recurrent networks for sequences, transformers for language, and more.\n",
    "\n",
    "Unlike simple perceptrons, implementing deep networks from scratch is **not recommended in practice**. \n",
    "\n",
    "<center>\n",
    "<img src=\"../images/meme.jpg\">\n",
    "</center>\n",
    "\n",
    "Instead, we use powerful open-source libraries such as:\n",
    "\n",
    "1. [TensorFlow](https://www.tensorflow.org/)\n",
    "2. [PyTorch](https://pytorch.org/)\n",
    "3. [Keras](https://keras.io/)  high-level API on top of TensorFlow\n",
    "4. [MXNet](https://mxnet.apache.org/)\n",
    "\n",
    "For intuitive experimentation with small networks, I highly recommend the interactive [TensorFlow Playground](https://playground.tensorflow.org/), where you can design your own architectures and see how they learn in real time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example \n",
    "\n",
    "All of todays examples are going to use the keras library. Let us start with the first example we used today, the sonar dataset. Let us construct a simple neural-net for binary classification, i.e. a perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(60,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sonar_X,sonar_y,train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8151 - loss: 0.4353 \n",
      "Epoch 2/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8114 - loss: 0.4388 \n",
      "Epoch 3/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7946 - loss: 0.4358 \n",
      "Epoch 4/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8194 - loss: 0.4080 \n",
      "Epoch 5/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8260 - loss: 0.4035 \n",
      "Epoch 6/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7916 - loss: 0.4269 \n",
      "Epoch 7/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7858 - loss: 0.4276 \n",
      "Epoch 8/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8034 - loss: 0.4345 \n",
      "Epoch 9/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7722 - loss: 0.4808 \n",
      "Epoch 10/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8299 - loss: 0.4458 \n",
      "Epoch 11/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8659 - loss: 0.4040 \n",
      "Epoch 12/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8083 - loss: 0.4376 \n",
      "Epoch 13/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8087 - loss: 0.4173 \n",
      "Epoch 14/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7810 - loss: 0.4618 \n",
      "Epoch 15/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8037 - loss: 0.4385 \n",
      "Epoch 16/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7977 - loss: 0.4303 \n",
      "Epoch 17/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8195 - loss: 0.4119 \n",
      "Epoch 18/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7585 - loss: 0.4771 \n",
      "Epoch 19/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7687 - loss: 0.4553 \n",
      "Epoch 20/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7536 - loss: 0.4716 \n",
      "Epoch 21/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7924 - loss: 0.4580 \n",
      "Epoch 22/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7811 - loss: 0.4663 \n",
      "Epoch 23/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7619 - loss: 0.4771 \n",
      "Epoch 24/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8067 - loss: 0.4235 \n",
      "Epoch 25/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7988 - loss: 0.4228 \n",
      "Epoch 26/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7855 - loss: 0.4311 \n",
      "Epoch 27/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7523 - loss: 0.4662 \n",
      "Epoch 28/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7631 - loss: 0.4659 \n",
      "Epoch 29/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7911 - loss: 0.4562 \n",
      "Epoch 30/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7803 - loss: 0.4640 \n",
      "Epoch 31/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7614 - loss: 0.4424 \n",
      "Epoch 32/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8038 - loss: 0.4271 \n",
      "Epoch 33/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7650 - loss: 0.4597 \n",
      "Epoch 34/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7609 - loss: 0.4549 \n",
      "Epoch 35/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7581 - loss: 0.4323 \n",
      "Epoch 36/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8001 - loss: 0.4114 \n",
      "Epoch 37/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7532 - loss: 0.4645 \n",
      "Epoch 38/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7561 - loss: 0.4801 \n",
      "Epoch 39/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7916 - loss: 0.4467 \n",
      "Epoch 40/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8127 - loss: 0.4157 \n",
      "Epoch 41/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7769 - loss: 0.4291 \n",
      "Epoch 42/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7646 - loss: 0.4658 \n",
      "Epoch 43/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8139 - loss: 0.4080 \n",
      "Epoch 44/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8012 - loss: 0.4293 \n",
      "Epoch 45/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7769 - loss: 0.4650 \n",
      "Epoch 46/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7862 - loss: 0.4395 \n",
      "Epoch 47/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7744 - loss: 0.4388 \n",
      "Epoch 48/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7841 - loss: 0.4411 \n",
      "Epoch 49/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8055 - loss: 0.4293 \n",
      "Epoch 50/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7906 - loss: 0.4486 \n",
      "Epoch 51/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8105 - loss: 0.4232 \n",
      "Epoch 52/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7642 - loss: 0.4470 \n",
      "Epoch 53/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8013 - loss: 0.4518 \n",
      "Epoch 54/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7748 - loss: 0.4423 \n",
      "Epoch 55/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8353 - loss: 0.4098 \n",
      "Epoch 56/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7759 - loss: 0.4404 \n",
      "Epoch 57/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7829 - loss: 0.4347 \n",
      "Epoch 58/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7874 - loss: 0.4445 \n",
      "Epoch 59/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7946 - loss: 0.4233 \n",
      "Epoch 60/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8296 - loss: 0.4161 \n",
      "Epoch 61/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7638 - loss: 0.4560 \n",
      "Epoch 62/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7825 - loss: 0.4434 \n",
      "Epoch 63/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7934 - loss: 0.4335 \n",
      "Epoch 64/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7634 - loss: 0.4466 \n",
      "Epoch 65/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7870 - loss: 0.4163 \n",
      "Epoch 66/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7849 - loss: 0.4542 \n",
      "Epoch 67/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7856 - loss: 0.4457 \n",
      "Epoch 68/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7967 - loss: 0.4354 \n",
      "Epoch 69/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7372 - loss: 0.4686 \n",
      "Epoch 70/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7859 - loss: 0.4515 \n",
      "Epoch 71/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7727 - loss: 0.4350 \n",
      "Epoch 72/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7550 - loss: 0.4655 \n",
      "Epoch 73/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7708 - loss: 0.4453 \n",
      "Epoch 74/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7953 - loss: 0.4409 \n",
      "Epoch 75/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7969 - loss: 0.4074 \n",
      "Epoch 76/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7984 - loss: 0.4217 \n",
      "Epoch 77/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8034 - loss: 0.4477 \n",
      "Epoch 78/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8091 - loss: 0.4292 \n",
      "Epoch 79/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7662 - loss: 0.4397 \n",
      "Epoch 80/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7948 - loss: 0.4236 \n",
      "Epoch 81/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8026 - loss: 0.4309 \n",
      "Epoch 82/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8002 - loss: 0.4456 \n",
      "Epoch 83/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8189 - loss: 0.4037 \n",
      "Epoch 84/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7762 - loss: 0.4487 \n",
      "Epoch 85/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7940 - loss: 0.4410 \n",
      "Epoch 86/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8123 - loss: 0.4030 \n",
      "Epoch 87/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8075 - loss: 0.4195 \n",
      "Epoch 88/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8032 - loss: 0.4155 \n",
      "Epoch 89/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7943 - loss: 0.4320 \n",
      "Epoch 90/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7775 - loss: 0.4343 \n",
      "Epoch 91/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7908 - loss: 0.4221 \n",
      "Epoch 92/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7955 - loss: 0.4116 \n",
      "Epoch 93/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8029 - loss: 0.4299 \n",
      "Epoch 94/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8053 - loss: 0.4166 \n",
      "Epoch 95/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8062 - loss: 0.4275 \n",
      "Epoch 96/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7820 - loss: 0.4516 \n",
      "Epoch 97/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7804 - loss: 0.4387 \n",
      "Epoch 98/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7556 - loss: 0.4589 \n",
      "Epoch 99/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7695 - loss: 0.4597 \n",
      "Epoch 100/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8117 - loss: 0.4496 \n",
      "Epoch 101/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8175 - loss: 0.4254 \n",
      "Epoch 102/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7774 - loss: 0.4509 \n",
      "Epoch 103/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8361 - loss: 0.4034 \n",
      "Epoch 104/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8338 - loss: 0.4081 \n",
      "Epoch 105/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8222 - loss: 0.4168 \n",
      "Epoch 106/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8158 - loss: 0.4171 \n",
      "Epoch 107/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8068 - loss: 0.4344 \n",
      "Epoch 108/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7676 - loss: 0.4464 \n",
      "Epoch 109/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7792 - loss: 0.4499 \n",
      "Epoch 110/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8189 - loss: 0.3935 \n",
      "Epoch 111/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7898 - loss: 0.4487 \n",
      "Epoch 112/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8083 - loss: 0.4225 \n",
      "Epoch 113/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7764 - loss: 0.4335 \n",
      "Epoch 114/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7946 - loss: 0.4323 \n",
      "Epoch 115/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7970 - loss: 0.4424 \n",
      "Epoch 116/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8348 - loss: 0.4199 \n",
      "Epoch 117/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8057 - loss: 0.3992 \n",
      "Epoch 118/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7887 - loss: 0.4318 \n",
      "Epoch 119/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8069 - loss: 0.4098 \n",
      "Epoch 120/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8324 - loss: 0.4135 \n",
      "Epoch 121/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8400 - loss: 0.3925 \n",
      "Epoch 122/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7671 - loss: 0.4418 \n",
      "Epoch 123/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8011 - loss: 0.4219 \n",
      "Epoch 124/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7887 - loss: 0.4357 \n",
      "Epoch 125/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7857 - loss: 0.4230 \n",
      "Epoch 126/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7995 - loss: 0.4165 \n",
      "Epoch 127/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7548 - loss: 0.4440 \n",
      "Epoch 128/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7815 - loss: 0.4302 \n",
      "Epoch 129/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7676 - loss: 0.4305 \n",
      "Epoch 130/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8059 - loss: 0.4259 \n",
      "Epoch 131/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8014 - loss: 0.4161 \n",
      "Epoch 132/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7702 - loss: 0.4270 \n",
      "Epoch 133/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7762 - loss: 0.4220 \n",
      "Epoch 134/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7960 - loss: 0.4441 \n",
      "Epoch 135/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7673 - loss: 0.4465 \n",
      "Epoch 136/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8084 - loss: 0.4119 \n",
      "Epoch 137/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7695 - loss: 0.4453 \n",
      "Epoch 138/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7519 - loss: 0.4504 \n",
      "Epoch 139/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7874 - loss: 0.4142 \n",
      "Epoch 140/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8025 - loss: 0.4189 \n",
      "Epoch 141/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8076 - loss: 0.4255 \n",
      "Epoch 142/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7502 - loss: 0.4622 \n",
      "Epoch 143/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8048 - loss: 0.4120 \n",
      "Epoch 144/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7867 - loss: 0.4356 \n",
      "Epoch 145/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8066 - loss: 0.4167 \n",
      "Epoch 146/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7958 - loss: 0.4073 \n",
      "Epoch 147/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8192 - loss: 0.4138 \n",
      "Epoch 148/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8055 - loss: 0.4180 \n",
      "Epoch 149/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8150 - loss: 0.4131 \n",
      "Epoch 150/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8340 - loss: 0.3935 \n",
      "Epoch 151/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7854 - loss: 0.4098 \n",
      "Epoch 152/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8302 - loss: 0.4121 \n",
      "Epoch 153/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7842 - loss: 0.4367 \n",
      "Epoch 154/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7977 - loss: 0.4116 \n",
      "Epoch 155/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7773 - loss: 0.4309 \n",
      "Epoch 156/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7406 - loss: 0.4488 \n",
      "Epoch 157/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8225 - loss: 0.4144 \n",
      "Epoch 158/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7719 - loss: 0.4518 \n",
      "Epoch 159/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8295 - loss: 0.4025 \n",
      "Epoch 160/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8192 - loss: 0.3976 \n",
      "Epoch 161/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8317 - loss: 0.4010 \n",
      "Epoch 162/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7923 - loss: 0.4066 \n",
      "Epoch 163/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7991 - loss: 0.4259 \n",
      "Epoch 164/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8042 - loss: 0.4133 \n",
      "Epoch 165/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7572 - loss: 0.4431 \n",
      "Epoch 166/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7528 - loss: 0.4482 \n",
      "Epoch 167/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7769 - loss: 0.4226 \n",
      "Epoch 168/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8269 - loss: 0.3775 \n",
      "Epoch 169/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7966 - loss: 0.3952 \n",
      "Epoch 170/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7829 - loss: 0.4124 \n",
      "Epoch 171/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7893 - loss: 0.4460 \n",
      "Epoch 172/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7771 - loss: 0.4329 \n",
      "Epoch 173/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7964 - loss: 0.4254 \n",
      "Epoch 174/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8059 - loss: 0.4076 \n",
      "Epoch 175/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8088 - loss: 0.4359 \n",
      "Epoch 176/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7832 - loss: 0.4185 \n",
      "Epoch 177/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7772 - loss: 0.4274 \n",
      "Epoch 178/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7684 - loss: 0.4427 \n",
      "Epoch 179/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7826 - loss: 0.4358 \n",
      "Epoch 180/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8073 - loss: 0.4058 \n",
      "Epoch 181/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8008 - loss: 0.4325 \n",
      "Epoch 182/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8064 - loss: 0.4071 \n",
      "Epoch 183/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8147 - loss: 0.4143 \n",
      "Epoch 184/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8126 - loss: 0.4219 \n",
      "Epoch 185/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7607 - loss: 0.4391 \n",
      "Epoch 186/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8206 - loss: 0.3893 \n",
      "Epoch 187/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7755 - loss: 0.4316 \n",
      "Epoch 188/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8000 - loss: 0.4084 \n",
      "Epoch 189/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8090 - loss: 0.4195 \n",
      "Epoch 190/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7903 - loss: 0.4147 \n",
      "Epoch 191/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7866 - loss: 0.4239 \n",
      "Epoch 192/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7659 - loss: 0.4262 \n",
      "Epoch 193/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7942 - loss: 0.4216 \n",
      "Epoch 194/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7689 - loss: 0.4223 \n",
      "Epoch 195/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7801 - loss: 0.4407 \n",
      "Epoch 196/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7794 - loss: 0.4309 \n",
      "Epoch 197/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7944 - loss: 0.4162 \n",
      "Epoch 198/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7849 - loss: 0.4168 \n",
      "Epoch 199/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8229 - loss: 0.3893 \n",
      "Epoch 200/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7825 - loss: 0.4257 \n",
      "Epoch 201/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7890 - loss: 0.4088 \n",
      "Epoch 202/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7963 - loss: 0.4196 \n",
      "Epoch 203/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7938 - loss: 0.4162 \n",
      "Epoch 204/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7896 - loss: 0.4336 \n",
      "Epoch 205/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7612 - loss: 0.4365 \n",
      "Epoch 206/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7302 - loss: 0.4530 \n",
      "Epoch 207/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8073 - loss: 0.3994 \n",
      "Epoch 208/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8035 - loss: 0.3966 \n",
      "Epoch 209/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8147 - loss: 0.3972 \n",
      "Epoch 210/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7988 - loss: 0.3990 \n",
      "Epoch 211/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7676 - loss: 0.4503 \n",
      "Epoch 212/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7977 - loss: 0.4147 \n",
      "Epoch 213/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7860 - loss: 0.4290 \n",
      "Epoch 214/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7779 - loss: 0.4113 \n",
      "Epoch 215/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7896 - loss: 0.4201 \n",
      "Epoch 216/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7587 - loss: 0.4538 \n",
      "Epoch 217/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7653 - loss: 0.4290 \n",
      "Epoch 218/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7697 - loss: 0.4212 \n",
      "Epoch 219/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7378 - loss: 0.4385 \n",
      "Epoch 220/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8062 - loss: 0.4020 \n",
      "Epoch 221/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8105 - loss: 0.4047 \n",
      "Epoch 222/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7425 - loss: 0.4406 \n",
      "Epoch 223/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7800 - loss: 0.4132 \n",
      "Epoch 224/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7851 - loss: 0.4162 \n",
      "Epoch 225/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8009 - loss: 0.4139 \n",
      "Epoch 226/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7959 - loss: 0.4158 \n",
      "Epoch 227/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8038 - loss: 0.3954 \n",
      "Epoch 228/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7896 - loss: 0.4234 \n",
      "Epoch 229/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7834 - loss: 0.4259 \n",
      "Epoch 230/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7681 - loss: 0.4205 \n",
      "Epoch 231/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7613 - loss: 0.4505 \n",
      "Epoch 232/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7982 - loss: 0.4211 \n",
      "Epoch 233/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7867 - loss: 0.4117 \n",
      "Epoch 234/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8130 - loss: 0.4035 \n",
      "Epoch 235/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7927 - loss: 0.4118 \n",
      "Epoch 236/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8188 - loss: 0.3757 \n",
      "Epoch 237/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7753 - loss: 0.4482 \n",
      "Epoch 238/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7900 - loss: 0.4010 \n",
      "Epoch 239/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7687 - loss: 0.4342 \n",
      "Epoch 240/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8283 - loss: 0.3854 \n",
      "Epoch 241/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7906 - loss: 0.4069 \n",
      "Epoch 242/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7803 - loss: 0.4170 \n",
      "Epoch 243/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7678 - loss: 0.4485 \n",
      "Epoch 244/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7709 - loss: 0.4117 \n",
      "Epoch 245/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8079 - loss: 0.3868 \n",
      "Epoch 246/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8352 - loss: 0.3777 \n",
      "Epoch 247/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7756 - loss: 0.4290 \n",
      "Epoch 248/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7481 - loss: 0.4324 \n",
      "Epoch 249/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7807 - loss: 0.4158 \n",
      "Epoch 250/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7517 - loss: 0.4166 \n",
      "Epoch 251/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7969 - loss: 0.3903 \n",
      "Epoch 252/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.7758 - loss: 0.4049 \n",
      "Epoch 253/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7385 - loss: 0.4491 \n",
      "Epoch 254/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7946 - loss: 0.4058 \n",
      "Epoch 255/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7933 - loss: 0.3966 \n",
      "Epoch 256/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8071 - loss: 0.3771 \n",
      "Epoch 257/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7958 - loss: 0.3874 \n",
      "Epoch 258/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7956 - loss: 0.4013 \n",
      "Epoch 259/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7716 - loss: 0.4028 \n",
      "Epoch 260/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7744 - loss: 0.4453 \n",
      "Epoch 261/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8002 - loss: 0.3866 \n",
      "Epoch 262/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7325 - loss: 0.4487 \n",
      "Epoch 263/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7909 - loss: 0.4037 \n",
      "Epoch 264/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8154 - loss: 0.4083 \n",
      "Epoch 265/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8200 - loss: 0.3858 \n",
      "Epoch 266/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7725 - loss: 0.4145 \n",
      "Epoch 267/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7881 - loss: 0.4089 \n",
      "Epoch 268/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7682 - loss: 0.4163 \n",
      "Epoch 269/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7997 - loss: 0.4191 \n",
      "Epoch 270/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7963 - loss: 0.4164 \n",
      "Epoch 271/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7697 - loss: 0.4328 \n",
      "Epoch 272/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7493 - loss: 0.4280 \n",
      "Epoch 273/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7887 - loss: 0.3954 \n",
      "Epoch 274/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8094 - loss: 0.4039 \n",
      "Epoch 275/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7471 - loss: 0.4378 \n",
      "Epoch 276/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8090 - loss: 0.3801 \n",
      "Epoch 277/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7990 - loss: 0.4073 \n",
      "Epoch 278/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7937 - loss: 0.4096 \n",
      "Epoch 279/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7968 - loss: 0.4013 \n",
      "Epoch 280/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7786 - loss: 0.4157 \n",
      "Epoch 281/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7926 - loss: 0.4162 \n",
      "Epoch 282/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7767 - loss: 0.4215 \n",
      "Epoch 283/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7917 - loss: 0.4004 \n",
      "Epoch 284/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7428 - loss: 0.4450 \n",
      "Epoch 285/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7610 - loss: 0.4245 \n",
      "Epoch 286/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7524 - loss: 0.4510 \n",
      "Epoch 287/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8005 - loss: 0.4126 \n",
      "Epoch 288/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7846 - loss: 0.4159 \n",
      "Epoch 289/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8122 - loss: 0.3874 \n",
      "Epoch 290/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7978 - loss: 0.3851 \n",
      "Epoch 291/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7598 - loss: 0.4082 \n",
      "Epoch 292/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8117 - loss: 0.3971 \n",
      "Epoch 293/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7784 - loss: 0.4057 \n",
      "Epoch 294/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7829 - loss: 0.4096 \n",
      "Epoch 295/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8205 - loss: 0.3917 \n",
      "Epoch 296/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8001 - loss: 0.4158 \n",
      "Epoch 297/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7881 - loss: 0.4211 \n",
      "Epoch 298/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7831 - loss: 0.4067 \n",
      "Epoch 299/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7855 - loss: 0.4194 \n",
      "Epoch 300/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8053 - loss: 0.3991 \n",
      "Epoch 301/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7416 - loss: 0.4453 \n",
      "Epoch 302/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8168 - loss: 0.3835 \n",
      "Epoch 303/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8021 - loss: 0.3865 \n",
      "Epoch 304/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8156 - loss: 0.3994 \n",
      "Epoch 305/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7717 - loss: 0.4291 \n",
      "Epoch 306/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7855 - loss: 0.4232 \n",
      "Epoch 307/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7958 - loss: 0.4243 \n",
      "Epoch 308/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7880 - loss: 0.4162 \n",
      "Epoch 309/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7593 - loss: 0.4588 \n",
      "Epoch 310/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7721 - loss: 0.4111 \n",
      "Epoch 311/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7522 - loss: 0.4408 \n",
      "Epoch 312/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7994 - loss: 0.3989 \n",
      "Epoch 313/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7996 - loss: 0.4217 \n",
      "Epoch 314/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8065 - loss: 0.3999 \n",
      "Epoch 315/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7962 - loss: 0.4132 \n",
      "Epoch 316/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8334 - loss: 0.3717 \n",
      "Epoch 317/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8039 - loss: 0.4020 \n",
      "Epoch 318/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7949 - loss: 0.4002 \n",
      "Epoch 319/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8157 - loss: 0.3948 \n",
      "Epoch 320/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7954 - loss: 0.3989 \n",
      "Epoch 321/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7783 - loss: 0.4217 \n",
      "Epoch 322/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7928 - loss: 0.3997 \n",
      "Epoch 323/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7511 - loss: 0.4305 \n",
      "Epoch 324/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7525 - loss: 0.4310 \n",
      "Epoch 325/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8305 - loss: 0.3849 \n",
      "Epoch 326/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8121 - loss: 0.3964 \n",
      "Epoch 327/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7512 - loss: 0.4191 \n",
      "Epoch 328/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7665 - loss: 0.4231 \n",
      "Epoch 329/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7693 - loss: 0.4263 \n",
      "Epoch 330/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8063 - loss: 0.3914 \n",
      "Epoch 331/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7948 - loss: 0.4110 \n",
      "Epoch 332/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7793 - loss: 0.4187 \n",
      "Epoch 333/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8300 - loss: 0.4068 \n",
      "Epoch 334/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7969 - loss: 0.4046 \n",
      "Epoch 335/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7905 - loss: 0.4292 \n",
      "Epoch 336/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7512 - loss: 0.4159 \n",
      "Epoch 337/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7845 - loss: 0.4045 \n",
      "Epoch 338/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7770 - loss: 0.4212 \n",
      "Epoch 339/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8057 - loss: 0.3787 \n",
      "Epoch 340/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7926 - loss: 0.4112 \n",
      "Epoch 341/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8125 - loss: 0.3917 \n",
      "Epoch 342/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7467 - loss: 0.4360 \n",
      "Epoch 343/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7876 - loss: 0.3978 \n",
      "Epoch 344/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7856 - loss: 0.3764 \n",
      "Epoch 345/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7931 - loss: 0.3935 \n",
      "Epoch 346/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7859 - loss: 0.3820 \n",
      "Epoch 347/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8034 - loss: 0.3726 \n",
      "Epoch 348/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7853 - loss: 0.3898 \n",
      "Epoch 349/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8314 - loss: 0.3805 \n",
      "Epoch 350/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7725 - loss: 0.3971 \n",
      "Epoch 351/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7675 - loss: 0.4262 \n",
      "Epoch 352/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8032 - loss: 0.3980 \n",
      "Epoch 353/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8048 - loss: 0.3933 \n",
      "Epoch 354/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7467 - loss: 0.4263 \n",
      "Epoch 355/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7887 - loss: 0.4134 \n",
      "Epoch 356/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7632 - loss: 0.4115 \n",
      "Epoch 357/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7892 - loss: 0.4009 \n",
      "Epoch 358/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8072 - loss: 0.4124 \n",
      "Epoch 359/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7844 - loss: 0.4054 \n",
      "Epoch 360/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7684 - loss: 0.3978 \n",
      "Epoch 361/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7965 - loss: 0.3828 \n",
      "Epoch 362/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7189 - loss: 0.4532 \n",
      "Epoch 363/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7772 - loss: 0.4058 \n",
      "Epoch 364/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7538 - loss: 0.4234 \n",
      "Epoch 365/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8003 - loss: 0.4203 \n",
      "Epoch 366/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7767 - loss: 0.4165 \n",
      "Epoch 367/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7886 - loss: 0.4063 \n",
      "Epoch 368/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7519 - loss: 0.4380 \n",
      "Epoch 369/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7756 - loss: 0.4004 \n",
      "Epoch 370/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8513 - loss: 0.3683 \n",
      "Epoch 371/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7795 - loss: 0.4029 \n",
      "Epoch 372/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8031 - loss: 0.4037 \n",
      "Epoch 373/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7779 - loss: 0.4174 \n",
      "Epoch 374/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8394 - loss: 0.3557 \n",
      "Epoch 375/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7665 - loss: 0.4244 \n",
      "Epoch 376/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8357 - loss: 0.3792 \n",
      "Epoch 377/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7898 - loss: 0.4018 \n",
      "Epoch 378/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7699 - loss: 0.4128 \n",
      "Epoch 379/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7702 - loss: 0.4077 \n",
      "Epoch 380/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7973 - loss: 0.3971 \n",
      "Epoch 381/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7565 - loss: 0.4180 \n",
      "Epoch 382/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7578 - loss: 0.4323 \n",
      "Epoch 383/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8027 - loss: 0.3904 \n",
      "Epoch 384/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7607 - loss: 0.4252 \n",
      "Epoch 385/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7298 - loss: 0.4404 \n",
      "Epoch 386/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7916 - loss: 0.4052 \n",
      "Epoch 387/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7967 - loss: 0.3814 \n",
      "Epoch 388/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7984 - loss: 0.3981 \n",
      "Epoch 389/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8016 - loss: 0.3838 \n",
      "Epoch 390/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8294 - loss: 0.3621 \n",
      "Epoch 391/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7841 - loss: 0.4284 \n",
      "Epoch 392/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8038 - loss: 0.4036 \n",
      "Epoch 393/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7989 - loss: 0.4027 \n",
      "Epoch 394/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7884 - loss: 0.4073 \n",
      "Epoch 395/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7830 - loss: 0.3968 \n",
      "Epoch 396/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7542 - loss: 0.4280 \n",
      "Epoch 397/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7735 - loss: 0.4053 \n",
      "Epoch 398/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7825 - loss: 0.3862 \n",
      "Epoch 399/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7971 - loss: 0.3914 \n",
      "Epoch 400/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7474 - loss: 0.4213 \n",
      "Epoch 401/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8043 - loss: 0.3868 \n",
      "Epoch 402/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7901 - loss: 0.3917 \n",
      "Epoch 403/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8255 - loss: 0.3514 \n",
      "Epoch 404/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7585 - loss: 0.4285 \n",
      "Epoch 405/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8081 - loss: 0.4000 \n",
      "Epoch 406/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7689 - loss: 0.4127 \n",
      "Epoch 407/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7886 - loss: 0.4062 \n",
      "Epoch 408/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7946 - loss: 0.3744 \n",
      "Epoch 409/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7844 - loss: 0.4214 \n",
      "Epoch 410/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8174 - loss: 0.3609 \n",
      "Epoch 411/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7889 - loss: 0.3982 \n",
      "Epoch 412/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7798 - loss: 0.4287 \n",
      "Epoch 413/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7885 - loss: 0.4046 \n",
      "Epoch 414/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7824 - loss: 0.4202 \n",
      "Epoch 415/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7991 - loss: 0.3989 \n",
      "Epoch 416/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7849 - loss: 0.4063 \n",
      "Epoch 417/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7838 - loss: 0.4019 \n",
      "Epoch 418/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7508 - loss: 0.4434 \n",
      "Epoch 419/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8083 - loss: 0.3766 \n",
      "Epoch 420/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7635 - loss: 0.4049 \n",
      "Epoch 421/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7933 - loss: 0.4082 \n",
      "Epoch 422/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7931 - loss: 0.4136 \n",
      "Epoch 423/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7787 - loss: 0.4285 \n",
      "Epoch 424/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7790 - loss: 0.4121 \n",
      "Epoch 425/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7767 - loss: 0.4170 \n",
      "Epoch 426/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7630 - loss: 0.4044 \n",
      "Epoch 427/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8359 - loss: 0.3521 \n",
      "Epoch 428/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7759 - loss: 0.3925 \n",
      "Epoch 429/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7955 - loss: 0.3842 \n",
      "Epoch 430/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8127 - loss: 0.3845 \n",
      "Epoch 431/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7543 - loss: 0.3961 \n",
      "Epoch 432/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7878 - loss: 0.4161 \n",
      "Epoch 433/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7567 - loss: 0.3867 \n",
      "Epoch 434/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8007 - loss: 0.3724 \n",
      "Epoch 435/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8117 - loss: 0.3759 \n",
      "Epoch 436/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7783 - loss: 0.4286 \n",
      "Epoch 437/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7949 - loss: 0.3895 \n",
      "Epoch 438/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7617 - loss: 0.4200 \n",
      "Epoch 439/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8412 - loss: 0.3890 \n",
      "Epoch 440/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7875 - loss: 0.4120 \n",
      "Epoch 441/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8449 - loss: 0.3590 \n",
      "Epoch 442/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8087 - loss: 0.4017 \n",
      "Epoch 443/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8346 - loss: 0.3927 \n",
      "Epoch 444/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8121 - loss: 0.3833 \n",
      "Epoch 445/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7978 - loss: 0.3947 \n",
      "Epoch 446/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7691 - loss: 0.4045 \n",
      "Epoch 447/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7579 - loss: 0.4139 \n",
      "Epoch 448/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7737 - loss: 0.4241 \n",
      "Epoch 449/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7814 - loss: 0.4153 \n",
      "Epoch 450/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8053 - loss: 0.3689 \n",
      "Epoch 451/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7993 - loss: 0.4077 \n",
      "Epoch 452/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8060 - loss: 0.4040 \n",
      "Epoch 453/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8328 - loss: 0.3812 \n",
      "Epoch 454/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8303 - loss: 0.4077 \n",
      "Epoch 455/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7691 - loss: 0.4045 \n",
      "Epoch 456/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7955 - loss: 0.3906 \n",
      "Epoch 457/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7827 - loss: 0.3998 \n",
      "Epoch 458/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7502 - loss: 0.4327 \n",
      "Epoch 459/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8339 - loss: 0.3770 \n",
      "Epoch 460/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8237 - loss: 0.3816 \n",
      "Epoch 461/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7521 - loss: 0.4474 \n",
      "Epoch 462/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7772 - loss: 0.4027 \n",
      "Epoch 463/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8046 - loss: 0.3720 \n",
      "Epoch 464/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7881 - loss: 0.3848 \n",
      "Epoch 465/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8031 - loss: 0.3747 \n",
      "Epoch 466/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7808 - loss: 0.4053 \n",
      "Epoch 467/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7649 - loss: 0.4221 \n",
      "Epoch 468/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8193 - loss: 0.3616 \n",
      "Epoch 469/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7921 - loss: 0.4124 \n",
      "Epoch 470/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8473 - loss: 0.3698 \n",
      "Epoch 471/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8127 - loss: 0.3815 \n",
      "Epoch 472/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8045 - loss: 0.4054 \n",
      "Epoch 473/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8032 - loss: 0.3788 \n",
      "Epoch 474/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7913 - loss: 0.4192 \n",
      "Epoch 475/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7879 - loss: 0.4412 \n",
      "Epoch 476/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7892 - loss: 0.3971 \n",
      "Epoch 477/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7901 - loss: 0.3784 \n",
      "Epoch 478/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7435 - loss: 0.4326 \n",
      "Epoch 479/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7952 - loss: 0.3854 \n",
      "Epoch 480/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7596 - loss: 0.4063 \n",
      "Epoch 481/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7942 - loss: 0.3768 \n",
      "Epoch 482/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7931 - loss: 0.3819 \n",
      "Epoch 483/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8046 - loss: 0.4024 \n",
      "Epoch 484/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7895 - loss: 0.4145 \n",
      "Epoch 485/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7994 - loss: 0.3745 \n",
      "Epoch 486/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7960 - loss: 0.3868 \n",
      "Epoch 487/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8030 - loss: 0.3616 \n",
      "Epoch 488/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7808 - loss: 0.4231 \n",
      "Epoch 489/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8075 - loss: 0.3866 \n",
      "Epoch 490/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8154 - loss: 0.3930 \n",
      "Epoch 491/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8435 - loss: 0.3950 \n",
      "Epoch 492/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8067 - loss: 0.4013 \n",
      "Epoch 493/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8158 - loss: 0.3944 \n",
      "Epoch 494/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7974 - loss: 0.4078 \n",
      "Epoch 495/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8120 - loss: 0.3913 \n",
      "Epoch 496/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8196 - loss: 0.3995 \n",
      "Epoch 497/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8355 - loss: 0.3814 \n",
      "Epoch 498/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7986 - loss: 0.3988 \n",
      "Epoch 499/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7778 - loss: 0.3983 \n",
      "Epoch 500/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8178 - loss: 0.3834 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7464778060f0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=500, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 20 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7464743345e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 21 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7464743345e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.73        29\n",
      "           1       1.00      0.38      0.55        34\n",
      "\n",
      "    accuracy                           0.67        63\n",
      "   macro avg       0.79      0.69      0.64        63\n",
      "weighted avg       0.81      0.67      0.64        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "tmp = np.vectorize(lambda x: helper(x,th=0.75))(y_predict.reshape(-1))\n",
    "print(classification_report(y_test,tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another standard small example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "x_iris = iris['data']\n",
    "y_iris = iris['target']\n",
    "\n",
    "labeler = LabelBinarizer()\n",
    "y = labeler.fit_transform(y_iris)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(4,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_iris, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - binary_accuracy: 0.7017 - loss: 0.6240  \n",
      "Epoch 2/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6706 - loss: 0.5484 \n",
      "Epoch 3/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7201 - loss: 0.5044 \n",
      "Epoch 4/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8837 - loss: 0.4565 \n",
      "Epoch 5/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8793 - loss: 0.4147 \n",
      "Epoch 6/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9029 - loss: 0.3867 \n",
      "Epoch 7/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8964 - loss: 0.3615 \n",
      "Epoch 8/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8982 - loss: 0.3364 \n",
      "Epoch 9/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8824 - loss: 0.3216 \n",
      "Epoch 10/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8917 - loss: 0.3036 \n",
      "Epoch 11/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8783 - loss: 0.2861 \n",
      "Epoch 12/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8844 - loss: 0.2884 \n",
      "Epoch 13/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8797 - loss: 0.2832 \n",
      "Epoch 14/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9102 - loss: 0.2690 \n",
      "Epoch 15/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8853 - loss: 0.2513 \n",
      "Epoch 16/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9195 - loss: 0.2459 \n",
      "Epoch 17/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9813 - loss: 0.2277 \n",
      "Epoch 18/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9260 - loss: 0.2362 \n",
      "Epoch 19/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9303 - loss: 0.2184 \n",
      "Epoch 20/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9564 - loss: 0.2061 \n",
      "Epoch 21/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9733 - loss: 0.1938 \n",
      "Epoch 22/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9678 - loss: 0.1916 \n",
      "Epoch 23/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9745 - loss: 0.1826 \n",
      "Epoch 24/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9768 - loss: 0.1966 \n",
      "Epoch 25/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9712 - loss: 0.1865 \n",
      "Epoch 26/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9757 - loss: 0.1709 \n",
      "Epoch 27/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9841 - loss: 0.1539 \n",
      "Epoch 28/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9770 - loss: 0.1531 \n",
      "Epoch 29/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9770 - loss: 0.1442 \n",
      "Epoch 30/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9793 - loss: 0.1395 \n",
      "Epoch 31/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9662 - loss: 0.1533 \n",
      "Epoch 32/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9829 - loss: 0.1396 \n",
      "Epoch 33/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9882 - loss: 0.1231 \n",
      "Epoch 34/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9709 - loss: 0.1468 \n",
      "Epoch 35/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9783 - loss: 0.1276 \n",
      "Epoch 36/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9872 - loss: 0.1255 \n",
      "Epoch 37/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9779 - loss: 0.1118 \n",
      "Epoch 38/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9864 - loss: 0.1198 \n",
      "Epoch 39/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9866 - loss: 0.1097 \n",
      "Epoch 40/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9906 - loss: 0.1023 \n",
      "Epoch 41/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.9886 - loss: 0.0960 \n",
      "Epoch 42/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9876 - loss: 0.0878 \n",
      "Epoch 43/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9943 - loss: 0.0875 \n",
      "Epoch 44/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9807 - loss: 0.0978 \n",
      "Epoch 45/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9906 - loss: 0.0896 \n",
      "Epoch 46/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9905 - loss: 0.0975 \n",
      "Epoch 47/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9847 - loss: 0.0830 \n",
      "Epoch 48/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9856 - loss: 0.0849 \n",
      "Epoch 49/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9892 - loss: 0.0754 \n",
      "Epoch 50/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9895 - loss: 0.0712 \n",
      "Epoch 51/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9783 - loss: 0.0873 \n",
      "Epoch 52/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9829 - loss: 0.0779 \n",
      "Epoch 53/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9919 - loss: 0.0717 \n",
      "Epoch 54/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9757 - loss: 0.0779 \n",
      "Epoch 55/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9938 - loss: 0.0641 \n",
      "Epoch 56/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9857 - loss: 0.0710 \n",
      "Epoch 57/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9827 - loss: 0.0823 \n",
      "Epoch 58/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9868 - loss: 0.0739 \n",
      "Epoch 59/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9844 - loss: 0.0689 \n",
      "Epoch 60/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9848 - loss: 0.0764 \n",
      "Epoch 61/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9796 - loss: 0.0777 \n",
      "Epoch 62/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9945 - loss: 0.0556 \n",
      "Epoch 63/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9848 - loss: 0.0734 \n",
      "Epoch 64/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9928 - loss: 0.0559 \n",
      "Epoch 65/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9820 - loss: 0.0616 \n",
      "Epoch 66/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9859 - loss: 0.0667 \n",
      "Epoch 67/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9749 - loss: 0.0838 \n",
      "Epoch 68/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0594 \n",
      "Epoch 69/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9836 - loss: 0.0623 \n",
      "Epoch 70/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9923 - loss: 0.0581 \n",
      "Epoch 71/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9756 - loss: 0.0692 \n",
      "Epoch 72/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9919 - loss: 0.0558 \n",
      "Epoch 73/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9956 - loss: 0.0467 \n",
      "Epoch 74/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9882 - loss: 0.0528 \n",
      "Epoch 75/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9923 - loss: 0.0515 \n",
      "Epoch 76/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9937 - loss: 0.0453 \n",
      "Epoch 77/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9923 - loss: 0.0408 \n",
      "Epoch 78/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9895 - loss: 0.0442 \n",
      "Epoch 79/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9822 - loss: 0.0756 \n",
      "Epoch 80/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9912 - loss: 0.0491 \n",
      "Epoch 81/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9918 - loss: 0.0462 \n",
      "Epoch 82/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9603 - loss: 0.0751 \n",
      "Epoch 83/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9935 - loss: 0.0468 \n",
      "Epoch 84/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9674 - loss: 0.0609 \n",
      "Epoch 85/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9883 - loss: 0.0518 \n",
      "Epoch 86/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9868 - loss: 0.0553 \n",
      "Epoch 87/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9880 - loss: 0.0504 \n",
      "Epoch 88/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9868 - loss: 0.0526 \n",
      "Epoch 89/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9913 - loss: 0.0528 \n",
      "Epoch 90/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9859 - loss: 0.0547 \n",
      "Epoch 91/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9935 - loss: 0.0480 \n",
      "Epoch 92/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9906 - loss: 0.0453 \n",
      "Epoch 93/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9925 - loss: 0.0390 \n",
      "Epoch 94/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9956 - loss: 0.0395 \n",
      "Epoch 95/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9870 - loss: 0.0365 \n",
      "Epoch 96/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9934 - loss: 0.0524 \n",
      "Epoch 97/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9844 - loss: 0.0614 \n",
      "Epoch 98/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9848 - loss: 0.0525 \n",
      "Epoch 99/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9857 - loss: 0.0554 \n",
      "Epoch 100/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9935 - loss: 0.0360 \n",
      "Epoch 101/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9818 - loss: 0.0466 \n",
      "Epoch 102/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9947 - loss: 0.0331 \n",
      "Epoch 103/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9929 - loss: 0.0409 \n",
      "Epoch 104/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9891 - loss: 0.0349 \n",
      "Epoch 105/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9919 - loss: 0.0386 \n",
      "Epoch 106/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9866 - loss: 0.0589 \n",
      "Epoch 107/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9811 - loss: 0.0609 \n",
      "Epoch 108/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9923 - loss: 0.0443 \n",
      "Epoch 109/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9844 - loss: 0.0530 \n",
      "Epoch 110/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9857 - loss: 0.0574 \n",
      "Epoch 111/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9891 - loss: 0.0422 \n",
      "Epoch 112/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9891 - loss: 0.0372 \n",
      "Epoch 113/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9811 - loss: 0.0640 \n",
      "Epoch 114/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9868 - loss: 0.0509 \n",
      "Epoch 115/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9773 - loss: 0.0635 \n",
      "Epoch 116/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9836 - loss: 0.0449 \n",
      "Epoch 117/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9905 - loss: 0.0354 \n",
      "Epoch 118/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9868 - loss: 0.0418 \n",
      "Epoch 119/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9917 - loss: 0.0395 \n",
      "Epoch 120/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9793 - loss: 0.0479 \n",
      "Epoch 121/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9820 - loss: 0.0427 \n",
      "Epoch 122/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9913 - loss: 0.0445 \n",
      "Epoch 123/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9937 - loss: 0.0387 \n",
      "Epoch 124/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9935 - loss: 0.0321 \n",
      "Epoch 125/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9829 - loss: 0.0588 \n",
      "Epoch 126/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9874 - loss: 0.0653 \n",
      "Epoch 127/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9945 - loss: 0.0394 \n",
      "Epoch 128/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9868 - loss: 0.0423 \n",
      "Epoch 129/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9809 - loss: 0.0426 \n",
      "Epoch 130/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9929 - loss: 0.0315 \n",
      "Epoch 131/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9962 - loss: 0.0392 \n",
      "Epoch 132/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9935 - loss: 0.0385 \n",
      "Epoch 133/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9907 - loss: 0.0442 \n",
      "Epoch 134/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9926 - loss: 0.0411 \n",
      "Epoch 135/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9880 - loss: 0.0369 \n",
      "Epoch 136/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9895 - loss: 0.0374 \n",
      "Epoch 137/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9900 - loss: 0.0327 \n",
      "Epoch 138/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9946 - loss: 0.0458 \n",
      "Epoch 139/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9899 - loss: 0.0368 \n",
      "Epoch 140/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9966 - loss: 0.0291 \n",
      "Epoch 141/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9935 - loss: 0.0336 \n",
      "Epoch 142/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9750 - loss: 0.0397 \n",
      "Epoch 143/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9820 - loss: 0.0435 \n",
      "Epoch 144/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9966 - loss: 0.0327 \n",
      "Epoch 145/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9872 - loss: 0.0427 \n",
      "Epoch 146/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9809 - loss: 0.0587 \n",
      "Epoch 147/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9848 - loss: 0.0401 \n",
      "Epoch 148/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9960 - loss: 0.0292 \n",
      "Epoch 149/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9749 - loss: 0.0635 \n",
      "Epoch 150/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9919 - loss: 0.0365 \n",
      "Epoch 151/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9874 - loss: 0.0619 \n",
      "Epoch 152/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9907 - loss: 0.0383 \n",
      "Epoch 153/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9855 - loss: 0.0539 \n",
      "Epoch 154/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9907 - loss: 0.0305 \n",
      "Epoch 155/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9835 - loss: 0.0563 \n",
      "Epoch 156/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9935 - loss: 0.0306 \n",
      "Epoch 157/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9966 - loss: 0.0251 \n",
      "Epoch 158/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9906 - loss: 0.0458 \n",
      "Epoch 159/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9773 - loss: 0.0517 \n",
      "Epoch 160/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9895 - loss: 0.0448 \n",
      "Epoch 161/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9895 - loss: 0.0482 \n",
      "Epoch 162/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9973 - loss: 0.0328 \n",
      "Epoch 163/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9874 - loss: 0.0607 \n",
      "Epoch 164/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9907 - loss: 0.0372 \n",
      "Epoch 165/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9966 - loss: 0.0277 \n",
      "Epoch 166/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9874 - loss: 0.0629 \n",
      "Epoch 167/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9891 - loss: 0.0364 \n",
      "Epoch 168/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9956 - loss: 0.0240 \n",
      "Epoch 169/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9784 - loss: 0.0400 \n",
      "Epoch 170/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9937 - loss: 0.0327 \n",
      "Epoch 171/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9820 - loss: 0.0567 \n",
      "Epoch 172/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9891 - loss: 0.0371 \n",
      "Epoch 173/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9883 - loss: 0.0461 \n",
      "Epoch 174/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9907 - loss: 0.0366 \n",
      "Epoch 175/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9848 - loss: 0.0341 \n",
      "Epoch 176/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9966 - loss: 0.0243 \n",
      "Epoch 177/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9794 - loss: 0.0531 \n",
      "Epoch 178/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9800 - loss: 0.0437 \n",
      "Epoch 179/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9868 - loss: 0.0389 \n",
      "Epoch 180/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9891 - loss: 0.0346 \n",
      "Epoch 181/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9874 - loss: 0.0570 \n",
      "Epoch 182/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9796 - loss: 0.0468 \n",
      "Epoch 183/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9834 - loss: 0.0575 \n",
      "Epoch 184/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9947 - loss: 0.0290 \n",
      "Epoch 185/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9809 - loss: 0.0379 \n",
      "Epoch 186/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9964 - loss: 0.0236 \n",
      "Epoch 187/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9922 - loss: 0.0408 \n",
      "Epoch 188/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9947 - loss: 0.0334 \n",
      "Epoch 189/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9831 - loss: 0.0327 \n",
      "Epoch 190/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9779 - loss: 0.0504 \n",
      "Epoch 191/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9835 - loss: 0.0500 \n",
      "Epoch 192/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9920 - loss: 0.0352 \n",
      "Epoch 193/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9973 - loss: 0.0242 \n",
      "Epoch 194/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9767 - loss: 0.0568 \n",
      "Epoch 195/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9895 - loss: 0.0427 \n",
      "Epoch 196/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9883 - loss: 0.0514 \n",
      "Epoch 197/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9882 - loss: 0.0350 \n",
      "Epoch 198/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9983 - loss: 0.0198 \n",
      "Epoch 199/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9905 - loss: 0.0413 \n",
      "Epoch 200/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9835 - loss: 0.0441 \n",
      "Epoch 201/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9887 - loss: 0.0386 \n",
      "Epoch 202/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9868 - loss: 0.0327 \n",
      "Epoch 203/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9966 - loss: 0.0209 \n",
      "Epoch 204/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9935 - loss: 0.0223 \n",
      "Epoch 205/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9729 - loss: 0.0553 \n",
      "Epoch 206/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9956 - loss: 0.0312 \n",
      "Epoch 207/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9962 - loss: 0.0266 \n",
      "Epoch 208/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9854 - loss: 0.0362 \n",
      "Epoch 209/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9891 - loss: 0.0319 \n",
      "Epoch 210/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9874 - loss: 0.0521 \n",
      "Epoch 211/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9945 - loss: 0.0309 \n",
      "Epoch 212/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9874 - loss: 0.0662 \n",
      "Epoch 213/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9909 - loss: 0.0325 \n",
      "Epoch 214/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9857 - loss: 0.0511 \n",
      "Epoch 215/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9855 - loss: 0.0664 \n",
      "Epoch 216/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9829 - loss: 0.0471 \n",
      "Epoch 217/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9796 - loss: 0.0547 \n",
      "Epoch 218/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9962 - loss: 0.0379 \n",
      "Epoch 219/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9835 - loss: 0.0608 \n",
      "Epoch 220/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9868 - loss: 0.0432 \n",
      "Epoch 221/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9922 - loss: 0.0331 \n",
      "Epoch 222/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9907 - loss: 0.0319 \n",
      "Epoch 223/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9934 - loss: 0.0331 \n",
      "Epoch 224/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9801 - loss: 0.0558 \n",
      "Epoch 225/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9895 - loss: 0.0444 \n",
      "Epoch 226/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9895 - loss: 0.0479 \n",
      "Epoch 227/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9836 - loss: 0.0507 \n",
      "Epoch 228/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9922 - loss: 0.0456 \n",
      "Epoch 229/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9946 - loss: 0.0447 \n",
      "Epoch 230/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9962 - loss: 0.0294 \n",
      "Epoch 231/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9983 - loss: 0.0242 \n",
      "Epoch 232/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9874 - loss: 0.0594 \n",
      "Epoch 233/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9749 - loss: 0.0590 \n",
      "Epoch 234/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9962 - loss: 0.0352 \n",
      "Epoch 235/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9983 - loss: 0.0378 \n",
      "Epoch 236/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9902 - loss: 0.0311 \n",
      "Epoch 237/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9960 - loss: 0.0282 \n",
      "Epoch 238/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0386 \n",
      "Epoch 239/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9883 - loss: 0.0377 \n",
      "Epoch 240/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9956 - loss: 0.0259 \n",
      "Epoch 241/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9848 - loss: 0.0319 \n",
      "Epoch 242/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9848 - loss: 0.0376 \n",
      "Epoch 243/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9857 - loss: 0.0584 \n",
      "Epoch 244/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9935 - loss: 0.0388 \n",
      "Epoch 245/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9796 - loss: 0.0532 \n",
      "Epoch 246/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9870 - loss: 0.0401 \n",
      "Epoch 247/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9946 - loss: 0.0398 \n",
      "Epoch 248/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9945 - loss: 0.0236 \n",
      "Epoch 249/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9791 - loss: 0.0429 \n",
      "Epoch 250/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9749 - loss: 0.0637 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7464744c8490>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, epochs=250, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "0.9473684210526315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      0.87      0.93        15\n",
      "           2       0.86      1.00      0.92        12\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.95      0.96      0.95        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "yy_pred = np.argmax(y_pred,axis=1)\n",
    "yy_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "print(accuracy_score(yy_test,yy_pred))\n",
    "print(classification_report(yy_test,yy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100. ,   0.1,   0. ],\n",
       "       [  0.2,  99.6,   0.1],\n",
       "       [  0. ,   1.9,  98.8],\n",
       "       [  0. ,  95.7,   4.3],\n",
       "       [  0. ,  14.3,  88.5],\n",
       "       [  0. ,   0. , 100. ],\n",
       "       [100. ,   0.1,   0. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [100. ,   0.1,   0. ],\n",
       "       [  0. ,  99.3,   0.4],\n",
       "       [ 99.9,   0.1,   0. ],\n",
       "       [  0. ,  99.8,   0.1],\n",
       "       [  0. ,  97. ,   2.7],\n",
       "       [  0. ,  99.9,   0. ],\n",
       "       [  0. ,  19. ,  87.2],\n",
       "       [  0. ,  97.8,   1.8],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [100. ,   0.1,   0. ],\n",
       "       [  0. ,  96.9,   2.3],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [  0. ,   1.6,  99. ],\n",
       "       [  0. ,   0. , 100. ],\n",
       "       [  0. ,   0.2,  99.9],\n",
       "       [  0. ,   0.2,  99.9],\n",
       "       [  0. ,  99.9,   0. ],\n",
       "       [  0. ,  99.8,   0.1],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [  1.2,  98.5,   0. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [  0. ,   0.3,  99.9],\n",
       "       [  0. ,   0. , 100. ],\n",
       "       [  0. ,  24.7,  84.3],\n",
       "       [  0. ,  17.8,  84.7],\n",
       "       [  0. ,  88.5,   9.3],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [  0. ,   0.6,  99.6],\n",
       "       [  0. ,   0. , 100. ],\n",
       "       [  0. ,  90. ,  10.7]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(100*y_pred,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Large Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "x_digits = digits['data']\n",
    "y_digits = digits['target']\n",
    "\n",
    "labeler = LabelBinarizer()\n",
    "\n",
    "yy_digits = labeler.fit_transform(y_digits)\n",
    "xx_digits = x_digits.reshape(1797,8,8,1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(xx_digits, yy_digits)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(8,8,1)))\n",
    "model.add(Conv2D(32, (3, 2), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - binary_accuracy: 0.8934 - loss: 0.5095   \n",
      "Epoch 2/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9543 - loss: 0.1546 \n",
      "Epoch 3/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9818 - loss: 0.0891 \n",
      "Epoch 4/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9872 - loss: 0.0641 \n",
      "Epoch 5/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9916 - loss: 0.0470 \n",
      "Epoch 6/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9947 - loss: 0.0350 \n",
      "Epoch 7/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9948 - loss: 0.0331 \n",
      "Epoch 8/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9966 - loss: 0.0249 \n",
      "Epoch 9/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9969 - loss: 0.0232 \n",
      "Epoch 10/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9982 - loss: 0.0171 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x74647439d150>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "yy_pred = np.argmax(y_pred,axis=1)\n",
    "yy_test = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        47\n",
      "           1       0.92      1.00      0.96        46\n",
      "           2       0.98      1.00      0.99        45\n",
      "           3       1.00      0.97      0.99        40\n",
      "           4       1.00      0.96      0.98        45\n",
      "           5       0.98      0.98      0.98        41\n",
      "           6       1.00      0.98      0.99        58\n",
      "           7       0.92      1.00      0.96        45\n",
      "           8       0.97      0.87      0.92        39\n",
      "           9       0.98      0.93      0.95        44\n",
      "\n",
      "    accuracy                           0.97       450\n",
      "   macro avg       0.97      0.97      0.97       450\n",
      "weighted avg       0.97      0.97      0.97       450\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[47,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 46,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 45,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 39,  0,  0,  0,  1,  0,  0],\n",
       "       [ 0,  0,  0,  0, 43,  0,  0,  1,  1,  0],\n",
       "       [ 0,  0,  0,  0,  0, 40,  0,  0,  0,  1],\n",
       "       [ 1,  0,  0,  0,  0,  0, 57,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 45,  0,  0],\n",
       "       [ 0,  3,  1,  0,  0,  0,  0,  1, 34,  0],\n",
       "       [ 0,  1,  0,  0,  0,  1,  0,  1,  0, 41]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(yy_test,yy_pred))\n",
    "confusion_matrix(yy_test,yy_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yet Another Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces(data_home='/home/kaygun/local/data/scikit_learn_data/')\n",
    "binarizer = LabelBinarizer()\n",
    "\n",
    "y = binarizer.fit_transform(faces.target.flatten()).reshape(-1,40)\n",
    "X = faces.data.flatten().reshape(-1,4096)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 3.8557 - val_loss: 3.7614\n",
      "Epoch 2/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.7156 - val_loss: 3.7500\n",
      "Epoch 3/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.6849 - val_loss: 3.7238\n",
      "Epoch 4/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.6453 - val_loss: 3.7417\n",
      "Epoch 5/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.6530 - val_loss: 3.7079\n",
      "Epoch 6/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.6210 - val_loss: 3.7051\n",
      "Epoch 7/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.6041 - val_loss: 3.6508\n",
      "Epoch 8/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.5595 - val_loss: 3.6577\n",
      "Epoch 9/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.5260 - val_loss: 3.6225\n",
      "Epoch 10/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.4379 - val_loss: 3.6206\n",
      "Epoch 11/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.3919 - val_loss: 3.5736\n",
      "Epoch 12/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.3508 - val_loss: 3.5221\n",
      "Epoch 13/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.2654 - val_loss: 3.4961\n",
      "Epoch 14/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.1707 - val_loss: 3.4334\n",
      "Epoch 15/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.0609 - val_loss: 3.4261\n",
      "Epoch 16/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.9910 - val_loss: 3.3701\n",
      "Epoch 17/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.8056 - val_loss: 3.2107\n",
      "Epoch 18/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.8068 - val_loss: 3.2030\n",
      "Epoch 19/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.8118 - val_loss: 3.1661\n",
      "Epoch 20/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.6480 - val_loss: 3.0370\n",
      "Epoch 21/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.5130 - val_loss: 2.9679\n",
      "Epoch 22/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.3970 - val_loss: 2.7892\n",
      "Epoch 23/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.2676 - val_loss: 2.7232\n",
      "Epoch 24/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.1345 - val_loss: 2.6429\n",
      "Epoch 25/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.0527 - val_loss: 2.5372\n",
      "Epoch 26/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.0090 - val_loss: 2.6154\n",
      "Epoch 27/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.0478 - val_loss: 2.3516\n",
      "Epoch 28/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.8846 - val_loss: 2.5099\n",
      "Epoch 29/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.8874 - val_loss: 2.4404\n",
      "Epoch 30/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.8526 - val_loss: 2.3349\n",
      "Epoch 31/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.6004 - val_loss: 2.1406\n",
      "Epoch 32/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.5559 - val_loss: 2.0823\n",
      "Epoch 33/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.5136 - val_loss: 1.9945\n",
      "Epoch 34/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.4560 - val_loss: 1.9872\n",
      "Epoch 35/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.3561 - val_loss: 1.8976\n",
      "Epoch 36/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.2937 - val_loss: 1.9383\n",
      "Epoch 37/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.2656 - val_loss: 1.8089\n",
      "Epoch 38/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.1928 - val_loss: 1.8333\n",
      "Epoch 39/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.2124 - val_loss: 1.6817\n",
      "Epoch 40/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.1754 - val_loss: 1.7444\n",
      "Epoch 41/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0898 - val_loss: 1.8954\n",
      "Epoch 42/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.1661 - val_loss: 1.6258\n",
      "Epoch 43/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.0236 - val_loss: 1.5775\n",
      "Epoch 44/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.9823 - val_loss: 1.5271\n",
      "Epoch 45/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.9325 - val_loss: 1.5899\n",
      "Epoch 46/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.9351 - val_loss: 1.4965\n",
      "Epoch 47/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.9486 - val_loss: 1.6594\n",
      "Epoch 48/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.9276 - val_loss: 1.3962\n",
      "Epoch 49/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.8072 - val_loss: 1.3719\n",
      "Epoch 50/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.7892 - val_loss: 1.3340\n",
      "Epoch 51/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.7348 - val_loss: 1.4174\n",
      "Epoch 52/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.7056 - val_loss: 1.2974\n",
      "Epoch 53/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6483 - val_loss: 1.2946\n",
      "Epoch 54/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6738 - val_loss: 1.2811\n",
      "Epoch 55/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6421 - val_loss: 1.2574\n",
      "Epoch 56/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5886 - val_loss: 1.3435\n",
      "Epoch 57/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.6407 - val_loss: 1.3000\n",
      "Epoch 58/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6259 - val_loss: 1.1782\n",
      "Epoch 59/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.6541 - val_loss: 1.2816\n",
      "Epoch 60/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5959 - val_loss: 1.2011\n",
      "Epoch 61/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.5465 - val_loss: 1.3642\n",
      "Epoch 62/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5504 - val_loss: 1.1415\n",
      "Epoch 63/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4753 - val_loss: 1.1269\n",
      "Epoch 64/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5299 - val_loss: 1.1253\n",
      "Epoch 65/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4899 - val_loss: 1.2015\n",
      "Epoch 66/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4552 - val_loss: 1.0461\n",
      "Epoch 67/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4329 - val_loss: 1.0847\n",
      "Epoch 68/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4229 - val_loss: 1.0464\n",
      "Epoch 69/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3866 - val_loss: 1.0487\n",
      "Epoch 70/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3554 - val_loss: 1.0414\n",
      "Epoch 71/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3699 - val_loss: 1.0514\n",
      "Epoch 72/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3684 - val_loss: 0.9718\n",
      "Epoch 73/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.3410 - val_loss: 1.0389\n",
      "Epoch 74/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3370 - val_loss: 1.1203\n",
      "Epoch 75/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4152 - val_loss: 1.0395\n",
      "Epoch 76/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3338 - val_loss: 0.9829\n",
      "Epoch 77/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3305 - val_loss: 1.0794\n",
      "Epoch 78/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3016 - val_loss: 0.9254\n",
      "Epoch 79/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2854 - val_loss: 1.1069\n",
      "Epoch 80/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2801 - val_loss: 0.9644\n",
      "Epoch 81/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2324 - val_loss: 0.9176\n",
      "Epoch 82/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2488 - val_loss: 1.0265\n",
      "Epoch 83/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2319 - val_loss: 0.8745\n",
      "Epoch 84/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2049 - val_loss: 0.8632\n",
      "Epoch 85/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1958 - val_loss: 0.9040\n",
      "Epoch 86/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1787 - val_loss: 0.8858\n",
      "Epoch 87/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1853 - val_loss: 0.8663\n",
      "Epoch 88/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1791 - val_loss: 0.9516\n",
      "Epoch 89/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1959 - val_loss: 0.8531\n",
      "Epoch 90/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1723 - val_loss: 0.8360\n",
      "Epoch 91/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1703 - val_loss: 0.9231\n",
      "Epoch 92/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1651 - val_loss: 0.8824\n",
      "Epoch 93/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1688 - val_loss: 0.8026\n",
      "Epoch 94/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1578 - val_loss: 0.9015\n",
      "Epoch 95/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1537 - val_loss: 1.0015\n",
      "Epoch 96/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1802 - val_loss: 0.8314\n",
      "Epoch 97/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1324 - val_loss: 0.8865\n",
      "Epoch 98/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1432 - val_loss: 0.8667\n",
      "Epoch 99/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1179 - val_loss: 0.8807\n",
      "Epoch 100/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1209 - val_loss: 0.7864\n",
      "Epoch 101/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1011 - val_loss: 0.9000\n",
      "Epoch 102/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1167 - val_loss: 0.9633\n",
      "Epoch 103/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1089 - val_loss: 0.8466\n",
      "Epoch 104/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1087 - val_loss: 0.8391\n",
      "Epoch 105/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0834 - val_loss: 0.8227\n",
      "Epoch 106/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0799 - val_loss: 0.7604\n",
      "Epoch 107/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0921 - val_loss: 0.8680\n",
      "Epoch 108/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0826 - val_loss: 0.8166\n",
      "Epoch 109/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0733 - val_loss: 0.7585\n",
      "Epoch 110/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0655 - val_loss: 0.7900\n",
      "Epoch 111/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0721 - val_loss: 0.8091\n",
      "Epoch 112/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0637 - val_loss: 0.8069\n",
      "Epoch 113/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0702 - val_loss: 0.7593\n",
      "Epoch 114/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0671 - val_loss: 0.7855\n",
      "Epoch 115/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0611 - val_loss: 0.7671\n",
      "Epoch 116/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0573 - val_loss: 0.8003\n",
      "Epoch 117/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0563 - val_loss: 0.7488\n",
      "Epoch 118/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0500 - val_loss: 0.8023\n",
      "Epoch 119/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0603 - val_loss: 0.7861\n",
      "Epoch 120/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0630 - val_loss: 0.8659\n",
      "Epoch 121/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0559 - val_loss: 0.7749\n",
      "Epoch 122/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0559 - val_loss: 0.8315\n",
      "Epoch 123/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0562 - val_loss: 0.7821\n",
      "Epoch 124/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0485 - val_loss: 0.8284\n",
      "Epoch 125/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0460 - val_loss: 0.7567\n",
      "Epoch 126/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0431 - val_loss: 0.8141\n",
      "Epoch 127/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0443 - val_loss: 0.7642\n",
      "Epoch 128/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0384 - val_loss: 0.7856\n",
      "Epoch 129/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0400 - val_loss: 0.7628\n",
      "Epoch 130/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0400 - val_loss: 0.7902\n",
      "Epoch 131/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0327 - val_loss: 0.7650\n",
      "Epoch 132/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0349 - val_loss: 0.7583\n",
      "Epoch 133/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0350 - val_loss: 0.7576\n",
      "Epoch 134/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0326 - val_loss: 0.7920\n",
      "Epoch 135/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0316 - val_loss: 0.7389\n",
      "Epoch 136/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0313 - val_loss: 0.7693\n",
      "Epoch 137/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0296 - val_loss: 0.7500\n",
      "Epoch 138/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0277 - val_loss: 0.7706\n",
      "Epoch 139/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0264 - val_loss: 0.7670\n",
      "Epoch 140/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0285 - val_loss: 0.7705\n",
      "Epoch 141/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0265 - val_loss: 0.7500\n",
      "Epoch 142/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0306 - val_loss: 0.7686\n",
      "Epoch 143/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0234 - val_loss: 0.7669\n",
      "Epoch 144/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0290 - val_loss: 0.7867\n",
      "Epoch 145/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0272 - val_loss: 0.7698\n",
      "Epoch 146/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0267 - val_loss: 0.7936\n",
      "Epoch 147/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0231 - val_loss: 0.7543\n",
      "Epoch 148/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0222 - val_loss: 0.7817\n",
      "Epoch 149/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0289 - val_loss: 0.7743\n",
      "Epoch 150/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0236 - val_loss: 0.7834\n",
      "Epoch 151/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0212 - val_loss: 0.7578\n",
      "Epoch 152/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0232 - val_loss: 0.7714\n",
      "Epoch 153/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0202 - val_loss: 0.7443\n",
      "Epoch 154/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0214 - val_loss: 0.7791\n",
      "Epoch 155/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0206 - val_loss: 0.7636\n",
      "Epoch 156/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0190 - val_loss: 0.7612\n",
      "Epoch 157/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0193 - val_loss: 0.7763\n",
      "Epoch 158/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0211 - val_loss: 0.7585\n",
      "Epoch 159/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0173 - val_loss: 0.7610\n",
      "Epoch 160/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0164 - val_loss: 0.7576\n",
      "Epoch 161/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0179 - val_loss: 0.7661\n",
      "Epoch 162/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0178 - val_loss: 0.7876\n",
      "Epoch 163/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0171 - val_loss: 0.7676\n",
      "Epoch 164/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0165 - val_loss: 0.7617\n",
      "Epoch 165/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0155 - val_loss: 0.7766\n",
      "Epoch 166/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0156 - val_loss: 0.7552\n",
      "Epoch 167/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0156 - val_loss: 0.7700\n",
      "Epoch 168/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0145 - val_loss: 0.7765\n",
      "Epoch 169/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0150 - val_loss: 0.7668\n",
      "Epoch 170/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0141 - val_loss: 0.7736\n",
      "Epoch 171/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0152 - val_loss: 0.7631\n",
      "Epoch 172/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0136 - val_loss: 0.7593\n",
      "Epoch 173/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0131 - val_loss: 0.7705\n",
      "Epoch 174/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0141 - val_loss: 0.7721\n",
      "Epoch 175/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0139 - val_loss: 0.7718\n",
      "Epoch 176/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0137 - val_loss: 0.7801\n",
      "Epoch 177/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0135 - val_loss: 0.7606\n",
      "Epoch 178/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0137 - val_loss: 0.7813\n",
      "Epoch 179/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0120 - val_loss: 0.7560\n",
      "Epoch 180/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0126 - val_loss: 0.7685\n",
      "Epoch 181/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0113 - val_loss: 0.7780\n",
      "Epoch 182/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0114 - val_loss: 0.7685\n",
      "Epoch 183/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0116 - val_loss: 0.7602\n",
      "Epoch 184/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0117 - val_loss: 0.7688\n",
      "Epoch 185/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0113 - val_loss: 0.7716\n",
      "Epoch 186/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0107 - val_loss: 0.7648\n",
      "Epoch 187/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0104 - val_loss: 0.7559\n",
      "Epoch 188/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0102 - val_loss: 0.7715\n",
      "Epoch 189/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0099 - val_loss: 0.7689\n",
      "Epoch 190/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0100 - val_loss: 0.7603\n",
      "Epoch 191/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0104 - val_loss: 0.7765\n",
      "Epoch 192/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0101 - val_loss: 0.7717\n",
      "Epoch 193/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0100 - val_loss: 0.7575\n",
      "Epoch 194/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0104 - val_loss: 0.7748\n",
      "Epoch 195/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0095 - val_loss: 0.7716\n",
      "Epoch 196/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0100 - val_loss: 0.7683\n",
      "Epoch 197/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0093 - val_loss: 0.7703\n",
      "Epoch 198/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0089 - val_loss: 0.7635\n",
      "Epoch 199/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0088 - val_loss: 0.7778\n",
      "Epoch 200/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0087 - val_loss: 0.7805\n",
      "Epoch 201/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0085 - val_loss: 0.7794\n",
      "Epoch 202/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0082 - val_loss: 0.7658\n",
      "Epoch 203/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0084 - val_loss: 0.7738\n",
      "Epoch 204/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0083 - val_loss: 0.7615\n",
      "Epoch 205/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0084 - val_loss: 0.7709\n",
      "Epoch 206/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0080 - val_loss: 0.7675\n",
      "Epoch 207/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0082 - val_loss: 0.7621\n",
      "Epoch 208/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0080 - val_loss: 0.7705\n",
      "Epoch 209/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0075 - val_loss: 0.7699\n",
      "Epoch 210/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0079 - val_loss: 0.7663\n",
      "Epoch 211/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0077 - val_loss: 0.7685\n",
      "Epoch 212/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0074 - val_loss: 0.7722\n",
      "Epoch 213/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0070 - val_loss: 0.7759\n",
      "Epoch 214/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0075 - val_loss: 0.7620\n",
      "Epoch 215/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0066 - val_loss: 0.7684\n",
      "Epoch 216/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0066 - val_loss: 0.7829\n",
      "Epoch 217/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0067 - val_loss: 0.7726\n",
      "Epoch 218/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0072 - val_loss: 0.7617\n",
      "Epoch 219/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0069 - val_loss: 0.7679\n",
      "Epoch 220/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0072 - val_loss: 0.7605\n",
      "Epoch 221/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0063 - val_loss: 0.7708\n",
      "Epoch 222/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0071 - val_loss: 0.7751\n",
      "Epoch 223/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0064 - val_loss: 0.7718\n",
      "Epoch 224/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0058 - val_loss: 0.7699\n",
      "Epoch 225/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0060 - val_loss: 0.7726\n",
      "Epoch 226/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0058 - val_loss: 0.7707\n",
      "Epoch 227/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0064 - val_loss: 0.7801\n",
      "Epoch 228/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0059 - val_loss: 0.7712\n",
      "Epoch 229/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0056 - val_loss: 0.7718\n",
      "Epoch 230/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0055 - val_loss: 0.7710\n",
      "Epoch 231/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0055 - val_loss: 0.7660\n",
      "Epoch 232/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0060 - val_loss: 0.7890\n",
      "Epoch 233/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0055 - val_loss: 0.7663\n",
      "Epoch 234/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0052 - val_loss: 0.7789\n",
      "Epoch 235/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0056 - val_loss: 0.7830\n",
      "Epoch 236/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0057 - val_loss: 0.7786\n",
      "Epoch 237/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0055 - val_loss: 0.7728\n",
      "Epoch 238/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0055 - val_loss: 0.7762\n",
      "Epoch 239/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0052 - val_loss: 0.7749\n",
      "Epoch 240/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0050 - val_loss: 0.7741\n",
      "Epoch 241/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0050 - val_loss: 0.7803\n",
      "Epoch 242/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0052 - val_loss: 0.7869\n",
      "Epoch 243/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0049 - val_loss: 0.7772\n",
      "Epoch 244/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0052 - val_loss: 0.7755\n",
      "Epoch 245/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0053 - val_loss: 0.7741\n",
      "Epoch 246/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0051 - val_loss: 0.7800\n",
      "Epoch 247/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0046 - val_loss: 0.7835\n",
      "Epoch 248/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0045 - val_loss: 0.7803\n",
      "Epoch 249/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0048 - val_loss: 0.7762\n",
      "Epoch 250/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0049 - val_loss: 0.7732\n",
      "Epoch 251/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0045 - val_loss: 0.7789\n",
      "Epoch 252/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0045 - val_loss: 0.7872\n",
      "Epoch 253/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0044 - val_loss: 0.7822\n",
      "Epoch 254/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0043 - val_loss: 0.7786\n",
      "Epoch 255/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0042 - val_loss: 0.7728\n",
      "Epoch 256/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0043 - val_loss: 0.7775\n",
      "Epoch 257/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0041 - val_loss: 0.7850\n",
      "Epoch 258/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - val_loss: 0.7850\n",
      "Epoch 259/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - val_loss: 0.7775\n",
      "Epoch 260/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0041 - val_loss: 0.7865\n",
      "Epoch 261/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0042 - val_loss: 0.7771\n",
      "Epoch 262/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0043 - val_loss: 0.7731\n",
      "Epoch 263/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0040 - val_loss: 0.7816\n",
      "Epoch 264/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0040 - val_loss: 0.7949\n",
      "Epoch 265/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0038 - val_loss: 0.7800\n",
      "Epoch 266/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0040 - val_loss: 0.7778\n",
      "Epoch 267/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0039 - val_loss: 0.7936\n",
      "Epoch 268/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0037 - val_loss: 0.7907\n",
      "Epoch 269/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0037 - val_loss: 0.7816\n",
      "Epoch 270/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - val_loss: 0.7860\n",
      "Epoch 271/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.7834\n",
      "Epoch 272/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.7796\n",
      "Epoch 273/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.7776\n",
      "Epoch 274/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.7834\n",
      "Epoch 275/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.7911\n",
      "Epoch 276/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.7825\n",
      "Epoch 277/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.7840\n",
      "Epoch 278/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.7849\n",
      "Epoch 279/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0033 - val_loss: 0.7874\n",
      "Epoch 280/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.7858\n",
      "Epoch 281/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.7836\n",
      "Epoch 282/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.7867\n",
      "Epoch 283/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0033 - val_loss: 0.7875\n",
      "Epoch 284/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.7868\n",
      "Epoch 285/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0032 - val_loss: 0.7868\n",
      "Epoch 286/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.7905\n",
      "Epoch 287/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0030 - val_loss: 0.7828\n",
      "Epoch 288/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0032 - val_loss: 0.7840\n",
      "Epoch 289/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0032 - val_loss: 0.7897\n",
      "Epoch 290/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0031 - val_loss: 0.7859\n",
      "Epoch 291/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0030 - val_loss: 0.7876\n",
      "Epoch 292/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030 - val_loss: 0.7896\n",
      "Epoch 293/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0032 - val_loss: 0.7927\n",
      "Epoch 294/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0031 - val_loss: 0.7901\n",
      "Epoch 295/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0031 - val_loss: 0.7892\n",
      "Epoch 296/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.7920\n",
      "Epoch 297/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0030 - val_loss: 0.7846\n",
      "Epoch 298/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028 - val_loss: 0.7921\n",
      "Epoch 299/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0029 - val_loss: 0.7976\n",
      "Epoch 300/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028 - val_loss: 0.7883\n",
      "Epoch 301/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0029 - val_loss: 0.7883\n",
      "Epoch 302/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0029 - val_loss: 0.7904\n",
      "Epoch 303/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.7916\n",
      "Epoch 304/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.7973\n",
      "Epoch 305/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.7956\n",
      "Epoch 306/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0026 - val_loss: 0.7935\n",
      "Epoch 307/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0027 - val_loss: 0.7937\n",
      "Epoch 308/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028 - val_loss: 0.7917\n",
      "Epoch 309/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.7887\n",
      "Epoch 310/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.7935\n",
      "Epoch 311/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.7932\n",
      "Epoch 312/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.7939\n",
      "Epoch 313/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.7947\n",
      "Epoch 314/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0024 - val_loss: 0.7934\n",
      "Epoch 315/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.7961\n",
      "Epoch 316/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.7969\n",
      "Epoch 317/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0024 - val_loss: 0.7896\n",
      "Epoch 318/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.7997\n",
      "Epoch 319/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.7983\n",
      "Epoch 320/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0024 - val_loss: 0.7979\n",
      "Epoch 321/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.7999\n",
      "Epoch 322/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.7958\n",
      "Epoch 323/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0024 - val_loss: 0.7889\n",
      "Epoch 324/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.7952\n",
      "Epoch 325/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.7959\n",
      "Epoch 326/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.7948\n",
      "Epoch 327/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0024 - val_loss: 0.7921\n",
      "Epoch 328/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.7975\n",
      "Epoch 329/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.8041\n",
      "Epoch 330/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.8000\n",
      "Epoch 331/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.7942\n",
      "Epoch 332/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.7961\n",
      "Epoch 333/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.7982\n",
      "Epoch 334/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.8004\n",
      "Epoch 335/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.8090\n",
      "Epoch 336/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.8038\n",
      "Epoch 337/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.7973\n",
      "Epoch 338/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.8019\n",
      "Epoch 339/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.8055\n",
      "Epoch 340/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.7998\n",
      "Epoch 341/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.7970\n",
      "Epoch 342/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.8007\n",
      "Epoch 343/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.8047\n",
      "Epoch 344/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.7991\n",
      "Epoch 345/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.8040\n",
      "Epoch 346/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.8062\n",
      "Epoch 347/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.8002\n",
      "Epoch 348/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.7992\n",
      "Epoch 349/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.8045\n",
      "Epoch 350/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.8057\n",
      "Epoch 351/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.7984\n",
      "Epoch 352/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.7970\n",
      "Epoch 353/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.8021\n",
      "Epoch 354/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.8107\n",
      "Epoch 355/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.8000\n",
      "Epoch 356/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.8005\n",
      "Epoch 357/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.8035\n",
      "Epoch 358/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.8051\n",
      "Epoch 359/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.8034\n",
      "Epoch 360/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.8005\n",
      "Epoch 361/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.8070\n",
      "Epoch 362/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.8080\n",
      "Epoch 363/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.8078\n",
      "Epoch 364/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.7968\n",
      "Epoch 365/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.8084\n",
      "Epoch 366/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.8066\n",
      "Epoch 367/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.8095\n",
      "Epoch 368/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.8050\n",
      "Epoch 369/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.8099\n",
      "Epoch 370/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.8046\n",
      "Epoch 371/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.8045\n",
      "Epoch 372/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.8047\n",
      "Epoch 373/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.8059\n",
      "Epoch 374/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0016 - val_loss: 0.8059\n",
      "Epoch 375/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.8100\n",
      "Epoch 376/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.8043\n",
      "Epoch 377/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.8018\n",
      "Epoch 378/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.8136\n",
      "Epoch 379/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.8140\n",
      "Epoch 380/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0015 - val_loss: 0.8084\n",
      "Epoch 381/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.8106\n",
      "Epoch 382/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.8123\n",
      "Epoch 383/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.8064\n",
      "Epoch 384/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.8082\n",
      "Epoch 385/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.8115\n",
      "Epoch 386/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0014 - val_loss: 0.8133\n",
      "Epoch 387/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.8115\n",
      "Epoch 388/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0014 - val_loss: 0.8128\n",
      "Epoch 389/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.8123\n",
      "Epoch 390/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.8111\n",
      "Epoch 391/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.8118\n",
      "Epoch 392/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.8144\n",
      "Epoch 393/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0014 - val_loss: 0.8098\n",
      "Epoch 394/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0014 - val_loss: 0.8098\n",
      "Epoch 395/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.8135\n",
      "Epoch 396/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.8127\n",
      "Epoch 397/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.8100\n",
      "Epoch 398/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.8125\n",
      "Epoch 399/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.8150\n",
      "Epoch 400/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.8137\n",
      "Epoch 401/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.8133\n",
      "Epoch 402/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.8177\n",
      "Epoch 403/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - val_loss: 0.8165\n",
      "Epoch 404/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.8126\n",
      "Epoch 405/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0014 - val_loss: 0.8137\n",
      "Epoch 406/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.8165\n",
      "Epoch 407/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.8124\n",
      "Epoch 408/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.8089\n",
      "Epoch 409/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0014 - val_loss: 0.8173\n",
      "Epoch 410/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0013 - val_loss: 0.8148\n",
      "Epoch 411/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.8127\n",
      "Epoch 412/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0013 - val_loss: 0.8130\n",
      "Epoch 413/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.8110\n",
      "Epoch 414/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.8196\n",
      "Epoch 415/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.8176\n",
      "Epoch 416/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.8138\n",
      "Epoch 417/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.8149\n",
      "Epoch 418/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0012 - val_loss: 0.8203\n",
      "Epoch 419/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.8173\n",
      "Epoch 420/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.8197\n",
      "Epoch 421/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.8208\n",
      "Epoch 422/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.8211\n",
      "Epoch 423/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.8167\n",
      "Epoch 424/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - val_loss: 0.8174\n",
      "Epoch 425/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.8188\n",
      "Epoch 426/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.8210\n",
      "Epoch 427/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.8237\n",
      "Epoch 428/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.8180\n",
      "Epoch 429/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.8188\n",
      "Epoch 430/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.8235\n",
      "Epoch 431/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.8226\n",
      "Epoch 432/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.8243\n",
      "Epoch 433/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.8208\n",
      "Epoch 434/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.8221\n",
      "Epoch 435/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.8233\n",
      "Epoch 436/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0011 - val_loss: 0.8232\n",
      "Epoch 437/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.8232\n",
      "Epoch 438/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.8217\n",
      "Epoch 439/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.8216\n",
      "Epoch 440/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0010 - val_loss: 0.8223\n",
      "Epoch 441/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.8237\n",
      "Epoch 442/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.8238\n",
      "Epoch 443/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0010 - val_loss: 0.8239\n",
      "Epoch 444/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.8241\n",
      "Epoch 445/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.8178\n",
      "Epoch 446/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.8233\n",
      "Epoch 447/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.2969e-04 - val_loss: 0.8251\n",
      "Epoch 448/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0010 - val_loss: 0.8204\n",
      "Epoch 449/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.8289\n",
      "Epoch 450/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0010 - val_loss: 0.8289\n",
      "Epoch 451/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.7878e-04 - val_loss: 0.8232\n",
      "Epoch 452/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.9875e-04 - val_loss: 0.8178\n",
      "Epoch 453/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.5340e-04 - val_loss: 0.8218\n",
      "Epoch 454/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.2894e-04 - val_loss: 0.8252\n",
      "Epoch 455/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.3551e-04 - val_loss: 0.8262\n",
      "Epoch 456/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.2987e-04 - val_loss: 0.8252\n",
      "Epoch 457/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.1958e-04 - val_loss: 0.8266\n",
      "Epoch 458/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.2099e-04 - val_loss: 0.8286\n",
      "Epoch 459/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.0428e-04 - val_loss: 0.8287\n",
      "Epoch 460/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.7587e-04 - val_loss: 0.8281\n",
      "Epoch 461/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.3729e-04 - val_loss: 0.8273\n",
      "Epoch 462/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.8886e-04 - val_loss: 0.8274\n",
      "Epoch 463/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.0729e-04 - val_loss: 0.8275\n",
      "Epoch 464/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.0305e-04 - val_loss: 0.8245\n",
      "Epoch 465/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.5338e-04 - val_loss: 0.8254\n",
      "Epoch 466/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.7293e-04 - val_loss: 0.8261\n",
      "Epoch 467/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.1983e-04 - val_loss: 0.8284\n",
      "Epoch 468/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.6999e-04 - val_loss: 0.8241\n",
      "Epoch 469/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.6941e-04 - val_loss: 0.8289\n",
      "Epoch 470/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.3833e-04 - val_loss: 0.8305\n",
      "Epoch 471/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.4303e-04 - val_loss: 0.8271\n",
      "Epoch 472/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.4394e-04 - val_loss: 0.8277\n",
      "Epoch 473/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.2260e-04 - val_loss: 0.8260\n",
      "Epoch 474/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.3589e-04 - val_loss: 0.8274\n",
      "Epoch 475/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.5250e-04 - val_loss: 0.8296\n",
      "Epoch 476/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.5405e-04 - val_loss: 0.8314\n",
      "Epoch 477/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.1029e-04 - val_loss: 0.8274\n",
      "Epoch 478/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.0424e-04 - val_loss: 0.8346\n",
      "Epoch 479/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.3091e-04 - val_loss: 0.8325\n",
      "Epoch 480/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.7302e-04 - val_loss: 0.8294\n",
      "Epoch 481/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.5043e-04 - val_loss: 0.8297\n",
      "Epoch 482/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.9797e-04 - val_loss: 0.8306\n",
      "Epoch 483/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.3049e-04 - val_loss: 0.8317\n",
      "Epoch 484/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.3556e-04 - val_loss: 0.8324\n",
      "Epoch 485/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.0365e-04 - val_loss: 0.8347\n",
      "Epoch 486/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.7150e-04 - val_loss: 0.8345\n",
      "Epoch 487/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.7617e-04 - val_loss: 0.8310\n",
      "Epoch 488/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.3690e-04 - val_loss: 0.8326\n",
      "Epoch 489/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.1317e-04 - val_loss: 0.8329\n",
      "Epoch 490/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.8047e-04 - val_loss: 0.8335\n",
      "Epoch 491/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.5781e-04 - val_loss: 0.8361\n",
      "Epoch 492/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.4397e-04 - val_loss: 0.8320\n",
      "Epoch 493/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.0483e-04 - val_loss: 0.8342\n",
      "Epoch 494/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.5860e-04 - val_loss: 0.8375\n",
      "Epoch 495/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.9734e-04 - val_loss: 0.8339\n",
      "Epoch 496/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.9357e-04 - val_loss: 0.8325\n",
      "Epoch 497/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.4457e-04 - val_loss: 0.8343\n",
      "Epoch 498/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.4012e-04 - val_loss: 0.8384\n",
      "Epoch 499/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.0001e-04 - val_loss: 0.8327\n",
      "Epoch 500/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.5580e-04 - val_loss: 0.8304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x74647422e850>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(4096,)))\n",
    "model.add(Dense(128, activation=\"relu\",))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(40, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=500, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      0.67      0.80         3\n",
      "           2       0.67      0.67      0.67         3\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.67      0.67      0.67         3\n",
      "           5       0.88      1.00      0.93         7\n",
      "           7       0.75      1.00      0.86         3\n",
      "           8       1.00      1.00      1.00         1\n",
      "           9       1.00      0.67      0.80         3\n",
      "          10       1.00      1.00      1.00         1\n",
      "          11       1.00      1.00      1.00         2\n",
      "          12       0.50      0.33      0.40         3\n",
      "          13       0.67      1.00      0.80         4\n",
      "          14       1.00      1.00      1.00         2\n",
      "          15       0.43      0.75      0.55         4\n",
      "          16       0.80      1.00      0.89         4\n",
      "          17       0.50      1.00      0.67         2\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      0.20      0.33         5\n",
      "          20       0.67      0.67      0.67         3\n",
      "          21       1.00      1.00      1.00         2\n",
      "          22       0.50      0.50      0.50         2\n",
      "          23       1.00      1.00      1.00         2\n",
      "          24       0.67      1.00      0.80         2\n",
      "          25       1.00      0.50      0.67         4\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       1.00      1.00      1.00         2\n",
      "          29       0.00      0.00      0.00         1\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       0.00      0.00      0.00         0\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         1\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       0.67      1.00      0.80         2\n",
      "          36       1.00      1.00      1.00         4\n",
      "          37       1.00      0.67      0.80         3\n",
      "          38       1.00      0.67      0.80         3\n",
      "          39       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.80       100\n",
      "   macro avg       0.80      0.79      0.77       100\n",
      "weighted avg       0.84      0.80      0.79       100\n",
      "\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 2 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 2 0 0]\n",
      " [0 0 0 ... 0 2 0]\n",
      " [0 0 0 ... 0 0 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "yy_pred = np.argmax(y_pred,axis=1)\n",
    "yy_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "print(classification_report(yy_test,yy_pred))\n",
    "print(confusion_matrix(yy_test,yy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
