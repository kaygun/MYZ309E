{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras as ks\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation   \n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "from keras.layers import Input, Dense, Conv2D, Dropout, Flatten, MaxPooling2D, LSTM, Embedding\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,  classification_report\n",
    "from sklearn.datasets import load_iris, load_digits, fetch_20newsgroups_vectorized, fetch_olivetti_faces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10 (Neural Networks)\n",
    "\n",
    "## The gradient descent\n",
    "\n",
    "Consider the following problem: we have a multivariable function $F(x_1,\\ldots,x_n)$ that we want to optimize, i.e. find the point at which $F$ attains its minimum or maximum. There is an iterative algorithm called [steepest descent algorithm](https://ocw.mit.edu/courses/mathematics/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/lecture-notes/MIT18_409F09_scribe21.pdf) similar to Newton-Raphson that we can use to find this point. The algorithm uses the [gradient](https://inst.eecs.berkeley.edu/~ee127/sp21/livebook/def_gradient.html) of the function. Recall that the gradient $\\nabla F$ at a point $x$ \n",
    "\n",
    "$$ \\nabla F = \\left(\\frac{\\partial F}{\\partial x_1},\\ldots,\\frac{\\partial F}{\\partial x_n}\\right) $$\n",
    "\n",
    "gives us the direction at which $F$ has the largest (in absolute value) derivative. The algorithm uses this information and iteratively pushes an initial guess into better and better approximations of the optimum point. Let us start with an initial guess $x^{(0)} = (x_1^{(0)},\\ldots,x_n^{(0)})$ for $F(x_1^{(0)},\\ldots,x_n^{(0)}) = c$, and move in the direction of the gradient with a small step (called **learning rate**). Then the update rule for the path we are going to follow is\n",
    "\n",
    "$$ x^{(m+1)} = x^{(m)} - \\eta \\left(\\nabla F\\right)(x_1^{(m)},\\ldots,x_n^{(m)}) $$\n",
    "\n",
    "where $\\eta$ is called *the learning rate*. \n",
    "\n",
    "Now, let us solve $ f(x) = c $ using gradient descent. The following function is minimized at $f(x)=c$. So,\n",
    "\n",
    "$$ F(x) = \\frac{1}{2} (f(x) - c)^2 \\quad \\Rightarrow \\quad \\nabla F(x) = (f(x)-c) \\nabla f(x) $$\n",
    "\n",
    "So this update rule is:\n",
    "$$ x^{(k+1)} = x^{(k)} - \\eta (f(x^{(k)}) - c) \\nabla f(x^{(k)}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSolve(f, c, x0, lr=0.01, h=1e-5, tol=1e-5, n=15000):\n",
    "    x0 = np.array(x0, dtype=float)\n",
    "    dim = len(x0)\n",
    "    \n",
    "    def numerical_grad(f, x):\n",
    "        grad = np.zeros_like(x)\n",
    "        for i in range(len(x)):\n",
    "            dx = np.zeros_like(x)\n",
    "            dx[i] = h\n",
    "            grad[i] = (f(x + dx) - f(x - dx)) / (2 * h)\n",
    "        return grad\n",
    "\n",
    "    for i in range(n):\n",
    "        fx = f(x0)\n",
    "        grad_f = numerical_grad(f, x0)\n",
    "        x1 = x0 - lr * (fx - c) * grad_f\n",
    "        if np.linalg.norm(x1 - x0) < tol:\n",
    "            break\n",
    "        x0 = x1\n",
    "    return [i, x1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us solve a specific example. Let $f(x,y) = x^2 + 3y^2$, let $c=0$, let initial point $(x_0,y_0)=(2,2)$ and let us set the learning rate at 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3952, array([7.93556903e-02, 1.49809119e-05])]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x[0]**2 + 3 * x[1]**2\n",
    "\n",
    "MSolve(f, c=0.0, x0=[2.0, 2.0], lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started at the point $(2, 2)$ with the learning rate at set 0.01.  The red dots show the successive steps taken by the algorithm as it descends toward the minimum at the origin $(0, 0)$. This function is **convex**. The descent follows an **elliptical path**, with rapid convergence in $y$ and slower motion in $x$. The gradient is $ \\nabla f(x, y) = (2x, 6y) $. So, the updates are:\n",
    "\n",
    "$$ x^{(n+1)} = x^{(n)} - 0.01 \\cdot 2x^{(n)},\\quad y^{(n+1)} = y^{(n)} - 0.01 \\cdot 6y^{(n)} $$\n",
    "\n",
    "<img width=\"500px\" src=\"../images/steepest_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The perceptron\n",
    "\n",
    "\n",
    "Perceptrons are the main building blocks of artificial neural networks. They are designed to solve binary classification problems. They take a collection of input values $x = (x_1,\\ldots,x_n)$ apply a linear combination \n",
    "\n",
    "$$\\alpha\\cdot x + \\beta = a_1 x_1 + \\cdots + a_n x_n + \\beta$$ \n",
    "\n",
    "using a collection of weights $\\alpha = (a_0,\\ldots,a_n)$ and $\\beta$ to be determined via an iterative approach. Then we apply an activation function $\\varphi(x)$ to get an output which is either 0 or 1.\n",
    "\n",
    "![Perceptron](../images/perceptron.gif)\n",
    "\n",
    "([Source: Multilayer perceptrons from \"Nonlinear Switching State-Space Models\" by Antti Honkela](https://users.ics.aalto.fi/ahonkela/dippa/node41.html))\n",
    "\n",
    "\n",
    "This is a generalization of the [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) algorithm we covered in earlier lectures.  In the logistic regression case $\\varphi(x) = \\frac{1}{1+e^{-x}}$.  So, if we have a collection of data points $(x^{(i)},y^{(i)})$ that we assume satisfy a relationship of the form\n",
    "\n",
    "$$ y^{(i)} - \\varphi(\\alpha\\cdot x^{(i)} + \\beta) \\sim N(0,\\sigma) $$\n",
    "\n",
    "where $\\varphi\\colon\\mathbb{R}\\to\\mathbb{R}$ is a real valued function of a single variable, $\\alpha$ and $x^{(i)}$ are vectors in an inner product space and $\\beta$ is a scalar.  Our task is to find the best fitting pair $(\\alpha,\\beta)$ such that \n",
    "\n",
    "$$ \\sum_i (y^{(i)} - \\varphi(\\alpha\\cdot x^{(i)} + \\beta))^2 $$\n",
    "\n",
    "is minimized. So, we proceed by an iterative update:\n",
    "\n",
    "$$ \\alpha^{(n+1)} = \\alpha^{(n)} - \\frac{\\eta \\delta^{(n)}}{\\varphi'(\\alpha^{(n)}\\cdot x^{(n)}+\\beta^{(n)})} x^{(n)} $$\n",
    "\n",
    "where $\\delta^{(n)} = \\varphi(\\alpha^{(n)}\\cdot x^{(n)} + \\beta^{(n)}) - y^{(n)}$\n",
    "\n",
    "### Feed-forward and back-propagation\n",
    "\n",
    "In the feed-forward stage of the computation, we calculate the output $\\varphi(\\alpha^{(n)}\\cdot x + \\beta^{(n)})$. In the back-propagation phase, we calculate the error $y - \\varphi(\\alpha^{(n)}\\cdot x + \\beta^{(n)})$ and adjust the weights as described above to obtain the next iteration of weights $(\\alpha^{(n+1)},\\beta^{(n+1)})$.\n",
    "\n",
    "### An example\n",
    "\n",
    "For this example, we are going to use a [toy dataset](http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)) from UCI: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar = fetch_ucirepo(id=151) \n",
    "  \n",
    "sonar_X = sonar.data.features \n",
    "sonar_y_raw = sonar.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "sonar_y = encoder.fit_transform(sonar_y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(xs, ys, f, f_prime, epochs, track=10, eta=0.1, tol=1e-4):\n",
    "    # Convert to NumPy arrays and add bias term\n",
    "    X = np.hstack([np.ones((len(xs), 1)), xs])\n",
    "    N, d = X.shape\n",
    "    w = np.random.randn(d)\n",
    "    errors = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pick a random training sample\n",
    "        j = np.random.randint(N)\n",
    "\n",
    "        # Forward pass\n",
    "        z = np.dot(w, X[j])\n",
    "        a = f(z)\n",
    "        delta = a - ys[j]\n",
    "\n",
    "        # Update rule if error exceeds tolerance\n",
    "        if abs(delta) > tol:\n",
    "            grad = delta * f_prime(z) * X[j]\n",
    "            w -= eta * grad\n",
    "\n",
    "        # Track training error every 10 steps\n",
    "        if i % track == 0:\n",
    "            z_all = X @ w\n",
    "            y_hat = f(z_all)\n",
    "            mse = np.mean((y_hat - ys) ** 2)\n",
    "            errors.append(mse)\n",
    "\n",
    "    # Final prediction\n",
    "    y_pred = f(X @ w)\n",
    "    return y_pred, errors, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE')"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Train\n",
    "y_pred, errs, weights = fit_predict(sonar_X, sonar_y.reshape(-1), sigmoid, sigmoid_prime, epochs=15000, track=100, eta=0.5, tol=5e-4)\n",
    "\n",
    "# Plot error\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(errs)\n",
    "plt.xlabel(\"Epoch / 50\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[105,   6],\n",
       "       [ 17,  80]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def helper(x,th=0.5):\n",
    "    if x>=th:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "tmp = np.vectorize(helper)(y_pred).reshape(-1)\n",
    "confusion_matrix(sonar_y,tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "While perceptrons are limited in their ability to learn complex non-linear relationships, they can be combined into more sophisticated architectures called [neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network). Another way of saying would be a neural network is a directed computation graph of perceptrons:\n",
    "\n",
    "![](../images/neural-net.png)\n",
    "\n",
    "Neural networks are composed of multiple layers of interconnected perceptrons. Thus their weights form a sequence of matrices (a tensor) $(w^i_{j,k})$.  In the feedforward phase of the network,  where the input $(x^i_j)$ at some layer $i$ is processed by the network as\n",
    "\n",
    "$$ x^{i+1}_k = \\sum_j f^i_k(x^i_j w^i_{j,k}) $$\n",
    "\n",
    "where $f^i_k$ is the activation function at the neuron $k$ at level $i$. Again, as in the case of perceptron, when we get the output from the output layer, we calculate the error and then propagate the error back updating weights iteratively. This procedure is a variation of the gradient descent algorithm we outlined above.\n",
    "\n",
    "There is a [very large number](http://www.asimovinstitute.org/neural-network-zoo/) of different types of neural networks. Unlike the perceptrons, it is neither practical nor recommended that you implement neural networks by hand. \n",
    "\n",
    "![](../images/meme.jpg)\n",
    "\n",
    " I would suggest Use one of the following libraries or frameworks: \n",
    "\n",
    "1. [TensorFlow](https://www.tensorflow.org/)\n",
    "2. [PyTorch](https://pytorch.org/)\n",
    "3. [Keras](https://keras.io/)\n",
    "4. [MXNet](https://mxnet.apache.org/versions/1.9.1/)\n",
    "\n",
    "\n",
    "Tensorflow has a very nice [playground](https://playground.tensorflow.org/) where you can experiment with different architectures, activations functions etc. I highly recommend it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example \n",
    "\n",
    "All of todays examples are going to use the keras library. Let us start with the first example we used today, the sonar dataset. Let us construct a simple neural-net for binary classification, i.e. a perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(60,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sonar_X,sonar_y,train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8534 - loss: 0.3919 \n",
      "Epoch 2/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8674 - loss: 0.3943 \n",
      "Epoch 3/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8442 - loss: 0.4154 \n",
      "Epoch 4/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8536 - loss: 0.4175 \n",
      "Epoch 5/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8113 - loss: 0.4405 \n",
      "Epoch 6/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8360 - loss: 0.3974 \n",
      "Epoch 7/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8689 - loss: 0.3924 \n",
      "Epoch 8/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8439 - loss: 0.4140 \n",
      "Epoch 9/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8705 - loss: 0.4175 \n",
      "Epoch 10/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8696 - loss: 0.4027 \n",
      "Epoch 11/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8013 - loss: 0.4431 \n",
      "Epoch 12/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8021 - loss: 0.4243 \n",
      "Epoch 13/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8409 - loss: 0.4050 \n",
      "Epoch 14/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8438 - loss: 0.4187 \n",
      "Epoch 15/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8605 - loss: 0.4039 \n",
      "Epoch 16/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8246 - loss: 0.4144 \n",
      "Epoch 17/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8492 - loss: 0.3950 \n",
      "Epoch 18/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8734 - loss: 0.3709 \n",
      "Epoch 19/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8614 - loss: 0.3859 \n",
      "Epoch 20/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8674 - loss: 0.3851 \n",
      "Epoch 21/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8165 - loss: 0.4443 \n",
      "Epoch 22/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8540 - loss: 0.4052 \n",
      "Epoch 23/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.8579 - loss: 0.4092 \n",
      "Epoch 24/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - binary_accuracy: 0.8105 - loss: 0.4324\n",
      "Epoch 25/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.8533 - loss: 0.3947 \n",
      "Epoch 26/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8367 - loss: 0.4089 \n",
      "Epoch 27/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - binary_accuracy: 0.8429 - loss: 0.4061\n",
      "Epoch 28/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.8438 - loss: 0.4156\n",
      "Epoch 29/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.8233 - loss: 0.4264\n",
      "Epoch 30/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.8285 - loss: 0.4298 \n",
      "Epoch 31/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.8696 - loss: 0.3855 \n",
      "Epoch 32/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8731 - loss: 0.3847 \n",
      "Epoch 33/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8319 - loss: 0.4449 \n",
      "Epoch 34/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8467 - loss: 0.3904 \n",
      "Epoch 35/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8482 - loss: 0.4095 \n",
      "Epoch 36/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8320 - loss: 0.4105 \n",
      "Epoch 37/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8592 - loss: 0.4064 \n",
      "Epoch 38/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8539 - loss: 0.3983 \n",
      "Epoch 39/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8702 - loss: 0.3766 \n",
      "Epoch 40/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7855 - loss: 0.4349 \n",
      "Epoch 41/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8342 - loss: 0.4185 \n",
      "Epoch 42/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8581 - loss: 0.3923 \n",
      "Epoch 43/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8371 - loss: 0.4121 \n",
      "Epoch 44/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8671 - loss: 0.3900 \n",
      "Epoch 45/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8607 - loss: 0.3806 \n",
      "Epoch 46/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8316 - loss: 0.4139 \n",
      "Epoch 47/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8732 - loss: 0.3866 \n",
      "Epoch 48/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8285 - loss: 0.3927 \n",
      "Epoch 49/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8611 - loss: 0.4098 \n",
      "Epoch 50/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8613 - loss: 0.4028 \n",
      "Epoch 51/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8222 - loss: 0.4557 \n",
      "Epoch 52/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8348 - loss: 0.3967 \n",
      "Epoch 53/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8448 - loss: 0.4032 \n",
      "Epoch 54/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8523 - loss: 0.3840 \n",
      "Epoch 55/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8184 - loss: 0.4066 \n",
      "Epoch 56/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8296 - loss: 0.3833 \n",
      "Epoch 57/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8434 - loss: 0.3808 \n",
      "Epoch 58/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8353 - loss: 0.3978 \n",
      "Epoch 59/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8463 - loss: 0.3757 \n",
      "Epoch 60/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8696 - loss: 0.3663 \n",
      "Epoch 61/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8344 - loss: 0.4099 \n",
      "Epoch 62/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8567 - loss: 0.3911 \n",
      "Epoch 63/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8660 - loss: 0.3885 \n",
      "Epoch 64/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8659 - loss: 0.3987 \n",
      "Epoch 65/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8306 - loss: 0.4023 \n",
      "Epoch 66/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8350 - loss: 0.4310 \n",
      "Epoch 67/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8726 - loss: 0.3547 \n",
      "Epoch 68/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8146 - loss: 0.4052 \n",
      "Epoch 69/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8469 - loss: 0.3967 \n",
      "Epoch 70/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8673 - loss: 0.3908 \n",
      "Epoch 71/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8883 - loss: 0.3615 \n",
      "Epoch 72/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8392 - loss: 0.4121 \n",
      "Epoch 73/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8605 - loss: 0.3692 \n",
      "Epoch 74/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8498 - loss: 0.3922 \n",
      "Epoch 75/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8721 - loss: 0.3649 \n",
      "Epoch 76/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8299 - loss: 0.4219 \n",
      "Epoch 77/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8466 - loss: 0.4151 \n",
      "Epoch 78/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8388 - loss: 0.4140 \n",
      "Epoch 79/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8467 - loss: 0.3975 \n",
      "Epoch 80/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8589 - loss: 0.4046 \n",
      "Epoch 81/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8429 - loss: 0.3743 \n",
      "Epoch 82/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8229 - loss: 0.4125 \n",
      "Epoch 83/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8246 - loss: 0.4188 \n",
      "Epoch 84/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8202 - loss: 0.3956 \n",
      "Epoch 85/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7826 - loss: 0.4678 \n",
      "Epoch 86/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8308 - loss: 0.4289 \n",
      "Epoch 87/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8242 - loss: 0.4082 \n",
      "Epoch 88/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8216 - loss: 0.3941 \n",
      "Epoch 89/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8129 - loss: 0.4196 \n",
      "Epoch 90/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8805 - loss: 0.3743 \n",
      "Epoch 91/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8242 - loss: 0.4046 \n",
      "Epoch 92/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8342 - loss: 0.4159 \n",
      "Epoch 93/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8218 - loss: 0.4295 \n",
      "Epoch 94/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8533 - loss: 0.3864 \n",
      "Epoch 95/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8562 - loss: 0.3710 \n",
      "Epoch 96/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8408 - loss: 0.3863 \n",
      "Epoch 97/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8538 - loss: 0.3907 \n",
      "Epoch 98/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8487 - loss: 0.3707 \n",
      "Epoch 99/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8578 - loss: 0.3827 \n",
      "Epoch 100/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8497 - loss: 0.4090 \n",
      "Epoch 101/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8639 - loss: 0.3688 \n",
      "Epoch 102/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8333 - loss: 0.3962 \n",
      "Epoch 103/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8779 - loss: 0.3626 \n",
      "Epoch 104/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8424 - loss: 0.3868 \n",
      "Epoch 105/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8353 - loss: 0.4009 \n",
      "Epoch 106/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8119 - loss: 0.4332 \n",
      "Epoch 107/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8407 - loss: 0.3994 \n",
      "Epoch 108/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8699 - loss: 0.3893 \n",
      "Epoch 109/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8578 - loss: 0.3890 \n",
      "Epoch 110/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8261 - loss: 0.3908 \n",
      "Epoch 111/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8806 - loss: 0.3627 \n",
      "Epoch 112/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8589 - loss: 0.3986 \n",
      "Epoch 113/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8325 - loss: 0.3910 \n",
      "Epoch 114/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8575 - loss: 0.3834 \n",
      "Epoch 115/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8077 - loss: 0.4196 \n",
      "Epoch 116/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8095 - loss: 0.4401 \n",
      "Epoch 117/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8419 - loss: 0.3816 \n",
      "Epoch 118/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8571 - loss: 0.4085 \n",
      "Epoch 119/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8591 - loss: 0.3730 \n",
      "Epoch 120/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8392 - loss: 0.4141 \n",
      "Epoch 121/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8638 - loss: 0.3553 \n",
      "Epoch 122/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8362 - loss: 0.3928 \n",
      "Epoch 123/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8251 - loss: 0.4108 \n",
      "Epoch 124/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8465 - loss: 0.3994 \n",
      "Epoch 125/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8496 - loss: 0.3786 \n",
      "Epoch 126/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8624 - loss: 0.3710 \n",
      "Epoch 127/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8103 - loss: 0.3931 \n",
      "Epoch 128/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8175 - loss: 0.3992 \n",
      "Epoch 129/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8275 - loss: 0.4154 \n",
      "Epoch 130/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8756 - loss: 0.3629 \n",
      "Epoch 131/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8458 - loss: 0.4036 \n",
      "Epoch 132/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8041 - loss: 0.4242 \n",
      "Epoch 133/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8510 - loss: 0.3683 \n",
      "Epoch 134/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8175 - loss: 0.4113 \n",
      "Epoch 135/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8497 - loss: 0.3958 \n",
      "Epoch 136/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8329 - loss: 0.3895 \n",
      "Epoch 137/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8290 - loss: 0.4005 \n",
      "Epoch 138/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8787 - loss: 0.3747 \n",
      "Epoch 139/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8515 - loss: 0.3908 \n",
      "Epoch 140/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8293 - loss: 0.3850 \n",
      "Epoch 141/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8517 - loss: 0.3695 \n",
      "Epoch 142/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7939 - loss: 0.4206 \n",
      "Epoch 143/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8273 - loss: 0.3837 \n",
      "Epoch 144/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8430 - loss: 0.3919 \n",
      "Epoch 145/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8352 - loss: 0.3947 \n",
      "Epoch 146/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8521 - loss: 0.3819 \n",
      "Epoch 147/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8489 - loss: 0.3921 \n",
      "Epoch 148/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8476 - loss: 0.4118 \n",
      "Epoch 149/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8423 - loss: 0.3962 \n",
      "Epoch 150/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8468 - loss: 0.3983 \n",
      "Epoch 151/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8315 - loss: 0.4265 \n",
      "Epoch 152/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8329 - loss: 0.3968 \n",
      "Epoch 153/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8112 - loss: 0.4445 \n",
      "Epoch 154/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8565 - loss: 0.3766 \n",
      "Epoch 155/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8286 - loss: 0.3920 \n",
      "Epoch 156/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8129 - loss: 0.4187 \n",
      "Epoch 157/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8216 - loss: 0.4177 \n",
      "Epoch 158/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8370 - loss: 0.3823 \n",
      "Epoch 159/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8462 - loss: 0.3728 \n",
      "Epoch 160/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8408 - loss: 0.4246 \n",
      "Epoch 161/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8471 - loss: 0.3933 \n",
      "Epoch 162/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8677 - loss: 0.4083 \n",
      "Epoch 163/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8709 - loss: 0.3997 \n",
      "Epoch 164/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8395 - loss: 0.3748 \n",
      "Epoch 165/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8473 - loss: 0.3649 \n",
      "Epoch 166/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8231 - loss: 0.3927 \n",
      "Epoch 167/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8688 - loss: 0.3589 \n",
      "Epoch 168/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8646 - loss: 0.3936 \n",
      "Epoch 169/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8532 - loss: 0.3670 \n",
      "Epoch 170/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8748 - loss: 0.3707 \n",
      "Epoch 171/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8190 - loss: 0.4027 \n",
      "Epoch 172/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8290 - loss: 0.3849 \n",
      "Epoch 173/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8647 - loss: 0.3591 \n",
      "Epoch 174/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8436 - loss: 0.4249 \n",
      "Epoch 175/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8429 - loss: 0.3942 \n",
      "Epoch 176/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8410 - loss: 0.4048 \n",
      "Epoch 177/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8547 - loss: 0.3889 \n",
      "Epoch 178/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8259 - loss: 0.3751 \n",
      "Epoch 179/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8139 - loss: 0.4045 \n",
      "Epoch 180/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8174 - loss: 0.3787 \n",
      "Epoch 181/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8188 - loss: 0.4055 \n",
      "Epoch 182/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8458 - loss: 0.3580 \n",
      "Epoch 183/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8558 - loss: 0.3725 \n",
      "Epoch 184/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8588 - loss: 0.3700 \n",
      "Epoch 185/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8753 - loss: 0.3665 \n",
      "Epoch 186/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8264 - loss: 0.3945 \n",
      "Epoch 187/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8490 - loss: 0.3725 \n",
      "Epoch 188/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8565 - loss: 0.3881 \n",
      "Epoch 189/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8401 - loss: 0.3847 \n",
      "Epoch 190/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8690 - loss: 0.3656 \n",
      "Epoch 191/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8747 - loss: 0.3630 \n",
      "Epoch 192/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8513 - loss: 0.3831 \n",
      "Epoch 193/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8593 - loss: 0.3530 \n",
      "Epoch 194/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8579 - loss: 0.3828 \n",
      "Epoch 195/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8497 - loss: 0.3799 \n",
      "Epoch 196/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8679 - loss: 0.3609 \n",
      "Epoch 197/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8815 - loss: 0.3654 \n",
      "Epoch 198/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8473 - loss: 0.3729 \n",
      "Epoch 199/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8285 - loss: 0.4189 \n",
      "Epoch 200/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8210 - loss: 0.3926 \n",
      "Epoch 201/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8253 - loss: 0.3995 \n",
      "Epoch 202/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8602 - loss: 0.3586 \n",
      "Epoch 203/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8239 - loss: 0.3856 \n",
      "Epoch 204/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8372 - loss: 0.4148 \n",
      "Epoch 205/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8636 - loss: 0.3475 \n",
      "Epoch 206/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8575 - loss: 0.3904 \n",
      "Epoch 207/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8509 - loss: 0.3709 \n",
      "Epoch 208/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8488 - loss: 0.3583 \n",
      "Epoch 209/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8456 - loss: 0.3699 \n",
      "Epoch 210/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8639 - loss: 0.3647 \n",
      "Epoch 211/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8354 - loss: 0.4108 \n",
      "Epoch 212/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8525 - loss: 0.3797 \n",
      "Epoch 213/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8533 - loss: 0.3739 \n",
      "Epoch 214/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8478 - loss: 0.3851 \n",
      "Epoch 215/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8627 - loss: 0.3688 \n",
      "Epoch 216/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8032 - loss: 0.3958 \n",
      "Epoch 217/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8390 - loss: 0.3761 \n",
      "Epoch 218/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.8695 - loss: 0.3848 \n",
      "Epoch 219/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8346 - loss: 0.4048 \n",
      "Epoch 220/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8591 - loss: 0.3979 \n",
      "Epoch 221/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8620 - loss: 0.3929 \n",
      "Epoch 222/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8403 - loss: 0.4048 \n",
      "Epoch 223/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8303 - loss: 0.3906 \n",
      "Epoch 224/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8590 - loss: 0.3641 \n",
      "Epoch 225/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8495 - loss: 0.3726 \n",
      "Epoch 226/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8947 - loss: 0.3747 \n",
      "Epoch 227/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8275 - loss: 0.3878 \n",
      "Epoch 228/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8604 - loss: 0.3760 \n",
      "Epoch 229/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8211 - loss: 0.3941 \n",
      "Epoch 230/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8554 - loss: 0.3722 \n",
      "Epoch 231/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8287 - loss: 0.3862 \n",
      "Epoch 232/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8707 - loss: 0.3773 \n",
      "Epoch 233/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8036 - loss: 0.4116 \n",
      "Epoch 234/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8353 - loss: 0.4120 \n",
      "Epoch 235/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8604 - loss: 0.3502 \n",
      "Epoch 236/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8484 - loss: 0.4073 \n",
      "Epoch 237/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8844 - loss: 0.3378 \n",
      "Epoch 238/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8306 - loss: 0.3975 \n",
      "Epoch 239/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8970 - loss: 0.3259 \n",
      "Epoch 240/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8681 - loss: 0.3428 \n",
      "Epoch 241/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8479 - loss: 0.3933 \n",
      "Epoch 242/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8628 - loss: 0.3613 \n",
      "Epoch 243/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8644 - loss: 0.3472 \n",
      "Epoch 244/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8470 - loss: 0.3665 \n",
      "Epoch 245/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8621 - loss: 0.3703 \n",
      "Epoch 246/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8342 - loss: 0.3743 \n",
      "Epoch 247/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8247 - loss: 0.4079 \n",
      "Epoch 248/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8748 - loss: 0.3821 \n",
      "Epoch 249/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8470 - loss: 0.3698 \n",
      "Epoch 250/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8558 - loss: 0.3961 \n",
      "Epoch 251/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8039 - loss: 0.4168 \n",
      "Epoch 252/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8380 - loss: 0.3985 \n",
      "Epoch 253/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8447 - loss: 0.3856 \n",
      "Epoch 254/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8423 - loss: 0.3811 \n",
      "Epoch 255/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8442 - loss: 0.3712 \n",
      "Epoch 256/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8534 - loss: 0.3886 \n",
      "Epoch 257/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8163 - loss: 0.4108 \n",
      "Epoch 258/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8608 - loss: 0.3836 \n",
      "Epoch 259/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8309 - loss: 0.3901 \n",
      "Epoch 260/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8567 - loss: 0.3661 \n",
      "Epoch 261/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8523 - loss: 0.3749 \n",
      "Epoch 262/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8906 - loss: 0.3496 \n",
      "Epoch 263/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8444 - loss: 0.3581 \n",
      "Epoch 264/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8362 - loss: 0.4137 \n",
      "Epoch 265/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8447 - loss: 0.3869 \n",
      "Epoch 266/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8685 - loss: 0.3709 \n",
      "Epoch 267/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8199 - loss: 0.3840 \n",
      "Epoch 268/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8242 - loss: 0.3844 \n",
      "Epoch 269/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8451 - loss: 0.3581 \n",
      "Epoch 270/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8015 - loss: 0.4375 \n",
      "Epoch 271/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8591 - loss: 0.3980 \n",
      "Epoch 272/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8367 - loss: 0.4024 \n",
      "Epoch 273/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8585 - loss: 0.3792 \n",
      "Epoch 274/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8475 - loss: 0.3689 \n",
      "Epoch 275/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8458 - loss: 0.4120 \n",
      "Epoch 276/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8368 - loss: 0.3650 \n",
      "Epoch 277/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8242 - loss: 0.3931 \n",
      "Epoch 278/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8025 - loss: 0.4084 \n",
      "Epoch 279/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8788 - loss: 0.3316 \n",
      "Epoch 280/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8517 - loss: 0.3763 \n",
      "Epoch 281/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8283 - loss: 0.3817 \n",
      "Epoch 282/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8491 - loss: 0.3732 \n",
      "Epoch 283/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8533 - loss: 0.3652 \n",
      "Epoch 284/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8383 - loss: 0.3810 \n",
      "Epoch 285/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8266 - loss: 0.4074 \n",
      "Epoch 286/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8581 - loss: 0.3680 \n",
      "Epoch 287/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8601 - loss: 0.3660 \n",
      "Epoch 288/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8337 - loss: 0.3819 \n",
      "Epoch 289/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8409 - loss: 0.3606 \n",
      "Epoch 290/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8344 - loss: 0.3702 \n",
      "Epoch 291/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8354 - loss: 0.3818 \n",
      "Epoch 292/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8480 - loss: 0.3545 \n",
      "Epoch 293/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8676 - loss: 0.3539 \n",
      "Epoch 294/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8713 - loss: 0.3556 \n",
      "Epoch 295/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8985 - loss: 0.3356 \n",
      "Epoch 296/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8496 - loss: 0.3715 \n",
      "Epoch 297/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8467 - loss: 0.3860 \n",
      "Epoch 298/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8501 - loss: 0.4115 \n",
      "Epoch 299/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8563 - loss: 0.3726 \n",
      "Epoch 300/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8498 - loss: 0.3650 \n",
      "Epoch 301/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8477 - loss: 0.3718 \n",
      "Epoch 302/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8313 - loss: 0.4050 \n",
      "Epoch 303/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8631 - loss: 0.3726 \n",
      "Epoch 304/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8624 - loss: 0.3650 \n",
      "Epoch 305/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8188 - loss: 0.4011 \n",
      "Epoch 306/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8444 - loss: 0.3649 \n",
      "Epoch 307/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8641 - loss: 0.3553 \n",
      "Epoch 308/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8611 - loss: 0.3541 \n",
      "Epoch 309/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8625 - loss: 0.3491 \n",
      "Epoch 310/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8555 - loss: 0.3904 \n",
      "Epoch 311/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8274 - loss: 0.3787 \n",
      "Epoch 312/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8735 - loss: 0.3744 \n",
      "Epoch 313/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8937 - loss: 0.3361 \n",
      "Epoch 314/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8319 - loss: 0.3971 \n",
      "Epoch 315/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8560 - loss: 0.4089 \n",
      "Epoch 316/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8175 - loss: 0.3924 \n",
      "Epoch 317/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8605 - loss: 0.3647 \n",
      "Epoch 318/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8275 - loss: 0.3897 \n",
      "Epoch 319/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8613 - loss: 0.4101 \n",
      "Epoch 320/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8599 - loss: 0.3689 \n",
      "Epoch 321/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8798 - loss: 0.3480 \n",
      "Epoch 322/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8549 - loss: 0.3558 \n",
      "Epoch 323/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8644 - loss: 0.3612 \n",
      "Epoch 324/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8369 - loss: 0.3826 \n",
      "Epoch 325/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8541 - loss: 0.3885 \n",
      "Epoch 326/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8696 - loss: 0.3697 \n",
      "Epoch 327/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8430 - loss: 0.3597 \n",
      "Epoch 328/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8660 - loss: 0.3507 \n",
      "Epoch 329/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8518 - loss: 0.3469 \n",
      "Epoch 330/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8531 - loss: 0.3583 \n",
      "Epoch 331/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8379 - loss: 0.3645 \n",
      "Epoch 332/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8465 - loss: 0.3681 \n",
      "Epoch 333/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8677 - loss: 0.3663 \n",
      "Epoch 334/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8345 - loss: 0.3669 \n",
      "Epoch 335/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8678 - loss: 0.3504 \n",
      "Epoch 336/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8442 - loss: 0.3704 \n",
      "Epoch 337/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8730 - loss: 0.3423 \n",
      "Epoch 338/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8539 - loss: 0.3619 \n",
      "Epoch 339/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8081 - loss: 0.3894 \n",
      "Epoch 340/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8620 - loss: 0.3616 \n",
      "Epoch 341/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8642 - loss: 0.3494 \n",
      "Epoch 342/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8626 - loss: 0.3863 \n",
      "Epoch 343/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8560 - loss: 0.3562 \n",
      "Epoch 344/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8699 - loss: 0.3654 \n",
      "Epoch 345/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8461 - loss: 0.3911 \n",
      "Epoch 346/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8565 - loss: 0.3601 \n",
      "Epoch 347/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8676 - loss: 0.3359 \n",
      "Epoch 348/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8639 - loss: 0.3335 \n",
      "Epoch 349/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8309 - loss: 0.3890 \n",
      "Epoch 350/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8220 - loss: 0.3800 \n",
      "Epoch 351/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8839 - loss: 0.3551 \n",
      "Epoch 352/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8474 - loss: 0.3835 \n",
      "Epoch 353/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8383 - loss: 0.3825 \n",
      "Epoch 354/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8340 - loss: 0.3833 \n",
      "Epoch 355/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8494 - loss: 0.3534 \n",
      "Epoch 356/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8286 - loss: 0.3653 \n",
      "Epoch 357/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8270 - loss: 0.3859 \n",
      "Epoch 358/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8378 - loss: 0.3790 \n",
      "Epoch 359/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8718 - loss: 0.3482 \n",
      "Epoch 360/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8320 - loss: 0.3692 \n",
      "Epoch 361/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8766 - loss: 0.3501 \n",
      "Epoch 362/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8334 - loss: 0.3676 \n",
      "Epoch 363/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8782 - loss: 0.3311 \n",
      "Epoch 364/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8580 - loss: 0.3664 \n",
      "Epoch 365/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8726 - loss: 0.3788 \n",
      "Epoch 366/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8661 - loss: 0.3779 \n",
      "Epoch 367/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8498 - loss: 0.3910 \n",
      "Epoch 368/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8629 - loss: 0.3690 \n",
      "Epoch 369/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8480 - loss: 0.3704 \n",
      "Epoch 370/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8302 - loss: 0.3734 \n",
      "Epoch 371/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8311 - loss: 0.3843 \n",
      "Epoch 372/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8889 - loss: 0.3409 \n",
      "Epoch 373/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8591 - loss: 0.3394 \n",
      "Epoch 374/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8674 - loss: 0.3870 \n",
      "Epoch 375/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8646 - loss: 0.3283 \n",
      "Epoch 376/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8578 - loss: 0.3665 \n",
      "Epoch 377/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8772 - loss: 0.3333 \n",
      "Epoch 378/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8774 - loss: 0.3404 \n",
      "Epoch 379/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8483 - loss: 0.4190 \n",
      "Epoch 380/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8270 - loss: 0.4193 \n",
      "Epoch 381/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8691 - loss: 0.3347 \n",
      "Epoch 382/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8195 - loss: 0.3822 \n",
      "Epoch 383/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8299 - loss: 0.3651 \n",
      "Epoch 384/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8915 - loss: 0.3437 \n",
      "Epoch 385/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8776 - loss: 0.3655 \n",
      "Epoch 386/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8619 - loss: 0.3665 \n",
      "Epoch 387/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8729 - loss: 0.3484 \n",
      "Epoch 388/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.8566 - loss: 0.3421 \n",
      "Epoch 389/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8637 - loss: 0.3585 \n",
      "Epoch 390/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8200 - loss: 0.4040 \n",
      "Epoch 391/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8456 - loss: 0.3980 \n",
      "Epoch 392/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8942 - loss: 0.3507 \n",
      "Epoch 393/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8779 - loss: 0.3381 \n",
      "Epoch 394/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7953 - loss: 0.4371 \n",
      "Epoch 395/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8603 - loss: 0.3839 \n",
      "Epoch 396/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8623 - loss: 0.3601 \n",
      "Epoch 397/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8630 - loss: 0.3666 \n",
      "Epoch 398/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8607 - loss: 0.3430 \n",
      "Epoch 399/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8355 - loss: 0.4158 \n",
      "Epoch 400/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8646 - loss: 0.3733 \n",
      "Epoch 401/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8770 - loss: 0.3361 \n",
      "Epoch 402/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8869 - loss: 0.3363 \n",
      "Epoch 403/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8336 - loss: 0.3783 \n",
      "Epoch 404/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8845 - loss: 0.3418 \n",
      "Epoch 405/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8292 - loss: 0.3934 \n",
      "Epoch 406/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8438 - loss: 0.3515 \n",
      "Epoch 407/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8470 - loss: 0.3692 \n",
      "Epoch 408/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8628 - loss: 0.3395 \n",
      "Epoch 409/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8658 - loss: 0.3363 \n",
      "Epoch 410/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8724 - loss: 0.3857 \n",
      "Epoch 411/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8430 - loss: 0.3605 \n",
      "Epoch 412/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8379 - loss: 0.3832 \n",
      "Epoch 413/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8467 - loss: 0.3598 \n",
      "Epoch 414/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8595 - loss: 0.3588 \n",
      "Epoch 415/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8740 - loss: 0.3416 \n",
      "Epoch 416/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8653 - loss: 0.3547 \n",
      "Epoch 417/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8655 - loss: 0.3506 \n",
      "Epoch 418/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8362 - loss: 0.3894 \n",
      "Epoch 419/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8839 - loss: 0.3275 \n",
      "Epoch 420/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8462 - loss: 0.3720 \n",
      "Epoch 421/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8107 - loss: 0.3696 \n",
      "Epoch 422/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8518 - loss: 0.3565 \n",
      "Epoch 423/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8145 - loss: 0.4343 \n",
      "Epoch 424/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8414 - loss: 0.3976 \n",
      "Epoch 425/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8588 - loss: 0.3461 \n",
      "Epoch 426/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8736 - loss: 0.3426 \n",
      "Epoch 427/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8739 - loss: 0.3381 \n",
      "Epoch 428/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8697 - loss: 0.3647 \n",
      "Epoch 429/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8581 - loss: 0.3424 \n",
      "Epoch 430/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8839 - loss: 0.3432 \n",
      "Epoch 431/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8486 - loss: 0.3956 \n",
      "Epoch 432/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8890 - loss: 0.3345 \n",
      "Epoch 433/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8801 - loss: 0.3397 \n",
      "Epoch 434/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8489 - loss: 0.3643 \n",
      "Epoch 435/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9022 - loss: 0.3167 \n",
      "Epoch 436/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8179 - loss: 0.3771 \n",
      "Epoch 437/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8769 - loss: 0.3322 \n",
      "Epoch 438/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8255 - loss: 0.3842 \n",
      "Epoch 439/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8729 - loss: 0.3555 \n",
      "Epoch 440/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8413 - loss: 0.3659 \n",
      "Epoch 441/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8448 - loss: 0.4010 \n",
      "Epoch 442/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8406 - loss: 0.3678 \n",
      "Epoch 443/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8646 - loss: 0.3410 \n",
      "Epoch 444/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8536 - loss: 0.3565 \n",
      "Epoch 445/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8587 - loss: 0.3382 \n",
      "Epoch 446/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8276 - loss: 0.3843 \n",
      "Epoch 447/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8478 - loss: 0.3576 \n",
      "Epoch 448/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8338 - loss: 0.3908 \n",
      "Epoch 449/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8471 - loss: 0.3766 \n",
      "Epoch 450/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8421 - loss: 0.3780 \n",
      "Epoch 451/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8137 - loss: 0.3710 \n",
      "Epoch 452/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8207 - loss: 0.3822 \n",
      "Epoch 453/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8655 - loss: 0.3689 \n",
      "Epoch 454/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8555 - loss: 0.3632 \n",
      "Epoch 455/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8576 - loss: 0.3379 \n",
      "Epoch 456/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8606 - loss: 0.3582 \n",
      "Epoch 457/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8750 - loss: 0.3493 \n",
      "Epoch 458/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8505 - loss: 0.3660 \n",
      "Epoch 459/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8611 - loss: 0.3516 \n",
      "Epoch 460/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8553 - loss: 0.3407 \n",
      "Epoch 461/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8768 - loss: 0.3573 \n",
      "Epoch 462/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8766 - loss: 0.3612 \n",
      "Epoch 463/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8637 - loss: 0.3745 \n",
      "Epoch 464/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8777 - loss: 0.3422 \n",
      "Epoch 465/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8593 - loss: 0.3386 \n",
      "Epoch 466/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8563 - loss: 0.3366 \n",
      "Epoch 467/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8700 - loss: 0.3402 \n",
      "Epoch 468/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8393 - loss: 0.3923 \n",
      "Epoch 469/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8481 - loss: 0.3727 \n",
      "Epoch 470/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8596 - loss: 0.3445 \n",
      "Epoch 471/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8487 - loss: 0.3664 \n",
      "Epoch 472/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8785 - loss: 0.3260 \n",
      "Epoch 473/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8680 - loss: 0.3816 \n",
      "Epoch 474/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8054 - loss: 0.4059 \n",
      "Epoch 475/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8347 - loss: 0.3792 \n",
      "Epoch 476/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8531 - loss: 0.3603 \n",
      "Epoch 477/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8340 - loss: 0.3801 \n",
      "Epoch 478/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8400 - loss: 0.3559 \n",
      "Epoch 479/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8586 - loss: 0.3528 \n",
      "Epoch 480/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8604 - loss: 0.3455 \n",
      "Epoch 481/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8395 - loss: 0.3516 \n",
      "Epoch 482/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8610 - loss: 0.3418 \n",
      "Epoch 483/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8416 - loss: 0.4028 \n",
      "Epoch 484/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8457 - loss: 0.3496 \n",
      "Epoch 485/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8462 - loss: 0.3705 \n",
      "Epoch 486/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8461 - loss: 0.3646 \n",
      "Epoch 487/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8829 - loss: 0.3324 \n",
      "Epoch 488/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8414 - loss: 0.3640 \n",
      "Epoch 489/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8540 - loss: 0.3486 \n",
      "Epoch 490/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8495 - loss: 0.3788 \n",
      "Epoch 491/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8213 - loss: 0.3836 \n",
      "Epoch 492/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8531 - loss: 0.3693 \n",
      "Epoch 493/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8594 - loss: 0.3746 \n",
      "Epoch 494/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8768 - loss: 0.3374 \n",
      "Epoch 495/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8625 - loss: 0.3488 \n",
      "Epoch 496/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8513 - loss: 0.3855 \n",
      "Epoch 497/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8162 - loss: 0.3644 \n",
      "Epoch 498/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8694 - loss: 0.3391 \n",
      "Epoch 499/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8666 - loss: 0.3187 \n",
      "Epoch 500/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8690 - loss: 0.3554 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ef75963ff00>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=500, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ef75877d080> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ef75877d080> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[34,  1],\n",
       "       [17, 11]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "tmp = np.vectorize(lambda x: helper(x,th=0.75))(y_predict.reshape(y_predict.shape[0]))\n",
    "confusion_matrix(y_test,tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another standard small example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "x_iris = iris['data']\n",
    "y_iris = iris['target']\n",
    "\n",
    "labeler = LabelBinarizer()\n",
    "y = labeler.fit_transform(y_iris)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(4,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_iris, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - binary_accuracy: 0.6057 - loss: 0.6667\n",
      "Epoch 2/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6694 - loss: 0.6057 \n",
      "Epoch 3/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6993 - loss: 0.5682 \n",
      "Epoch 4/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.7718 - loss: 0.5355 \n",
      "Epoch 5/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.7750 - loss: 0.4989\n",
      "Epoch 6/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.7547 - loss: 0.4817\n",
      "Epoch 7/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.7971 - loss: 0.4694 \n",
      "Epoch 8/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8539 - loss: 0.4425 \n",
      "Epoch 9/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8860 - loss: 0.4156 \n",
      "Epoch 10/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8853 - loss: 0.4003 \n",
      "Epoch 11/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.8695 - loss: 0.3822 \n",
      "Epoch 12/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8586 - loss: 0.3771 \n",
      "Epoch 13/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8563 - loss: 0.3680 \n",
      "Epoch 14/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8660 - loss: 0.3604 \n",
      "Epoch 15/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8741 - loss: 0.3277 \n",
      "Epoch 16/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8954 - loss: 0.3045 \n",
      "Epoch 17/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8927 - loss: 0.2957 \n",
      "Epoch 18/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8989 - loss: 0.2838 \n",
      "Epoch 19/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8893 - loss: 0.2910 \n",
      "Epoch 20/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8820 - loss: 0.2957 \n",
      "Epoch 21/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8988 - loss: 0.2627 \n",
      "Epoch 22/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8948 - loss: 0.2604 \n",
      "Epoch 23/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8885 - loss: 0.2639 \n",
      "Epoch 24/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8826 - loss: 0.2673 \n",
      "Epoch 25/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8996 - loss: 0.2394 \n",
      "Epoch 26/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.8714 - loss: 0.2639 \n",
      "Epoch 27/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8678 - loss: 0.2539 \n",
      "Epoch 28/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - binary_accuracy: 0.8833 - loss: 0.2494\n",
      "Epoch 29/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - binary_accuracy: 0.8788 - loss: 0.2395\n",
      "Epoch 30/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8849 - loss: 0.2339 \n",
      "Epoch 31/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8898 - loss: 0.2182 \n",
      "Epoch 32/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8898 - loss: 0.2320 \n",
      "Epoch 33/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9052 - loss: 0.2091 \n",
      "Epoch 34/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8904 - loss: 0.2168 \n",
      "Epoch 35/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9202 - loss: 0.2102 \n",
      "Epoch 36/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9208 - loss: 0.2081 \n",
      "Epoch 37/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9348 - loss: 0.1810 \n",
      "Epoch 38/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.9364 - loss: 0.1730  \n",
      "Epoch 39/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.9270 - loss: 0.2026 \n",
      "Epoch 40/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.9674 - loss: 0.1626\n",
      "Epoch 41/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.9353 - loss: 0.1936 \n",
      "Epoch 42/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.9734 - loss: 0.1637 \n",
      "Epoch 43/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9429 - loss: 0.1710 \n",
      "Epoch 44/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9654 - loss: 0.1536 \n",
      "Epoch 45/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9441 - loss: 0.1649 \n",
      "Epoch 46/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9830 - loss: 0.1496 \n",
      "Epoch 47/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9835 - loss: 0.1540 \n",
      "Epoch 48/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9758 - loss: 0.1516 \n",
      "Epoch 49/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9795 - loss: 0.1447 \n",
      "Epoch 50/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9625 - loss: 0.1579 \n",
      "Epoch 51/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9793 - loss: 0.1437 \n",
      "Epoch 52/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9853 - loss: 0.1207 \n",
      "Epoch 53/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9812 - loss: 0.1234 \n",
      "Epoch 54/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9901 - loss: 0.1191 \n",
      "Epoch 55/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9829 - loss: 0.1189 \n",
      "Epoch 56/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9823 - loss: 0.1221 \n",
      "Epoch 57/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9841 - loss: 0.1106 \n",
      "Epoch 58/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9880 - loss: 0.1165 \n",
      "Epoch 59/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9820 - loss: 0.1168 \n",
      "Epoch 60/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9757 - loss: 0.1142 \n",
      "Epoch 61/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9908 - loss: 0.0924 \n",
      "Epoch 62/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9809 - loss: 0.0987 \n",
      "Epoch 63/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9915 - loss: 0.0971 \n",
      "Epoch 64/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9899 - loss: 0.0918 \n",
      "Epoch 65/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9935 - loss: 0.0925 \n",
      "Epoch 66/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9827 - loss: 0.0970 \n",
      "Epoch 67/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9929 - loss: 0.0872 \n",
      "Epoch 68/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9907 - loss: 0.0833 \n",
      "Epoch 69/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9904 - loss: 0.0778 \n",
      "Epoch 70/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0844 \n",
      "Epoch 71/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.0919 \n",
      "Epoch 72/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.0878 \n",
      "Epoch 73/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9945 - loss: 0.0795 \n",
      "Epoch 74/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9857 - loss: 0.0763 \n",
      "Epoch 75/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9868 - loss: 0.0822  \n",
      "Epoch 76/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0765 \n",
      "Epoch 77/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9923 - loss: 0.0652 \n",
      "Epoch 78/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0737 \n",
      "Epoch 79/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0747 \n",
      "Epoch 80/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9945 - loss: 0.0602 \n",
      "Epoch 81/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9819 - loss: 0.0701 \n",
      "Epoch 82/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9883 - loss: 0.0694 \n",
      "Epoch 83/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9709 - loss: 0.0849 \n",
      "Epoch 84/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9848 - loss: 0.0734 \n",
      "Epoch 85/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.0646 \n",
      "Epoch 86/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9895 - loss: 0.0580 \n",
      "Epoch 87/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9839 - loss: 0.0584 \n",
      "Epoch 88/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0567 \n",
      "Epoch 89/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.0609 \n",
      "Epoch 90/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9882 - loss: 0.0563 \n",
      "Epoch 91/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9907 - loss: 0.0519 \n",
      "Epoch 92/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9836 - loss: 0.0623 \n",
      "Epoch 93/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0542 \n",
      "Epoch 94/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9935 - loss: 0.0454 \n",
      "Epoch 95/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9905 - loss: 0.0532 \n",
      "Epoch 96/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9894 - loss: 0.0442 \n",
      "Epoch 97/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9945 - loss: 0.0533 \n",
      "Epoch 98/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9891 - loss: 0.0455 \n",
      "Epoch 99/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9881 - loss: 0.0503 \n",
      "Epoch 100/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9935 - loss: 0.0417 \n",
      "Epoch 101/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - binary_accuracy: 0.9956 - loss: 0.0381\n",
      "Epoch 102/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9929 - loss: 0.0412 \n",
      "Epoch 103/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9868 - loss: 0.0489 \n",
      "Epoch 104/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0531 \n",
      "Epoch 105/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9935 - loss: 0.0381 \n",
      "Epoch 106/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9820 - loss: 0.0573 \n",
      "Epoch 107/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9935 - loss: 0.0458 \n",
      "Epoch 108/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0518 \n",
      "Epoch 109/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0522 \n",
      "Epoch 110/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9919 - loss: 0.0436 \n",
      "Epoch 111/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9935 - loss: 0.0401 \n",
      "Epoch 112/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0492 \n",
      "Epoch 113/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9927 - loss: 0.0348 \n",
      "Epoch 114/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9929 - loss: 0.0385 \n",
      "Epoch 115/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9851 - loss: 0.0450 \n",
      "Epoch 116/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - binary_accuracy: 0.9919 - loss: 0.0469\n",
      "Epoch 117/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9895 - loss: 0.0425 \n",
      "Epoch 118/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.0436 \n",
      "Epoch 119/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9722 - loss: 0.0528 \n",
      "Epoch 120/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0501 \n",
      "Epoch 121/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9966 - loss: 0.0270 \n",
      "Epoch 122/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9926 - loss: 0.0441 \n",
      "Epoch 123/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9983 - loss: 0.0378 \n",
      "Epoch 124/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9945 - loss: 0.0389 \n",
      "Epoch 125/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9722 - loss: 0.0599 \n",
      "Epoch 126/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9935 - loss: 0.0425 \n",
      "Epoch 127/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.0423 \n",
      "Epoch 128/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9929 - loss: 0.0362 \n",
      "Epoch 129/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9902 - loss: 0.0319 \n",
      "Epoch 130/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0366 \n",
      "Epoch 131/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0430 \n",
      "Epoch 132/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0381 \n",
      "Epoch 133/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9929 - loss: 0.0358 \n",
      "Epoch 134/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9956 - loss: 0.0270 \n",
      "Epoch 135/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9811 - loss: 0.0468 \n",
      "Epoch 136/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9793 - loss: 0.0469 \n",
      "Epoch 137/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9956 - loss: 0.0324 \n",
      "Epoch 138/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9840 - loss: 0.0366 \n",
      "Epoch 139/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - binary_accuracy: 0.9818 - loss: 0.0355\n",
      "Epoch 140/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9928 - loss: 0.0321 \n",
      "Epoch 141/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9966 - loss: 0.0275 \n",
      "Epoch 142/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9942 - loss: 0.0378 \n",
      "Epoch 143/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9851 - loss: 0.0348 \n",
      "Epoch 144/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.9836 - loss: 0.0460 \n",
      "Epoch 145/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.9883 - loss: 0.0405\n",
      "Epoch 146/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9895 - loss: 0.0339 \n",
      "Epoch 147/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0435 \n",
      "Epoch 148/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0359 \n",
      "Epoch 149/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9890 - loss: 0.0358 \n",
      "Epoch 150/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0353 \n",
      "Epoch 151/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0350 \n",
      "Epoch 152/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9919 - loss: 0.0272 \n",
      "Epoch 153/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9907 - loss: 0.0368 \n",
      "Epoch 154/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9757 - loss: 0.0419 \n",
      "Epoch 155/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9836 - loss: 0.0395 \n",
      "Epoch 156/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9895 - loss: 0.0327 \n",
      "Epoch 157/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9900 - loss: 0.0285 \n",
      "Epoch 158/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9817 - loss: 0.0349 \n",
      "Epoch 159/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0367 \n",
      "Epoch 160/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0369 \n",
      "Epoch 161/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0329 \n",
      "Epoch 162/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9891 - loss: 0.0316 \n",
      "Epoch 163/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0422 \n",
      "Epoch 164/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9919 - loss: 0.0318 \n",
      "Epoch 165/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9891 - loss: 0.0286 \n",
      "Epoch 166/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9836 - loss: 0.0357 \n",
      "Epoch 167/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9905 - loss: 0.0290 \n",
      "Epoch 168/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0268 \n",
      "Epoch 169/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9930 - loss: 0.0253 \n",
      "Epoch 170/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9935 - loss: 0.0299 \n",
      "Epoch 171/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9844 - loss: 0.0398 \n",
      "Epoch 172/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9908 - loss: 0.0257 \n",
      "Epoch 173/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9895 - loss: 0.0293 \n",
      "Epoch 174/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.0326 \n",
      "Epoch 175/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9923 - loss: 0.0281 \n",
      "Epoch 176/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.0342 \n",
      "Epoch 177/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0380 \n",
      "Epoch 178/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0375 \n",
      "Epoch 179/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9878 - loss: 0.0279 \n",
      "Epoch 180/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9929 - loss: 0.0271 \n",
      "Epoch 181/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9820 - loss: 0.0415 \n",
      "Epoch 182/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9841 - loss: 0.0336 \n",
      "Epoch 183/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9797 - loss: 0.0350 \n",
      "Epoch 184/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0340 \n",
      "Epoch 185/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9742 - loss: 0.0377 \n",
      "Epoch 186/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0314 \n",
      "Epoch 187/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0390 \n",
      "Epoch 188/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9829 - loss: 0.0294 \n",
      "Epoch 189/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9920 - loss: 0.0255 \n",
      "Epoch 190/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9796 - loss: 0.0439 \n",
      "Epoch 191/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9821 - loss: 0.0329 \n",
      "Epoch 192/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9860 - loss: 0.0310 \n",
      "Epoch 193/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9956 - loss: 0.0262 \n",
      "Epoch 194/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9876 - loss: 0.0302 \n",
      "Epoch 195/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9966 - loss: 0.0258 \n",
      "Epoch 196/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9906 - loss: 0.0257 \n",
      "Epoch 197/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9915 - loss: 0.0237 \n",
      "Epoch 198/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9844 - loss: 0.0317 \n",
      "Epoch 199/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9749 - loss: 0.0521 \n",
      "Epoch 200/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9929 - loss: 0.0236 \n",
      "Epoch 201/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0370 \n",
      "Epoch 202/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9945 - loss: 0.0224 \n",
      "Epoch 203/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9945 - loss: 0.0242 \n",
      "Epoch 204/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9896 - loss: 0.0305 \n",
      "Epoch 205/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9813 - loss: 0.0283 \n",
      "Epoch 206/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9796 - loss: 0.0337 \n",
      "Epoch 207/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9923 - loss: 0.0231 \n",
      "Epoch 208/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9935 - loss: 0.0179 \n",
      "Epoch 209/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0289 \n",
      "Epoch 210/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0282 \n",
      "Epoch 211/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9907 - loss: 0.0275 \n",
      "Epoch 212/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9836 - loss: 0.0353 \n",
      "Epoch 213/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9842 - loss: 0.0347 \n",
      "Epoch 214/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9836 - loss: 0.0368 \n",
      "Epoch 215/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9749 - loss: 0.0503 \n",
      "Epoch 216/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9891 - loss: 0.0316 \n",
      "Epoch 217/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9836 - loss: 0.0305 \n",
      "Epoch 218/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0310 \n",
      "Epoch 219/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0277 \n",
      "Epoch 220/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0265 \n",
      "Epoch 221/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9844 - loss: 0.0324 \n",
      "Epoch 222/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9796 - loss: 0.0383 \n",
      "Epoch 223/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0335 \n",
      "Epoch 224/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9732 - loss: 0.0393 \n",
      "Epoch 225/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9857 - loss: 0.0332 \n",
      "Epoch 226/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9888 - loss: 0.0250 \n",
      "Epoch 227/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9973 - loss: 0.0232 \n",
      "Epoch 228/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9796 - loss: 0.0274 \n",
      "Epoch 229/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0332 \n",
      "Epoch 230/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9937 - loss: 0.0263 \n",
      "Epoch 231/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9844 - loss: 0.0328 \n",
      "Epoch 232/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9962 - loss: 0.0286 \n",
      "Epoch 233/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0297 \n",
      "Epoch 234/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0247 \n",
      "Epoch 235/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9836 - loss: 0.0303 \n",
      "Epoch 236/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9935 - loss: 0.0213 \n",
      "Epoch 237/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0284 \n",
      "Epoch 238/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9923 - loss: 0.0245 \n",
      "Epoch 239/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9710 - loss: 0.0406 \n",
      "Epoch 240/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0283 \n",
      "Epoch 241/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9947 - loss: 0.0169 \n",
      "Epoch 242/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9831 - loss: 0.0296 \n",
      "Epoch 243/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9890 - loss: 0.0210 \n",
      "Epoch 244/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9966 - loss: 0.0202 \n",
      "Epoch 245/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9809 - loss: 0.0322 \n",
      "Epoch 246/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9945 - loss: 0.0239 \n",
      "Epoch 247/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9868 - loss: 0.0339 \n",
      "Epoch 248/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0278 \n",
      "Epoch 249/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.0297 \n",
      "Epoch 250/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9935 - loss: 0.0221 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ef65ccf82f0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, epochs=250, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "0.9473684210526315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[14,  0,  0],\n",
       "       [ 0, 14,  2],\n",
       "       [ 0,  0,  8]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "yy_pred = np.argmax(y_pred,axis=1)\n",
    "yy_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "print(accuracy_score(yy_test,yy_pred))\n",
    "confusion_matrix(yy_test,yy_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0. ,   0. , 100. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [  0. ,  99.7,   0.1],\n",
       "       [  0.4,  99.8,   0. ],\n",
       "       [  0. ,   0. , 100. ],\n",
       "       [  0. ,  18.7,  84. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [  0. ,  98. ,   0.8],\n",
       "       [ 99.6,   0.6,   0. ],\n",
       "       [  0.1,  99.8,   0. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [ 99.9,   0.1,   0. ],\n",
       "       [  0. ,   0. , 100. ],\n",
       "       [ 99.9,   0.1,   0. ],\n",
       "       [  0. ,  99.9,   0. ],\n",
       "       [  0. ,   0.4,  99.9],\n",
       "       [ 99.9,   0.1,   0. ],\n",
       "       [  0.1,  99.9,   0. ],\n",
       "       [  0. ,  99.2,   0.3],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [  0. ,  99.9,   0. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [  0.6,  99.7,   0. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [  0. ,  81.2,  12.6],\n",
       "       [  0. ,   0.1, 100. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [  0. ,   4.6,  97.4],\n",
       "       [  0. ,  99.1,   0.3],\n",
       "       [  0.2,  99.9,   0. ],\n",
       "       [  0. ,  44.2,  56.8],\n",
       "       [  0. ,  99.6,   0.1],\n",
       "       [  0. ,  98.2,   0.7],\n",
       "       [  0. ,   0. , 100. ],\n",
       "       [  0. ,   0. , 100. ]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(100*y_pred,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Large Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "x_digits = digits['data']\n",
    "y_digits = digits['target']\n",
    "\n",
    "labeler = LabelBinarizer()\n",
    "\n",
    "yy_digits = labeler.fit_transform(y_digits)\n",
    "xx_digits = x_digits.reshape(1797,8,8,1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(xx_digits, yy_digits)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(8,8,1)))\n",
    "model.add(Conv2D(32, (3, 2), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9975 - loss: 0.0183 \n",
      "Epoch 2/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9966 - loss: 0.0200 \n",
      "Epoch 3/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9986 - loss: 0.0149 \n",
      "Epoch 4/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9993 - loss: 0.0127 \n",
      "Epoch 5/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9988 - loss: 0.0140 \n",
      "Epoch 6/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9993 - loss: 0.0107 \n",
      "Epoch 7/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9995 - loss: 0.0103 \n",
      "Epoch 8/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9993 - loss: 0.0096 \n",
      "Epoch 9/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9993 - loss: 0.0086 \n",
      "Epoch 10/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9997 - loss: 0.0071 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ef73fc93e30>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step  \n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "yy_pred = np.argmax(y_pred,axis=1)\n",
    "yy_test = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        43\n",
      "           1       0.96      1.00      0.98        50\n",
      "           2       1.00      1.00      1.00        41\n",
      "           3       1.00      1.00      1.00        47\n",
      "           4       1.00      1.00      1.00        39\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        51\n",
      "           7       0.98      0.96      0.97        48\n",
      "           8       0.98      0.94      0.96        47\n",
      "           9       0.98      1.00      0.99        43\n",
      "\n",
      "    accuracy                           0.99       450\n",
      "   macro avg       0.99      0.99      0.99       450\n",
      "weighted avg       0.99      0.99      0.99       450\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[43,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 50,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 41,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 47,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 39,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 41,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0, 51,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 46,  1,  1],\n",
       "       [ 0,  2,  0,  0,  0,  0,  0,  1, 44,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 43]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(yy_test,yy_pred))\n",
    "confusion_matrix(yy_test,yy_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yet Another Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces(data_home='/home/kaygun/local/data/scikit_learn_data/')\n",
    "binarizer = LabelBinarizer()\n",
    "\n",
    "y = binarizer.fit_transform(faces.target.flatten()).reshape(-1,40)\n",
    "X = faces.data.flatten().reshape(-1,4096)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 3.8238 - val_loss: 3.7896\n",
      "Epoch 2/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.7030 - val_loss: 3.7817\n",
      "Epoch 3/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.6520 - val_loss: 3.7342\n",
      "Epoch 4/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3.6428 - val_loss: 3.7029\n",
      "Epoch 5/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3.6011 - val_loss: 3.6871\n",
      "Epoch 6/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3.5587 - val_loss: 3.6852\n",
      "Epoch 7/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 3.4955 - val_loss: 3.6371\n",
      "Epoch 8/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 3.4448 - val_loss: 3.6269\n",
      "Epoch 9/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.3865 - val_loss: 3.5295\n",
      "Epoch 10/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 3.3029 - val_loss: 3.4962\n",
      "Epoch 11/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.1657 - val_loss: 3.3882\n",
      "Epoch 12/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 3.1058 - val_loss: 3.2854\n",
      "Epoch 13/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.9890 - val_loss: 3.2022\n",
      "Epoch 14/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.8561 - val_loss: 3.1111\n",
      "Epoch 15/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.6790 - val_loss: 2.9704\n",
      "Epoch 16/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 2.6104 - val_loss: 2.8859\n",
      "Epoch 17/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.4912 - val_loss: 2.7499\n",
      "Epoch 18/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 2.4005 - val_loss: 2.6238\n",
      "Epoch 19/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.2373 - val_loss: 2.4909\n",
      "Epoch 20/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 2.1915 - val_loss: 2.3552\n",
      "Epoch 21/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.0283 - val_loss: 2.2690\n",
      "Epoch 22/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.8876 - val_loss: 2.1956\n",
      "Epoch 23/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.8625 - val_loss: 2.1173\n",
      "Epoch 24/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.6632 - val_loss: 1.9938\n",
      "Epoch 25/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.5412 - val_loss: 1.8813\n",
      "Epoch 26/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.4308 - val_loss: 1.7980\n",
      "Epoch 27/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.3572 - val_loss: 1.7197\n",
      "Epoch 28/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.2924 - val_loss: 1.7321\n",
      "Epoch 29/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.2515 - val_loss: 1.5576\n",
      "Epoch 30/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.1501 - val_loss: 1.5904\n",
      "Epoch 31/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.0922 - val_loss: 1.4685\n",
      "Epoch 32/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.0253 - val_loss: 1.4313\n",
      "Epoch 33/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.9482 - val_loss: 1.3575\n",
      "Epoch 34/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.8804 - val_loss: 1.2959\n",
      "Epoch 35/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.8755 - val_loss: 1.2086\n",
      "Epoch 36/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.7674 - val_loss: 1.3743\n",
      "Epoch 37/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.7879 - val_loss: 1.2550\n",
      "Epoch 38/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.7180 - val_loss: 1.0978\n",
      "Epoch 39/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.6395 - val_loss: 1.1766\n",
      "Epoch 40/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6439 - val_loss: 1.0345\n",
      "Epoch 41/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.5494 - val_loss: 1.0386\n",
      "Epoch 42/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.5924 - val_loss: 1.1019\n",
      "Epoch 43/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.5904 - val_loss: 0.9215\n",
      "Epoch 44/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.5320 - val_loss: 0.9677\n",
      "Epoch 45/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5213 - val_loss: 1.0743\n",
      "Epoch 46/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.5176 - val_loss: 0.8725\n",
      "Epoch 47/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4408 - val_loss: 0.8229\n",
      "Epoch 48/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4140 - val_loss: 0.8604\n",
      "Epoch 49/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3969 - val_loss: 0.7465\n",
      "Epoch 50/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3571 - val_loss: 0.7730\n",
      "Epoch 51/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3402 - val_loss: 0.6972\n",
      "Epoch 52/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2777 - val_loss: 0.7051\n",
      "Epoch 53/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2481 - val_loss: 0.6665\n",
      "Epoch 54/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2698 - val_loss: 0.7255\n",
      "Epoch 55/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2693 - val_loss: 0.6455\n",
      "Epoch 56/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2526 - val_loss: 0.6560\n",
      "Epoch 57/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2389 - val_loss: 0.6807\n",
      "Epoch 58/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2016 - val_loss: 0.6311\n",
      "Epoch 59/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2262 - val_loss: 0.5987\n",
      "Epoch 60/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2157 - val_loss: 0.5887\n",
      "Epoch 61/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2012 - val_loss: 0.6047\n",
      "Epoch 62/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.1665 - val_loss: 0.6295\n",
      "Epoch 63/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.1756 - val_loss: 0.6131\n",
      "Epoch 64/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1786 - val_loss: 0.5603\n",
      "Epoch 65/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1661 - val_loss: 0.5682\n",
      "Epoch 66/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1517 - val_loss: 0.5351\n",
      "Epoch 67/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1442 - val_loss: 0.5194\n",
      "Epoch 68/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.1257 - val_loss: 0.5039\n",
      "Epoch 69/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.1198 - val_loss: 0.5465\n",
      "Epoch 70/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.1182 - val_loss: 0.5265\n",
      "Epoch 71/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1292 - val_loss: 0.5752\n",
      "Epoch 72/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1226 - val_loss: 0.5344\n",
      "Epoch 73/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1245 - val_loss: 0.5222\n",
      "Epoch 74/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0974 - val_loss: 0.4752\n",
      "Epoch 75/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0930 - val_loss: 0.5525\n",
      "Epoch 76/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1000 - val_loss: 0.4896\n",
      "Epoch 77/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0875 - val_loss: 0.4704\n",
      "Epoch 78/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0914 - val_loss: 0.5113\n",
      "Epoch 79/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0960 - val_loss: 0.4409\n",
      "Epoch 80/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0709 - val_loss: 0.4629\n",
      "Epoch 81/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0758 - val_loss: 0.4861\n",
      "Epoch 82/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0842 - val_loss: 0.4889\n",
      "Epoch 83/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0804 - val_loss: 0.4295\n",
      "Epoch 84/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0672 - val_loss: 0.4724\n",
      "Epoch 85/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0633 - val_loss: 0.4196\n",
      "Epoch 86/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0613 - val_loss: 0.4310\n",
      "Epoch 87/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0559 - val_loss: 0.4172\n",
      "Epoch 88/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0503 - val_loss: 0.4268\n",
      "Epoch 89/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0534 - val_loss: 0.4078\n",
      "Epoch 90/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0488 - val_loss: 0.4069\n",
      "Epoch 91/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0493 - val_loss: 0.4151\n",
      "Epoch 92/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0509 - val_loss: 0.4411\n",
      "Epoch 93/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0489 - val_loss: 0.4037\n",
      "Epoch 94/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0417 - val_loss: 0.4359\n",
      "Epoch 95/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0487 - val_loss: 0.3910\n",
      "Epoch 96/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0420 - val_loss: 0.4262\n",
      "Epoch 97/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0405 - val_loss: 0.3869\n",
      "Epoch 98/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0364 - val_loss: 0.4044\n",
      "Epoch 99/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0377 - val_loss: 0.4011\n",
      "Epoch 100/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0394 - val_loss: 0.4109\n",
      "Epoch 101/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0354 - val_loss: 0.4075\n",
      "Epoch 102/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0385 - val_loss: 0.3789\n",
      "Epoch 103/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0330 - val_loss: 0.4035\n",
      "Epoch 104/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0300 - val_loss: 0.3835\n",
      "Epoch 105/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0289 - val_loss: 0.3984\n",
      "Epoch 106/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0297 - val_loss: 0.3726\n",
      "Epoch 107/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0309 - val_loss: 0.3914\n",
      "Epoch 108/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0275 - val_loss: 0.3739\n",
      "Epoch 109/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0259 - val_loss: 0.3834\n",
      "Epoch 110/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0266 - val_loss: 0.3711\n",
      "Epoch 111/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0253 - val_loss: 0.3819\n",
      "Epoch 112/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0265 - val_loss: 0.3798\n",
      "Epoch 113/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0256 - val_loss: 0.3624\n",
      "Epoch 114/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0207 - val_loss: 0.3924\n",
      "Epoch 115/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0244 - val_loss: 0.3724\n",
      "Epoch 116/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0215 - val_loss: 0.3600\n",
      "Epoch 117/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0223 - val_loss: 0.3945\n",
      "Epoch 118/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0217 - val_loss: 0.3732\n",
      "Epoch 119/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0220 - val_loss: 0.3746\n",
      "Epoch 120/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0212 - val_loss: 0.3633\n",
      "Epoch 121/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0197 - val_loss: 0.3555\n",
      "Epoch 122/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0191 - val_loss: 0.3833\n",
      "Epoch 123/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0188 - val_loss: 0.3590\n",
      "Epoch 124/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0185 - val_loss: 0.3581\n",
      "Epoch 125/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0169 - val_loss: 0.3576\n",
      "Epoch 126/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0182 - val_loss: 0.3730\n",
      "Epoch 127/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0142 - val_loss: 0.3608\n",
      "Epoch 128/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0173 - val_loss: 0.3641\n",
      "Epoch 129/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0167 - val_loss: 0.3511\n",
      "Epoch 130/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0147 - val_loss: 0.3692\n",
      "Epoch 131/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0162 - val_loss: 0.3667\n",
      "Epoch 132/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0146 - val_loss: 0.3505\n",
      "Epoch 133/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0139 - val_loss: 0.3588\n",
      "Epoch 134/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0134 - val_loss: 0.3481\n",
      "Epoch 135/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0125 - val_loss: 0.3556\n",
      "Epoch 136/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0124 - val_loss: 0.3607\n",
      "Epoch 137/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0127 - val_loss: 0.3710\n",
      "Epoch 138/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0124 - val_loss: 0.3504\n",
      "Epoch 139/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0117 - val_loss: 0.3468\n",
      "Epoch 140/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0123 - val_loss: 0.3513\n",
      "Epoch 141/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0132 - val_loss: 0.3550\n",
      "Epoch 142/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0122 - val_loss: 0.3603\n",
      "Epoch 143/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0111 - val_loss: 0.3392\n",
      "Epoch 144/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0117 - val_loss: 0.3454\n",
      "Epoch 145/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0112 - val_loss: 0.3540\n",
      "Epoch 146/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0115 - val_loss: 0.3428\n",
      "Epoch 147/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0106 - val_loss: 0.3532\n",
      "Epoch 148/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0109 - val_loss: 0.3514\n",
      "Epoch 149/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0105 - val_loss: 0.3476\n",
      "Epoch 150/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0098 - val_loss: 0.3492\n",
      "Epoch 151/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0102 - val_loss: 0.3487\n",
      "Epoch 152/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0093 - val_loss: 0.3419\n",
      "Epoch 153/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0099 - val_loss: 0.3436\n",
      "Epoch 154/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0093 - val_loss: 0.3450\n",
      "Epoch 155/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0094 - val_loss: 0.3446\n",
      "Epoch 156/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0089 - val_loss: 0.3433\n",
      "Epoch 157/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0087 - val_loss: 0.3477\n",
      "Epoch 158/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0090 - val_loss: 0.3400\n",
      "Epoch 159/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0089 - val_loss: 0.3493\n",
      "Epoch 160/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0085 - val_loss: 0.3451\n",
      "Epoch 161/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0081 - val_loss: 0.3355\n",
      "Epoch 162/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0083 - val_loss: 0.3567\n",
      "Epoch 163/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0079 - val_loss: 0.3391\n",
      "Epoch 164/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0081 - val_loss: 0.3452\n",
      "Epoch 165/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0081 - val_loss: 0.3445\n",
      "Epoch 166/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0082 - val_loss: 0.3333\n",
      "Epoch 167/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0071 - val_loss: 0.3476\n",
      "Epoch 168/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0080 - val_loss: 0.3369\n",
      "Epoch 169/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0072 - val_loss: 0.3321\n",
      "Epoch 170/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0072 - val_loss: 0.3456\n",
      "Epoch 171/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0072 - val_loss: 0.3405\n",
      "Epoch 172/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0067 - val_loss: 0.3307\n",
      "Epoch 173/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0076 - val_loss: 0.3430\n",
      "Epoch 174/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0065 - val_loss: 0.3415\n",
      "Epoch 175/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0069 - val_loss: 0.3314\n",
      "Epoch 176/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0062 - val_loss: 0.3367\n",
      "Epoch 177/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0061 - val_loss: 0.3428\n",
      "Epoch 178/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0066 - val_loss: 0.3423\n",
      "Epoch 179/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0065 - val_loss: 0.3367\n",
      "Epoch 180/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0061 - val_loss: 0.3322\n",
      "Epoch 181/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0063 - val_loss: 0.3405\n",
      "Epoch 182/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0058 - val_loss: 0.3373\n",
      "Epoch 183/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0061 - val_loss: 0.3387\n",
      "Epoch 184/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0061 - val_loss: 0.3360\n",
      "Epoch 185/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0060 - val_loss: 0.3420\n",
      "Epoch 186/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0061 - val_loss: 0.3295\n",
      "Epoch 187/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0063 - val_loss: 0.3365\n",
      "Epoch 188/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0057 - val_loss: 0.3306\n",
      "Epoch 189/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0056 - val_loss: 0.3356\n",
      "Epoch 190/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0053 - val_loss: 0.3457\n",
      "Epoch 191/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0056 - val_loss: 0.3316\n",
      "Epoch 192/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0056 - val_loss: 0.3376\n",
      "Epoch 193/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0052 - val_loss: 0.3265\n",
      "Epoch 194/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0054 - val_loss: 0.3225\n",
      "Epoch 195/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0049 - val_loss: 0.3390\n",
      "Epoch 196/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0048 - val_loss: 0.3383\n",
      "Epoch 197/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0053 - val_loss: 0.3268\n",
      "Epoch 198/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0048 - val_loss: 0.3314\n",
      "Epoch 199/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0051 - val_loss: 0.3383\n",
      "Epoch 200/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0046 - val_loss: 0.3282\n",
      "Epoch 201/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0044 - val_loss: 0.3279\n",
      "Epoch 202/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0046 - val_loss: 0.3279\n",
      "Epoch 203/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0050 - val_loss: 0.3343\n",
      "Epoch 204/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0043 - val_loss: 0.3338\n",
      "Epoch 205/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0042 - val_loss: 0.3235\n",
      "Epoch 206/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0046 - val_loss: 0.3311\n",
      "Epoch 207/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0044 - val_loss: 0.3286\n",
      "Epoch 208/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0044 - val_loss: 0.3321\n",
      "Epoch 209/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0044 - val_loss: 0.3276\n",
      "Epoch 210/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0035 - val_loss: 0.3307\n",
      "Epoch 211/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0043 - val_loss: 0.3320\n",
      "Epoch 212/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0042 - val_loss: 0.3273\n",
      "Epoch 213/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0040 - val_loss: 0.3200\n",
      "Epoch 214/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0043 - val_loss: 0.3312\n",
      "Epoch 215/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0038 - val_loss: 0.3333\n",
      "Epoch 216/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0043 - val_loss: 0.3218\n",
      "Epoch 217/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0038 - val_loss: 0.3244\n",
      "Epoch 218/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0040 - val_loss: 0.3383\n",
      "Epoch 219/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0040 - val_loss: 0.3307\n",
      "Epoch 220/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0037 - val_loss: 0.3238\n",
      "Epoch 221/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0037 - val_loss: 0.3328\n",
      "Epoch 222/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0035 - val_loss: 0.3298\n",
      "Epoch 223/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0038 - val_loss: 0.3233\n",
      "Epoch 224/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0035 - val_loss: 0.3282\n",
      "Epoch 225/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0034 - val_loss: 0.3239\n",
      "Epoch 226/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0034 - val_loss: 0.3277\n",
      "Epoch 227/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0036 - val_loss: 0.3326\n",
      "Epoch 228/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0032 - val_loss: 0.3238\n",
      "Epoch 229/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0035 - val_loss: 0.3203\n",
      "Epoch 230/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0033 - val_loss: 0.3333\n",
      "Epoch 231/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0034 - val_loss: 0.3300\n",
      "Epoch 232/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0033 - val_loss: 0.3265\n",
      "Epoch 233/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0031 - val_loss: 0.3306\n",
      "Epoch 234/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0031 - val_loss: 0.3237\n",
      "Epoch 235/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0032 - val_loss: 0.3199\n",
      "Epoch 236/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0034 - val_loss: 0.3357\n",
      "Epoch 237/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0032 - val_loss: 0.3259\n",
      "Epoch 238/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0031 - val_loss: 0.3274\n",
      "Epoch 239/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0033 - val_loss: 0.3325\n",
      "Epoch 240/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0031 - val_loss: 0.3269\n",
      "Epoch 241/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0030 - val_loss: 0.3223\n",
      "Epoch 242/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0031 - val_loss: 0.3210\n",
      "Epoch 243/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0031 - val_loss: 0.3242\n",
      "Epoch 244/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0030 - val_loss: 0.3297\n",
      "Epoch 245/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0027 - val_loss: 0.3187\n",
      "Epoch 246/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0029 - val_loss: 0.3259\n",
      "Epoch 247/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0026 - val_loss: 0.3300\n",
      "Epoch 248/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0029 - val_loss: 0.3184\n",
      "Epoch 249/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0027 - val_loss: 0.3300\n",
      "Epoch 250/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0027 - val_loss: 0.3290\n",
      "Epoch 251/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0027 - val_loss: 0.3240\n",
      "Epoch 252/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0029 - val_loss: 0.3195\n",
      "Epoch 253/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0027 - val_loss: 0.3196\n",
      "Epoch 254/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0028 - val_loss: 0.3274\n",
      "Epoch 255/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0028 - val_loss: 0.3226\n",
      "Epoch 256/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0025 - val_loss: 0.3228\n",
      "Epoch 257/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0025 - val_loss: 0.3262\n",
      "Epoch 258/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0025 - val_loss: 0.3170\n",
      "Epoch 259/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0026 - val_loss: 0.3229\n",
      "Epoch 260/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0025 - val_loss: 0.3244\n",
      "Epoch 261/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0025 - val_loss: 0.3176\n",
      "Epoch 262/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0024 - val_loss: 0.3279\n",
      "Epoch 263/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0025 - val_loss: 0.3262\n",
      "Epoch 264/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0024 - val_loss: 0.3183\n",
      "Epoch 265/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0023 - val_loss: 0.3230\n",
      "Epoch 266/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0023 - val_loss: 0.3216\n",
      "Epoch 267/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0022 - val_loss: 0.3178\n",
      "Epoch 268/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0023 - val_loss: 0.3202\n",
      "Epoch 269/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0023 - val_loss: 0.3181\n",
      "Epoch 270/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0022 - val_loss: 0.3197\n",
      "Epoch 271/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0021 - val_loss: 0.3182\n",
      "Epoch 272/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0021 - val_loss: 0.3230\n",
      "Epoch 273/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0023 - val_loss: 0.3186\n",
      "Epoch 274/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 0.3199\n",
      "Epoch 275/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0021 - val_loss: 0.3220\n",
      "Epoch 276/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0023 - val_loss: 0.3201\n",
      "Epoch 277/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0021 - val_loss: 0.3204\n",
      "Epoch 278/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 0.3173\n",
      "Epoch 279/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0024 - val_loss: 0.3186\n",
      "Epoch 280/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 0.3267\n",
      "Epoch 281/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0022 - val_loss: 0.3211\n",
      "Epoch 282/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0019 - val_loss: 0.3190\n",
      "Epoch 283/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0019 - val_loss: 0.3184\n",
      "Epoch 284/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0019 - val_loss: 0.3203\n",
      "Epoch 285/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0020 - val_loss: 0.3221\n",
      "Epoch 286/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0019 - val_loss: 0.3255\n",
      "Epoch 287/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0019 - val_loss: 0.3225\n",
      "Epoch 288/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0018 - val_loss: 0.3187\n",
      "Epoch 289/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0018 - val_loss: 0.3205\n",
      "Epoch 290/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 0.3212\n",
      "Epoch 291/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0019 - val_loss: 0.3201\n",
      "Epoch 292/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0018 - val_loss: 0.3202\n",
      "Epoch 293/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0018 - val_loss: 0.3206\n",
      "Epoch 294/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0018 - val_loss: 0.3202\n",
      "Epoch 295/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0018 - val_loss: 0.3233\n",
      "Epoch 296/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0019 - val_loss: 0.3236\n",
      "Epoch 297/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0019 - val_loss: 0.3237\n",
      "Epoch 298/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0018 - val_loss: 0.3160\n",
      "Epoch 299/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0017 - val_loss: 0.3233\n",
      "Epoch 300/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0017 - val_loss: 0.3253\n",
      "Epoch 301/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0017 - val_loss: 0.3190\n",
      "Epoch 302/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0018 - val_loss: 0.3211\n",
      "Epoch 303/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0016 - val_loss: 0.3233\n",
      "Epoch 304/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0017 - val_loss: 0.3153\n",
      "Epoch 305/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0017 - val_loss: 0.3183\n",
      "Epoch 306/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0017 - val_loss: 0.3225\n",
      "Epoch 307/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0016 - val_loss: 0.3199\n",
      "Epoch 308/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 0.3222\n",
      "Epoch 309/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0016 - val_loss: 0.3239\n",
      "Epoch 310/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 0.3154\n",
      "Epoch 311/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0017 - val_loss: 0.3160\n",
      "Epoch 312/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0015 - val_loss: 0.3232\n",
      "Epoch 313/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0016 - val_loss: 0.3200\n",
      "Epoch 314/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 0.3188\n",
      "Epoch 315/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0015 - val_loss: 0.3170\n",
      "Epoch 316/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0015 - val_loss: 0.3206\n",
      "Epoch 317/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0015 - val_loss: 0.3215\n",
      "Epoch 318/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0016 - val_loss: 0.3160\n",
      "Epoch 319/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0014 - val_loss: 0.3177\n",
      "Epoch 320/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0015 - val_loss: 0.3225\n",
      "Epoch 321/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0014 - val_loss: 0.3190\n",
      "Epoch 322/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0015 - val_loss: 0.3191\n",
      "Epoch 323/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0014 - val_loss: 0.3190\n",
      "Epoch 324/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0014 - val_loss: 0.3211\n",
      "Epoch 325/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0014 - val_loss: 0.3163\n",
      "Epoch 326/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0013 - val_loss: 0.3220\n",
      "Epoch 327/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0014 - val_loss: 0.3198\n",
      "Epoch 328/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0015 - val_loss: 0.3184\n",
      "Epoch 329/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0014 - val_loss: 0.3168\n",
      "Epoch 330/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0015 - val_loss: 0.3216\n",
      "Epoch 331/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0014 - val_loss: 0.3195\n",
      "Epoch 332/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0014 - val_loss: 0.3191\n",
      "Epoch 333/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0014 - val_loss: 0.3197\n",
      "Epoch 334/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0014 - val_loss: 0.3197\n",
      "Epoch 335/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0013 - val_loss: 0.3218\n",
      "Epoch 336/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0014 - val_loss: 0.3178\n",
      "Epoch 337/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 0.3174\n",
      "Epoch 338/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0012 - val_loss: 0.3166\n",
      "Epoch 339/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0013 - val_loss: 0.3167\n",
      "Epoch 340/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0013 - val_loss: 0.3161\n",
      "Epoch 341/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0013 - val_loss: 0.3150\n",
      "Epoch 342/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0012 - val_loss: 0.3185\n",
      "Epoch 343/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0012 - val_loss: 0.3198\n",
      "Epoch 344/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0012 - val_loss: 0.3153\n",
      "Epoch 345/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0012 - val_loss: 0.3175\n",
      "Epoch 346/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0013 - val_loss: 0.3175\n",
      "Epoch 347/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0012 - val_loss: 0.3129\n",
      "Epoch 348/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0012 - val_loss: 0.3163\n",
      "Epoch 349/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0012 - val_loss: 0.3167\n",
      "Epoch 350/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0013 - val_loss: 0.3193\n",
      "Epoch 351/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0012 - val_loss: 0.3195\n",
      "Epoch 352/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0012 - val_loss: 0.3142\n",
      "Epoch 353/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0012 - val_loss: 0.3123\n",
      "Epoch 354/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0012 - val_loss: 0.3158\n",
      "Epoch 355/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0011 - val_loss: 0.3188\n",
      "Epoch 356/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0011 - val_loss: 0.3164\n",
      "Epoch 357/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0011 - val_loss: 0.3183\n",
      "Epoch 358/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0012 - val_loss: 0.3214\n",
      "Epoch 359/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0011 - val_loss: 0.3165\n",
      "Epoch 360/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0010 - val_loss: 0.3170\n",
      "Epoch 361/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0011 - val_loss: 0.3163\n",
      "Epoch 362/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0011 - val_loss: 0.3157\n",
      "Epoch 363/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0011 - val_loss: 0.3163\n",
      "Epoch 364/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0011 - val_loss: 0.3194\n",
      "Epoch 365/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0011 - val_loss: 0.3202\n",
      "Epoch 366/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0010 - val_loss: 0.3151\n",
      "Epoch 367/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0011 - val_loss: 0.3174\n",
      "Epoch 368/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0010 - val_loss: 0.3147\n",
      "Epoch 369/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.8317e-04 - val_loss: 0.3138\n",
      "Epoch 370/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0011 - val_loss: 0.3194\n",
      "Epoch 371/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0011 - val_loss: 0.3188\n",
      "Epoch 372/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0011 - val_loss: 0.3139\n",
      "Epoch 373/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0011 - val_loss: 0.3117\n",
      "Epoch 374/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.8963e-04 - val_loss: 0.3140\n",
      "Epoch 375/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0010 - val_loss: 0.3183\n",
      "Epoch 376/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0010 - val_loss: 0.3158\n",
      "Epoch 377/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0010 - val_loss: 0.3183\n",
      "Epoch 378/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0011 - val_loss: 0.3147\n",
      "Epoch 379/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0010 - val_loss: 0.3178\n",
      "Epoch 380/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.4087e-04 - val_loss: 0.3153\n",
      "Epoch 381/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.7455e-04 - val_loss: 0.3131\n",
      "Epoch 382/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0010 - val_loss: 0.3157\n",
      "Epoch 383/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.8689e-04 - val_loss: 0.3181\n",
      "Epoch 384/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0010 - val_loss: 0.3151\n",
      "Epoch 385/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 9.1890e-04 - val_loss: 0.3151\n",
      "Epoch 386/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.2689e-04 - val_loss: 0.3134\n",
      "Epoch 387/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 9.6754e-04 - val_loss: 0.3169\n",
      "Epoch 388/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.0059e-04 - val_loss: 0.3154\n",
      "Epoch 389/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.6813e-04 - val_loss: 0.3131\n",
      "Epoch 390/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.9803e-04 - val_loss: 0.3136\n",
      "Epoch 391/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 9.2579e-04 - val_loss: 0.3179\n",
      "Epoch 392/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.5442e-04 - val_loss: 0.3148\n",
      "Epoch 393/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.3639e-04 - val_loss: 0.3115\n",
      "Epoch 394/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.9196e-04 - val_loss: 0.3139\n",
      "Epoch 395/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.2455e-04 - val_loss: 0.3131\n",
      "Epoch 396/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.2557e-04 - val_loss: 0.3136\n",
      "Epoch 397/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.8821e-04 - val_loss: 0.3145\n",
      "Epoch 398/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.8973e-04 - val_loss: 0.3154\n",
      "Epoch 399/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 8.7228e-04 - val_loss: 0.3146\n",
      "Epoch 400/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.4793e-04 - val_loss: 0.3130\n",
      "Epoch 401/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.9720e-04 - val_loss: 0.3167\n",
      "Epoch 402/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 8.7531e-04 - val_loss: 0.3175\n",
      "Epoch 403/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 7.9911e-04 - val_loss: 0.3149\n",
      "Epoch 404/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 7.6714e-04 - val_loss: 0.3125\n",
      "Epoch 405/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 8.4417e-04 - val_loss: 0.3172\n",
      "Epoch 406/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 8.2893e-04 - val_loss: 0.3150\n",
      "Epoch 407/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.8353e-04 - val_loss: 0.3144\n",
      "Epoch 408/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.0752e-04 - val_loss: 0.3166\n",
      "Epoch 409/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.7561e-04 - val_loss: 0.3164\n",
      "Epoch 410/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.2756e-04 - val_loss: 0.3149\n",
      "Epoch 411/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.3699e-04 - val_loss: 0.3147\n",
      "Epoch 412/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.2767e-04 - val_loss: 0.3116\n",
      "Epoch 413/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 7.9719e-04 - val_loss: 0.3148\n",
      "Epoch 414/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.0705e-04 - val_loss: 0.3167\n",
      "Epoch 415/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.7342e-04 - val_loss: 0.3160\n",
      "Epoch 416/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.7399e-04 - val_loss: 0.3130\n",
      "Epoch 417/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 7.2462e-04 - val_loss: 0.3184\n",
      "Epoch 418/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 7.6749e-04 - val_loss: 0.3128\n",
      "Epoch 419/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.5762e-04 - val_loss: 0.3131\n",
      "Epoch 420/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.3406e-04 - val_loss: 0.3135\n",
      "Epoch 421/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.4533e-04 - val_loss: 0.3140\n",
      "Epoch 422/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.0744e-04 - val_loss: 0.3140\n",
      "Epoch 423/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.3730e-04 - val_loss: 0.3133\n",
      "Epoch 424/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 7.7936e-04 - val_loss: 0.3122\n",
      "Epoch 425/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.0753e-04 - val_loss: 0.3123\n",
      "Epoch 426/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.4783e-04 - val_loss: 0.3163\n",
      "Epoch 427/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.3814e-04 - val_loss: 0.3118\n",
      "Epoch 428/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.9206e-04 - val_loss: 0.3146\n",
      "Epoch 429/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.7390e-04 - val_loss: 0.3114\n",
      "Epoch 430/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.7639e-04 - val_loss: 0.3139\n",
      "Epoch 431/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.4635e-04 - val_loss: 0.3140\n",
      "Epoch 432/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.0351e-04 - val_loss: 0.3136\n",
      "Epoch 433/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.8512e-04 - val_loss: 0.3104\n",
      "Epoch 434/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.5672e-04 - val_loss: 0.3156\n",
      "Epoch 435/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.0696e-04 - val_loss: 0.3157\n",
      "Epoch 436/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.7787e-04 - val_loss: 0.3123\n",
      "Epoch 437/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.7153e-04 - val_loss: 0.3145\n",
      "Epoch 438/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.9224e-04 - val_loss: 0.3128\n",
      "Epoch 439/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.6244e-04 - val_loss: 0.3135\n",
      "Epoch 440/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.9487e-04 - val_loss: 0.3121\n",
      "Epoch 441/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.8075e-04 - val_loss: 0.3124\n",
      "Epoch 442/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.9279e-04 - val_loss: 0.3104\n",
      "Epoch 443/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 6.6353e-04 - val_loss: 0.3131\n",
      "Epoch 444/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 6.2815e-04 - val_loss: 0.3128\n",
      "Epoch 445/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.3284e-04 - val_loss: 0.3152\n",
      "Epoch 446/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.0954e-04 - val_loss: 0.3124\n",
      "Epoch 447/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.2709e-04 - val_loss: 0.3110\n",
      "Epoch 448/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.5158e-04 - val_loss: 0.3119\n",
      "Epoch 449/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 6.3614e-04 - val_loss: 0.3150\n",
      "Epoch 450/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 6.4694e-04 - val_loss: 0.3123\n",
      "Epoch 451/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 6.0886e-04 - val_loss: 0.3138\n",
      "Epoch 452/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 5.9322e-04 - val_loss: 0.3150\n",
      "Epoch 453/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.2060e-04 - val_loss: 0.3117\n",
      "Epoch 454/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.8491e-04 - val_loss: 0.3127\n",
      "Epoch 455/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.0540e-04 - val_loss: 0.3132\n",
      "Epoch 456/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 6.0509e-04 - val_loss: 0.3129\n",
      "Epoch 457/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 5.6301e-04 - val_loss: 0.3115\n",
      "Epoch 458/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 5.8427e-04 - val_loss: 0.3155\n",
      "Epoch 459/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.2933e-04 - val_loss: 0.3154\n",
      "Epoch 460/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.8728e-04 - val_loss: 0.3119\n",
      "Epoch 461/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.1882e-04 - val_loss: 0.3106\n",
      "Epoch 462/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.1813e-04 - val_loss: 0.3141\n",
      "Epoch 463/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.9355e-04 - val_loss: 0.3144\n",
      "Epoch 464/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.8838e-04 - val_loss: 0.3146\n",
      "Epoch 465/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.9123e-04 - val_loss: 0.3124\n",
      "Epoch 466/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 5.8373e-04 - val_loss: 0.3139\n",
      "Epoch 467/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 5.4028e-04 - val_loss: 0.3124\n",
      "Epoch 468/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.7385e-04 - val_loss: 0.3148\n",
      "Epoch 469/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.9990e-04 - val_loss: 0.3142\n",
      "Epoch 470/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.3582e-04 - val_loss: 0.3131\n",
      "Epoch 471/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.7508e-04 - val_loss: 0.3167\n",
      "Epoch 472/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 5.8907e-04 - val_loss: 0.3118\n",
      "Epoch 473/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 5.5199e-04 - val_loss: 0.3128\n",
      "Epoch 474/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 5.5175e-04 - val_loss: 0.3117\n",
      "Epoch 475/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 5.2371e-04 - val_loss: 0.3099\n",
      "Epoch 476/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 5.4753e-04 - val_loss: 0.3169\n",
      "Epoch 477/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 5.3322e-04 - val_loss: 0.3139\n",
      "Epoch 478/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 5.7188e-04 - val_loss: 0.3121\n",
      "Epoch 479/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 5.4149e-04 - val_loss: 0.3132\n",
      "Epoch 480/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 4.9880e-04 - val_loss: 0.3137\n",
      "Epoch 481/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 5.2890e-04 - val_loss: 0.3148\n",
      "Epoch 482/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.2499e-04 - val_loss: 0.3148\n",
      "Epoch 483/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.3328e-04 - val_loss: 0.3110\n",
      "Epoch 484/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 5.1783e-04 - val_loss: 0.3133\n",
      "Epoch 485/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 5.2005e-04 - val_loss: 0.3125\n",
      "Epoch 486/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 5.1753e-04 - val_loss: 0.3122\n",
      "Epoch 487/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 5.0512e-04 - val_loss: 0.3144\n",
      "Epoch 488/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 5.0772e-04 - val_loss: 0.3131\n",
      "Epoch 489/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 4.9542e-04 - val_loss: 0.3130\n",
      "Epoch 490/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 5.2398e-04 - val_loss: 0.3153\n",
      "Epoch 491/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 5.2816e-04 - val_loss: 0.3118\n",
      "Epoch 492/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 5.4610e-04 - val_loss: 0.3125\n",
      "Epoch 493/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 5.0600e-04 - val_loss: 0.3138\n",
      "Epoch 494/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 5.1956e-04 - val_loss: 0.3133\n",
      "Epoch 495/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 5.1282e-04 - val_loss: 0.3179\n",
      "Epoch 496/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 4.8363e-04 - val_loss: 0.3141\n",
      "Epoch 497/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 5.0169e-04 - val_loss: 0.3112\n",
      "Epoch 498/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 4.9967e-04 - val_loss: 0.3111\n",
      "Epoch 499/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.4904e-04 - val_loss: 0.3120\n",
      "Epoch 500/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.8147e-04 - val_loss: 0.3136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ef66c7e7770>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(4096,)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\",))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(40, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=500, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         4\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      0.75      0.86         4\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.50      0.67         2\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.67      1.00      0.80         2\n",
      "           7       1.00      1.00      1.00         3\n",
      "           9       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         1\n",
      "          11       0.50      1.00      0.67         1\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.67      1.00      0.80         2\n",
      "          14       1.00      1.00      1.00         5\n",
      "          15       0.75      1.00      0.86         3\n",
      "          16       1.00      1.00      1.00         2\n",
      "          17       1.00      1.00      1.00         4\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      0.75      0.86         4\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      1.00      1.00         2\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       1.00      1.00      1.00         1\n",
      "          25       1.00      0.75      0.86         4\n",
      "          26       1.00      1.00      1.00         6\n",
      "          27       1.00      1.00      1.00         1\n",
      "          28       1.00      1.00      1.00         2\n",
      "          29       1.00      1.00      1.00         6\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         5\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         1\n",
      "          35       1.00      0.75      0.86         4\n",
      "          36       1.00      1.00      1.00         3\n",
      "          37       1.00      1.00      1.00         2\n",
      "          38       1.00      1.00      1.00         2\n",
      "          39       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.92       100\n",
      "   macro avg       0.89      0.87      0.87       100\n",
      "weighted avg       0.96      0.92      0.93       100\n",
      "\n",
      "[[2 0 0 ... 0 0 0]\n",
      " [0 3 0 ... 0 0 0]\n",
      " [0 0 3 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 0 2 0]\n",
      " [0 0 0 ... 0 0 3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "yy_pred = np.argmax(y_pred,axis=1)\n",
    "yy_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "print(classification_report(yy_test,yy_pred))\n",
    "print(confusion_matrix(yy_test,yy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
