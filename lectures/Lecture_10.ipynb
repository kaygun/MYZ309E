{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 09:55:52.563260: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras as ks\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation   \n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "from keras.layers import Input, Dense, Conv2D, Dropout, Flatten, MaxPooling2D, LSTM, Embedding\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,  classification_report\n",
    "from sklearn.datasets import load_iris, load_digits, fetch_20newsgroups_vectorized, fetch_olivetti_faces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10 (Neural Networks)\n",
    "\n",
    "## The Gradient Descent\n",
    "\n",
    "Consider the following problem: we have a multivariable function $F(x_1,\\ldots,x_n)$ that we want to optimize, i.e. find the point at which $F$ attains its minimum or maximum. There is an iterative algorithm called [steepest descent algorithm](https://ocw.mit.edu/courses/mathematics/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/lecture-notes/MIT18_409F09_scribe21.pdf) that we can use to find this point. The algorithm uses the [gradient](https://inst.eecs.berkeley.edu/~ee127/sp21/livebook/def_gradient.html) of the function. Recall that the gradient $\\nabla F$ at a point $x$ \n",
    "\n",
    "$$ \\nabla F = \\left(\\frac{\\partial F}{\\partial x_1},\\ldots,\\frac{\\partial F}{\\partial x_n}\\right) $$\n",
    "\n",
    "gives us the direction at which $F$ has the largest (in absolute value) derivative. The algorithm uses this information and iteratively pushes an initial guess into better and better approximations of the optimum point. Let us start with an initial guess $x^{(0)} = (x_1^{(0)},\\ldots,x_n^{(0)})$ for a minimum of $F$, and move in the opposite direction of the gradient with a small step (called **learning rate**). Then the update rule for the path we are going to follow is\n",
    "\n",
    "$$ x^{(m+1)} = x^{(m)} - \\eta \\left(\\nabla F\\right)(x_1^{(m)},\\ldots,x_n^{(m)}) $$\n",
    "\n",
    "where $\\eta$ is called *the learning rate*. \n",
    "\n",
    "Now, let try to find a solution to $ f(x) = c $ using gradient descent. We convert the root-finding problem $f(x) = c$ into an optimization problem by minimizing the squared error $(f(x) - c)^2$. This approach is particularly effective when $f$ is differentiable but not invertible. Then we need the following function is minimized at $f(x)=c$\n",
    "\n",
    "$$ F(x) = \\frac{1}{2} (f(x) - c)^2 \\quad \\Rightarrow \\quad \\nabla F(x) = (f(x)-c) \\nabla f(x) $$\n",
    "\n",
    "The gradient $\\nabla F(x)$ points in the direction of steepest increase of the function $F$. Therefore, to minimize $F$, we move in the direction $-\\nabla F(x)$. So this update rule is:\n",
    "\n",
    "$$ x^{(k+1)} = x^{(k)} - \\eta (f(x^{(k)}) - c) \\nabla f(x^{(k)}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSolve(f, c, x0, lr=0.01, h=1e-5, tol=1e-5, n=15000):\n",
    "    x0 = np.array(x0, dtype=float)\n",
    "    dim = len(x0)\n",
    "    \n",
    "    def numerical_grad(f, x):\n",
    "        grad = np.zeros_like(x)\n",
    "        for i in range(len(x)):\n",
    "            dx = np.zeros_like(x)\n",
    "            dx[i] = h\n",
    "            grad[i] = (f(x + dx) - f(x - dx)) / (2 * h)\n",
    "        return grad\n",
    "\n",
    "    for i in range(n):\n",
    "        fx = f(x0)\n",
    "        grad_f = numerical_grad(f, x0)\n",
    "        x1 = x0 - lr * (fx - c) * grad_f\n",
    "        if np.linalg.norm(x1 - x0) < tol:\n",
    "            break\n",
    "        x0 = x1\n",
    "    return [i, x1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us solve a specific example. Let $f(x,y) = x^2 + 3y^2$, let $c=0$, let initial point $(x_0,y_0)=(2,2)$ and let us set the learning rate at 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3952, array([7.93556903e-02, 1.49809119e-05])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x[0]**2 + 3 * x[1]**2\n",
    "\n",
    "MSolve(f, c=0.0, x0=[2.0, 2.0], lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started at the point $(2, 2)$ with the learning rate at set 0.01.  The red dots show the successive steps taken by the algorithm as it descends toward the minimum at the origin $(0, 0)$. This function is **convex**. The descent follows an **elliptical path**, with rapid convergence in $y$ and slower motion in $x$. The gradient is $ \\nabla f(x, y) = (2x, 6y) $. So, the updates are:\n",
    "\n",
    "$$ x^{(n+1)} = x^{(n)} - 0.01 \\cdot 2x^{(n)},\\quad y^{(n+1)} = y^{(n)} - 0.01 \\cdot 6y^{(n)} $$\n",
    "\n",
    "<center><img width=\"500px\" src=\"../images/steepest_descent.png\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "\n",
    "Perceptrons are the main building blocks of artificial neural networks. They are designed to solve binary classification problems. They take a collection of input values $x = (x_1,\\ldots,x_n)$ apply a linear combination \n",
    "\n",
    "$$\\alpha\\cdot x + \\beta = a_1 x_1 + \\cdots + a_n x_n + \\beta$$ \n",
    "\n",
    "using a collection of weights $\\alpha = (a_0,\\ldots,a_n)$ and $\\beta$ to be determined via an iterative approach. Then we apply an activation function $\\varphi(x)$ to get an output which is either 0 or 1.\n",
    "\n",
    "<center><img src=\"../images/perceptron.gif\"></center>\n",
    "\n",
    "[Source: Multilayer perceptrons from \"Nonlinear Switching State-Space Models\" by Antti Honkela](https://users.ics.aalto.fi/ahonkela/dippa/node41.html)\n",
    "\n",
    "### The Optimization Problem\n",
    "\n",
    "As in the case of logistic regression, we have a collection of data points $(x^{(i)},y^{(i)})$ that we assume satisfy a relationship of the form\n",
    "\n",
    "$$ y^{(i)} - \\varphi(\\alpha\\cdot x^{(i)} + \\beta) \\sim N(0,\\sigma) $$\n",
    "\n",
    "where $\\varphi\\colon\\mathbb{R}\\to\\mathbb{R}$ is a real valued function of a single variable, $\\alpha$ and $x^{(i)}$ are vectors in an inner product space and $\\beta$ is a scalar.  Our task is to find the best fitting pair $(\\alpha,\\beta)$ such that \n",
    "\n",
    "$$ \\sum_i (y^{(i)} - \\varphi(\\alpha\\cdot x^{(i)} + \\beta))^2 $$\n",
    "\n",
    "is minimized. This setup is a slight generalization of the [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) algorithm we covered in earlier lectures.  In the logistic regression case $\\varphi(x) = \\frac{1}{1+e^{-x}}$.  Now, we can change $\\varphi$. Here are a couple of options:\n",
    "\n",
    "- Linear: $\\varphi(z) = z$ (gives **linear regression**),\n",
    "- Sigmoid: $\\varphi(z) = \\frac{1}{1 + e^{-z}}$ (logistic regression if combined with cross-entropy loss),\n",
    "- ReLU: $\\varphi(z) = \\max(0,z)$,\n",
    "- Tanh: $\\varphi(z) = \\tanh(z)$, etc.\n",
    "\n",
    "### Minimizing MSE is Equivalent to MLE\n",
    "\n",
    "Under the Gaussian noise assumption minimizing the MSE is equivalent to finding the maximum likelihood estimation. This is because when we write the likelihood function\n",
    "\n",
    "$$\n",
    "p(y^{(i)} \\mid x^{(i)}; \\alpha, \\beta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{1}{2\\sigma^2} (y^{(i)} - \\varphi(\\alpha \\cdot x^{(i)} + \\beta))^2 \\right)\n",
    "$$\n",
    "\n",
    "We obtain the log-likelihood over all samples as\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\alpha, \\beta) = -\\frac{N}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y^{(i)} - \\varphi(\\alpha \\cdot x^{(i)} + \\beta))^2\n",
    "$$\n",
    "\n",
    "Maximizing this is equivalent to minimizing the following function\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N (y^{(i)} - \\varphi(\\alpha \\cdot x^{(i)} + \\beta))^2\n",
    "$$\n",
    "\n",
    "which justifies the choice of the squared loss.\n",
    "\n",
    "\n",
    "### The Gradient Descent Update Rule\n",
    "\n",
    "We are going to use a gradient descent method update to minimize $L(\\alpha, \\beta)$. Here we have a couple of options: We can use\n",
    "\n",
    "1. the whole dataset (ordinary gradient descent)\n",
    "2. a small random batch of points (batch gradient descent)\n",
    "3. only a single point (stochastic gradient descent)\n",
    "\n",
    "to calculate the mean square error. Below, I'll do SGD for simplicity, but you may use any of these methods. \n",
    "\n",
    "Suppose at iteration $n$, we randomly pick the data point $(x^{(n)}, y^{(n)})$, and define:\n",
    "\n",
    "$$\n",
    "\\delta^{(n)} := \\varphi(\\alpha^{(n)} \\cdot x^{(n)} + \\beta^{(n)}) - y^{(n)}\n",
    "$$\n",
    "\n",
    "We define the **per-sample loss function** $\\ell^{(n)}$ at the sample $(x^{(n)}, y^{(n)})$ as:\n",
    "\n",
    "$$\n",
    "\\ell^{(n)}(\\alpha,\\beta) := (y^{(n)} - \\varphi(\\alpha^{(n)} \\cdot x^{(n)} + \\beta^{(n)}))^2 = (\\delta^{(n)})^2\n",
    "$$\n",
    "\n",
    "We compute the gradient of $\\ell^{(n)}$ with respect to $\\alpha$ as follows: Let\n",
    "\n",
    "$$\n",
    "z^{(n)} := \\alpha^{(n)} \\cdot x^{(n)} + \\beta^{(n)} \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\ell^{(n)} = (y^{(n)} - \\varphi(z^{(n)}))^2\n",
    "$$\n",
    "\n",
    "Applying the chain rule:\n",
    "\n",
    "$$\n",
    "\\nabla_\\alpha \\ell^{(n)} = 2(y^{(n)} - \\varphi(z^{(n)})) \\cdot (-\\varphi'(z^{(n)})) \\cdot x^{(n)} = -2 \\delta^{(n)} \\varphi'(z^{(n)}) x^{(n)}\n",
    "$$\n",
    "\n",
    "If we drop the constant $2$ into the learning rate $\\eta$, the update for $\\alpha$ and $\\beta$, we obtain:\n",
    "\n",
    "$$\n",
    "\\alpha^{(n+1)} = \\alpha^{(n)} - \\eta \\cdot \\delta^{(n)} \\varphi'(z^{(n)}) x^{(n)}\n",
    "\\qquad\n",
    "\\beta^{(n+1)} = \\beta^{(n)} - \\eta \\cdot \\delta^{(n)} \\varphi'(z^{(n)})\n",
    "$$\n",
    "\n",
    "### Feed-forward and back-propagation\n",
    "\n",
    "In the feed-forward stage of the computation, we calculate the output $\\varphi(\\alpha^{(n)}\\cdot x + \\beta^{(n)})$. In the back-propagation phase, we calculate the error $y - \\varphi(\\alpha^{(n)}\\cdot x + \\beta^{(n)})$ and adjust the weights as described above to obtain the next iteration of weights $(\\alpha^{(n+1)},\\beta^{(n+1)})$.\n",
    "\n",
    "### An example\n",
    "\n",
    "For this example, we are going to use a [toy dataset](http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)) from UCI: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar = fetch_ucirepo(id=151) \n",
    "  \n",
    "sonar_X = sonar.data.features \n",
    "sonar_y_raw = sonar.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "sonar_y = encoder.fit_transform(sonar_y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(xs, ys, f, f_prime, epochs, track=10, eta=0.1, tol=1e-4):\n",
    "    # Convert to NumPy arrays and add bias term\n",
    "    X = np.hstack([np.ones((len(xs), 1)), xs])\n",
    "    N, d = X.shape\n",
    "    w = np.random.randn(d)\n",
    "    errors = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pick a random training sample\n",
    "        j = np.random.randint(N)\n",
    "\n",
    "        # Forward pass\n",
    "        z = np.dot(w, X[j])\n",
    "        a = f(z)\n",
    "        delta = a - ys[j]\n",
    "\n",
    "        # Update rule if error exceeds tolerance\n",
    "        if abs(delta) > tol:\n",
    "            grad = delta * f_prime(z) * X[j]\n",
    "            w -= eta * grad\n",
    "\n",
    "        # Track training error every 10 steps\n",
    "        if i % track == 0:\n",
    "            z_all = X @ w\n",
    "            y_hat = f(z_all)\n",
    "            mse = np.mean((y_hat - ys) ** 2)\n",
    "            errors.append(mse)\n",
    "\n",
    "    # Final prediction\n",
    "    y_pred = f(X @ w)\n",
    "    return y_pred, errors, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Train\n",
    "y_pred, errs, weights = fit_predict(sonar_X, sonar_y.reshape(-1), sigmoid, sigmoid_prime, epochs=15000, track=100, eta=0.5, tol=5e-4)\n",
    "\n",
    "# Plot error\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(errs)\n",
    "plt.xlabel(\"Epochs (every 100 steps)\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.81      0.85       111\n",
      "           1       0.80      0.89      0.84        97\n",
      "\n",
      "    accuracy                           0.85       208\n",
      "   macro avg       0.85      0.85      0.85       208\n",
      "weighted avg       0.85      0.85      0.85       208\n",
      "\n",
      "Accuracy: 0.8461538461538461\n"
     ]
    }
   ],
   "source": [
    "def helper(x,th=0.5):\n",
    "    if x>=th:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "tmp = np.vectorize(helper)(y_pred).reshape(-1)\n",
    "\n",
    "print(classification_report(sonar_y, tmp))\n",
    "print(\"Accuracy:\", accuracy_score(sonar_y, tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "While perceptrons are limited in their ability to represent complex non-linear relationships, they can be composed into deeper architectures known as [neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network). \n",
    "\n",
    "Another way to describe a neural network is as a **directed computational graph** of perceptrons layered in stages:\n",
    "\n",
    "<center>\n",
    "<img width=\"450px\" src=\"../images/neural-net.png\">\n",
    "</center>\n",
    "\n",
    "Each layer applies a **linear transformation followed by a non-linear activation function**. If we denote by $x^i \\in \\mathbb{R}^{d_i}$ the vector of activations at layer $i$, and by $W^i \\in \\mathbb{R}^{d_{i+1} \\times d_i}$ the weight matrix between layers $i$ and $i+1$, then the feedforward rule is:\n",
    "\n",
    "$$\n",
    "x^{i+1} = f^i(W^i x^i + b^i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $f^i$ is an activation function (e.g., ReLU, sigmoid),\n",
    "- $b^i \\in \\mathbb{R}^{d_{i+1}}$ is a bias vector.\n",
    "\n",
    "This transformation is applied layer by layer, from the input $x^0$ to the final output $x^L$, where $L$ is the number of layers.\n",
    "\n",
    "Just as in the perceptron case, once an output is obtained, the error with respect to the true label is measured via a loss function (e.g., squared error or cross-entropy). Then, using a technique known as **backpropagation**, we compute gradients of the loss with respect to each weight via the **chain rule** of calculus.\n",
    "\n",
    "These gradients are used to iteratively update weights via **gradient descent**:\n",
    "\n",
    "$$\n",
    "W^i \\leftarrow W^i - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^i}\n",
    "$$\n",
    "\n",
    "This completes one iteration of the **training phase** of the network.\n",
    "\n",
    "Neural networks with just one hidden layer and sufficient width can approximate any continuous function on compact subsets of $\\mathbb{R}^n$, a result known as the **universal approximation theorem**. There are [many different types of neural network architectures](http://www.asimovinstitute.org/neural-network-zoo/): convolutional networks for images, recurrent networks for sequences, transformers for language, and more.\n",
    "\n",
    "Unlike simple perceptrons, implementing deep networks from scratch is **not recommended in practice**. \n",
    "\n",
    "<center>\n",
    "<img src=\"../images/meme.jpg\">\n",
    "</center>\n",
    "\n",
    "Instead, we use powerful open-source libraries such as:\n",
    "\n",
    "1. [TensorFlow](https://www.tensorflow.org/)\n",
    "2. [PyTorch](https://pytorch.org/)\n",
    "3. [Keras](https://keras.io/)  high-level API on top of TensorFlow\n",
    "4. [MXNet](https://mxnet.apache.org/)\n",
    "\n",
    "For intuitive experimentation with small networks, I highly recommend the interactive [TensorFlow Playground](https://playground.tensorflow.org/), where you can design your own architectures and see how they learn in real time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example \n",
    "\n",
    "All of todays examples are going to use the keras library. Let us start with the first example we used today, the sonar dataset. Let us construct a simple neural-net for binary classification, i.e. a perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 11:44:59.740546: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(60,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sonar_X,sonar_y,train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - binary_accuracy: 0.5741 - loss: 0.6896  \n",
      "Epoch 2/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.5197 - loss: 0.6991 \n",
      "Epoch 3/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5141 - loss: 0.6992 \n",
      "Epoch 4/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5225 - loss: 0.7047 \n",
      "Epoch 5/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5631 - loss: 0.6759 \n",
      "Epoch 6/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5628 - loss: 0.6827 \n",
      "Epoch 7/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5047 - loss: 0.6993 \n",
      "Epoch 8/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5693 - loss: 0.6812 \n",
      "Epoch 9/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5813 - loss: 0.6670 \n",
      "Epoch 10/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.5723 - loss: 0.6714 \n",
      "Epoch 11/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.5064 - loss: 0.6902 \n",
      "Epoch 12/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.5710 - loss: 0.6709 \n",
      "Epoch 13/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5552 - loss: 0.6701 \n",
      "Epoch 14/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.5932 - loss: 0.6659 \n",
      "Epoch 15/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5330 - loss: 0.6805 \n",
      "Epoch 16/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5543 - loss: 0.6685 \n",
      "Epoch 17/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6045 - loss: 0.6523 \n",
      "Epoch 18/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5812 - loss: 0.6720 \n",
      "Epoch 19/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5618 - loss: 0.6670 \n",
      "Epoch 20/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5791 - loss: 0.6689 \n",
      "Epoch 21/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6071 - loss: 0.6616 \n",
      "Epoch 22/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5382 - loss: 0.6716 \n",
      "Epoch 23/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5848 - loss: 0.6631 \n",
      "Epoch 24/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5746 - loss: 0.6576 \n",
      "Epoch 25/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5862 - loss: 0.6557 \n",
      "Epoch 26/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5884 - loss: 0.6573 \n",
      "Epoch 27/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5847 - loss: 0.6584 \n",
      "Epoch 28/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.5889 - loss: 0.6592 \n",
      "Epoch 29/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5759 - loss: 0.6740 \n",
      "Epoch 30/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6295 - loss: 0.6528 \n",
      "Epoch 31/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6130 - loss: 0.6559 \n",
      "Epoch 32/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6472 - loss: 0.6513 \n",
      "Epoch 33/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6383 - loss: 0.6407 \n",
      "Epoch 34/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5595 - loss: 0.6702 \n",
      "Epoch 35/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6170 - loss: 0.6533 \n",
      "Epoch 36/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6300 - loss: 0.6348 \n",
      "Epoch 37/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6229 - loss: 0.6437 \n",
      "Epoch 38/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6174 - loss: 0.6387 \n",
      "Epoch 39/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6384 - loss: 0.6335 \n",
      "Epoch 40/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5836 - loss: 0.6515 \n",
      "Epoch 41/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6342 - loss: 0.6410 \n",
      "Epoch 42/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6704 - loss: 0.6316 \n",
      "Epoch 43/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6559 - loss: 0.6283 \n",
      "Epoch 44/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6561 - loss: 0.6255 \n",
      "Epoch 45/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6352 - loss: 0.6329 \n",
      "Epoch 46/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6587 - loss: 0.6200 \n",
      "Epoch 47/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6514 - loss: 0.6342 \n",
      "Epoch 48/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6645 - loss: 0.6207 \n",
      "Epoch 49/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6271 - loss: 0.6405 \n",
      "Epoch 50/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6437 - loss: 0.6358 \n",
      "Epoch 51/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6357 - loss: 0.6292 \n",
      "Epoch 52/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6573 - loss: 0.6300 \n",
      "Epoch 53/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6300 - loss: 0.6360 \n",
      "Epoch 54/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6694 - loss: 0.6316 \n",
      "Epoch 55/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6656 - loss: 0.6313 \n",
      "Epoch 56/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7078 - loss: 0.6235 \n",
      "Epoch 57/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6376 - loss: 0.6321 \n",
      "Epoch 58/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6207 - loss: 0.6395 \n",
      "Epoch 59/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6954 - loss: 0.6121 \n",
      "Epoch 60/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6868 - loss: 0.6127 \n",
      "Epoch 61/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6786 - loss: 0.6087 \n",
      "Epoch 62/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7108 - loss: 0.6150 \n",
      "Epoch 63/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7022 - loss: 0.6127 \n",
      "Epoch 64/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7018 - loss: 0.6120 \n",
      "Epoch 65/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6852 - loss: 0.6328 \n",
      "Epoch 66/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6590 - loss: 0.6275 \n",
      "Epoch 67/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7280 - loss: 0.6117 \n",
      "Epoch 68/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6965 - loss: 0.6112 \n",
      "Epoch 69/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6444 - loss: 0.6207 \n",
      "Epoch 70/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6368 - loss: 0.6319 \n",
      "Epoch 71/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6894 - loss: 0.6007 \n",
      "Epoch 72/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6803 - loss: 0.6087 \n",
      "Epoch 73/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6789 - loss: 0.6092 \n",
      "Epoch 74/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6712 - loss: 0.6257 \n",
      "Epoch 75/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6815 - loss: 0.6167 \n",
      "Epoch 76/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6501 - loss: 0.6278 \n",
      "Epoch 77/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7070 - loss: 0.6067 \n",
      "Epoch 78/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6338 - loss: 0.6280 \n",
      "Epoch 79/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7175 - loss: 0.5978 \n",
      "Epoch 80/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7273 - loss: 0.5928 \n",
      "Epoch 81/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6703 - loss: 0.6156 \n",
      "Epoch 82/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7036 - loss: 0.6016 \n",
      "Epoch 83/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7386 - loss: 0.5968 \n",
      "Epoch 84/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7270 - loss: 0.5945 \n",
      "Epoch 85/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7268 - loss: 0.5996 \n",
      "Epoch 86/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7403 - loss: 0.6008 \n",
      "Epoch 87/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6952 - loss: 0.6019 \n",
      "Epoch 88/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7247 - loss: 0.5913 \n",
      "Epoch 89/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6962 - loss: 0.6003 \n",
      "Epoch 90/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7087 - loss: 0.5994 \n",
      "Epoch 91/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7108 - loss: 0.6018 \n",
      "Epoch 92/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7260 - loss: 0.5932 \n",
      "Epoch 93/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6860 - loss: 0.5981 \n",
      "Epoch 94/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7404 - loss: 0.5867 \n",
      "Epoch 95/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7332 - loss: 0.5986 \n",
      "Epoch 96/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7362 - loss: 0.6018 \n",
      "Epoch 97/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6588 - loss: 0.6151 \n",
      "Epoch 98/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6965 - loss: 0.5899 \n",
      "Epoch 99/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7270 - loss: 0.5820 \n",
      "Epoch 100/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7414 - loss: 0.5754 \n",
      "Epoch 101/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7432 - loss: 0.5732 \n",
      "Epoch 102/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6840 - loss: 0.6057 \n",
      "Epoch 103/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7117 - loss: 0.5978 \n",
      "Epoch 104/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6933 - loss: 0.5912 \n",
      "Epoch 105/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.7288 - loss: 0.5635\n",
      "Epoch 106/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.7282 - loss: 0.5889 \n",
      "Epoch 107/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7002 - loss: 0.6007 \n",
      "Epoch 108/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7722 - loss: 0.5740 \n",
      "Epoch 109/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7527 - loss: 0.5804 \n",
      "Epoch 110/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7637 - loss: 0.5681 \n",
      "Epoch 111/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7541 - loss: 0.5808 \n",
      "Epoch 112/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7533 - loss: 0.5831 \n",
      "Epoch 113/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7062 - loss: 0.6053 \n",
      "Epoch 114/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7466 - loss: 0.5853 \n",
      "Epoch 115/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7559 - loss: 0.5849 \n",
      "Epoch 116/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7240 - loss: 0.5839 \n",
      "Epoch 117/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7282 - loss: 0.5730 \n",
      "Epoch 118/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7452 - loss: 0.5842 \n",
      "Epoch 119/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7488 - loss: 0.5839 \n",
      "Epoch 120/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7440 - loss: 0.5701 \n",
      "Epoch 121/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7484 - loss: 0.5750 \n",
      "Epoch 122/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7463 - loss: 0.5857 \n",
      "Epoch 123/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7863 - loss: 0.5511 \n",
      "Epoch 124/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7712 - loss: 0.5673 \n",
      "Epoch 125/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7550 - loss: 0.5717 \n",
      "Epoch 126/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7527 - loss: 0.5670 \n",
      "Epoch 127/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7762 - loss: 0.5723 \n",
      "Epoch 128/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7478 - loss: 0.5755 \n",
      "Epoch 129/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7422 - loss: 0.5771 \n",
      "Epoch 130/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7913 - loss: 0.5585 \n",
      "Epoch 131/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7760 - loss: 0.5483 \n",
      "Epoch 132/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7625 - loss: 0.5648 \n",
      "Epoch 133/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7368 - loss: 0.5667 \n",
      "Epoch 134/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7717 - loss: 0.5606 \n",
      "Epoch 135/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7859 - loss: 0.5497 \n",
      "Epoch 136/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7830 - loss: 0.5578 \n",
      "Epoch 137/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7098 - loss: 0.5758 \n",
      "Epoch 138/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7338 - loss: 0.5687 \n",
      "Epoch 139/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.7470 - loss: 0.5684 \n",
      "Epoch 140/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7238 - loss: 0.5738 \n",
      "Epoch 141/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7486 - loss: 0.5637 \n",
      "Epoch 142/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7496 - loss: 0.5837 \n",
      "Epoch 143/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7248 - loss: 0.5706 \n",
      "Epoch 144/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7166 - loss: 0.5701 \n",
      "Epoch 145/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7508 - loss: 0.5667 \n",
      "Epoch 146/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7272 - loss: 0.5920 \n",
      "Epoch 147/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7663 - loss: 0.5585 \n",
      "Epoch 148/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7383 - loss: 0.5656 \n",
      "Epoch 149/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7416 - loss: 0.5512 \n",
      "Epoch 150/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7413 - loss: 0.5754 \n",
      "Epoch 151/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7809 - loss: 0.5526 \n",
      "Epoch 152/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7420 - loss: 0.5718 \n",
      "Epoch 153/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7859 - loss: 0.5560 \n",
      "Epoch 154/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7387 - loss: 0.5679  \n",
      "Epoch 155/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7713 - loss: 0.5549 \n",
      "Epoch 156/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7870 - loss: 0.5422 \n",
      "Epoch 157/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7247 - loss: 0.5617 \n",
      "Epoch 158/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7538 - loss: 0.5420 \n",
      "Epoch 159/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7802 - loss: 0.5407 \n",
      "Epoch 160/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7568 - loss: 0.5622 \n",
      "Epoch 161/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7570 - loss: 0.5531 \n",
      "Epoch 162/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7416 - loss: 0.5562 \n",
      "Epoch 163/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7763 - loss: 0.5498 \n",
      "Epoch 164/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7576 - loss: 0.5505 \n",
      "Epoch 165/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7212 - loss: 0.5789 \n",
      "Epoch 166/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7263 - loss: 0.5483 \n",
      "Epoch 167/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7303 - loss: 0.5717 \n",
      "Epoch 168/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.7575 - loss: 0.5456 \n",
      "Epoch 169/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7559 - loss: 0.5581 \n",
      "Epoch 170/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7783 - loss: 0.5437 \n",
      "Epoch 171/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7709 - loss: 0.5309 \n",
      "Epoch 172/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7284 - loss: 0.5328 \n",
      "Epoch 173/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7194 - loss: 0.5594 \n",
      "Epoch 174/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7353 - loss: 0.5456 \n",
      "Epoch 175/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7475 - loss: 0.5475 \n",
      "Epoch 176/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7602 - loss: 0.5321 \n",
      "Epoch 177/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7346 - loss: 0.5364 \n",
      "Epoch 178/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7213 - loss: 0.5619 \n",
      "Epoch 179/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6984 - loss: 0.5611 \n",
      "Epoch 180/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7816 - loss: 0.5404 \n",
      "Epoch 181/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7177 - loss: 0.5717 \n",
      "Epoch 182/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7469 - loss: 0.5510 \n",
      "Epoch 183/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7784 - loss: 0.5440 \n",
      "Epoch 184/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7717 - loss: 0.5443 \n",
      "Epoch 185/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7472 - loss: 0.5371 \n",
      "Epoch 186/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7968 - loss: 0.5198 \n",
      "Epoch 187/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7881 - loss: 0.5340 \n",
      "Epoch 188/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7731 - loss: 0.5308 \n",
      "Epoch 189/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7720 - loss: 0.5539 \n",
      "Epoch 190/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8108 - loss: 0.5350 \n",
      "Epoch 191/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7698 - loss: 0.5385 \n",
      "Epoch 192/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7686 - loss: 0.5317 \n",
      "Epoch 193/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7841 - loss: 0.5294 \n",
      "Epoch 194/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8068 - loss: 0.5263 \n",
      "Epoch 195/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7841 - loss: 0.5433 \n",
      "Epoch 196/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7589 - loss: 0.5384 \n",
      "Epoch 197/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7580 - loss: 0.5444 \n",
      "Epoch 198/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7356 - loss: 0.5458 \n",
      "Epoch 199/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7378 - loss: 0.5334 \n",
      "Epoch 200/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6999 - loss: 0.5616 \n",
      "Epoch 201/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7730 - loss: 0.5406 \n",
      "Epoch 202/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7686 - loss: 0.5293 \n",
      "Epoch 203/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7541 - loss: 0.5459 \n",
      "Epoch 204/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8027 - loss: 0.5117 \n",
      "Epoch 205/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8105 - loss: 0.5053 \n",
      "Epoch 206/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7791 - loss: 0.5285 \n",
      "Epoch 207/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7834 - loss: 0.5314 \n",
      "Epoch 208/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7988 - loss: 0.5342 \n",
      "Epoch 209/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7729 - loss: 0.5463 \n",
      "Epoch 210/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7495 - loss: 0.5508 \n",
      "Epoch 211/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7603 - loss: 0.5576 \n",
      "Epoch 212/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7536 - loss: 0.5631 \n",
      "Epoch 213/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7827 - loss: 0.5079 \n",
      "Epoch 214/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7612 - loss: 0.5394 \n",
      "Epoch 215/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7501 - loss: 0.5155 \n",
      "Epoch 216/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7995 - loss: 0.5239 \n",
      "Epoch 217/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7758 - loss: 0.5139 \n",
      "Epoch 218/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7753 - loss: 0.5272 \n",
      "Epoch 219/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7482 - loss: 0.5386 \n",
      "Epoch 220/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8294 - loss: 0.5183 \n",
      "Epoch 221/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8098 - loss: 0.5227 \n",
      "Epoch 222/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7492 - loss: 0.5431 \n",
      "Epoch 223/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8043 - loss: 0.5159 \n",
      "Epoch 224/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7498 - loss: 0.5290 \n",
      "Epoch 225/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7680 - loss: 0.5069 \n",
      "Epoch 226/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7362 - loss: 0.5397 \n",
      "Epoch 227/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7851 - loss: 0.5107 \n",
      "Epoch 228/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7501 - loss: 0.5433 \n",
      "Epoch 229/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7776 - loss: 0.5172 \n",
      "Epoch 230/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7297 - loss: 0.5390 \n",
      "Epoch 231/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7542 - loss: 0.5280 \n",
      "Epoch 232/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7857 - loss: 0.5126 \n",
      "Epoch 233/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8306 - loss: 0.5069 \n",
      "Epoch 234/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8060 - loss: 0.4971 \n",
      "Epoch 235/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8076 - loss: 0.5138 \n",
      "Epoch 236/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7283 - loss: 0.5591 \n",
      "Epoch 237/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8020 - loss: 0.5148 \n",
      "Epoch 238/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8007 - loss: 0.5257 \n",
      "Epoch 239/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8175 - loss: 0.5014 \n",
      "Epoch 240/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8149 - loss: 0.4920 \n",
      "Epoch 241/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8057 - loss: 0.5157 \n",
      "Epoch 242/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7797 - loss: 0.5251 \n",
      "Epoch 243/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7871 - loss: 0.5329 \n",
      "Epoch 244/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7678 - loss: 0.5181 \n",
      "Epoch 245/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8091 - loss: 0.4895 \n",
      "Epoch 246/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7932 - loss: 0.5244 \n",
      "Epoch 247/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7925 - loss: 0.5196 \n",
      "Epoch 248/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7914 - loss: 0.5006 \n",
      "Epoch 249/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7845 - loss: 0.5101 \n",
      "Epoch 250/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7861 - loss: 0.5063 \n",
      "Epoch 251/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8156 - loss: 0.4965 \n",
      "Epoch 252/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7918 - loss: 0.5233 \n",
      "Epoch 253/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7705 - loss: 0.5332 \n",
      "Epoch 254/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7355 - loss: 0.5238 \n",
      "Epoch 255/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7741 - loss: 0.5127 \n",
      "Epoch 256/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8110 - loss: 0.4882 \n",
      "Epoch 257/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7445 - loss: 0.5039 \n",
      "Epoch 258/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7792 - loss: 0.5012 \n",
      "Epoch 259/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7783 - loss: 0.5211 \n",
      "Epoch 260/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7700 - loss: 0.4990 \n",
      "Epoch 261/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7694 - loss: 0.5180 \n",
      "Epoch 262/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7354 - loss: 0.5403 \n",
      "Epoch 263/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7903 - loss: 0.5027 \n",
      "Epoch 264/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8008 - loss: 0.5078 \n",
      "Epoch 265/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7622 - loss: 0.5154 \n",
      "Epoch 266/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7554 - loss: 0.5139 \n",
      "Epoch 267/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7894 - loss: 0.4978 \n",
      "Epoch 268/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8062 - loss: 0.5073 \n",
      "Epoch 269/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7905 - loss: 0.5061 \n",
      "Epoch 270/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7934 - loss: 0.4965 \n",
      "Epoch 271/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7842 - loss: 0.5084 \n",
      "Epoch 272/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8006 - loss: 0.4988 \n",
      "Epoch 273/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7928 - loss: 0.5116 \n",
      "Epoch 274/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7883 - loss: 0.5033 \n",
      "Epoch 275/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7840 - loss: 0.5111 \n",
      "Epoch 276/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7270 - loss: 0.5401 \n",
      "Epoch 277/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8167 - loss: 0.4993 \n",
      "Epoch 278/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7533 - loss: 0.5208 \n",
      "Epoch 279/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8327 - loss: 0.4840 \n",
      "Epoch 280/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7835 - loss: 0.5039 \n",
      "Epoch 281/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7470 - loss: 0.5237 \n",
      "Epoch 282/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7570 - loss: 0.5216 \n",
      "Epoch 283/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7577 - loss: 0.5200 \n",
      "Epoch 284/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7742 - loss: 0.5267 \n",
      "Epoch 285/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7797 - loss: 0.5387 \n",
      "Epoch 286/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7798 - loss: 0.5066 \n",
      "Epoch 287/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7276 - loss: 0.5279 \n",
      "Epoch 288/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7873 - loss: 0.5096 \n",
      "Epoch 289/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7437 - loss: 0.5376 \n",
      "Epoch 290/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7827 - loss: 0.5161 \n",
      "Epoch 291/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7959 - loss: 0.5157 \n",
      "Epoch 292/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8257 - loss: 0.4864 \n",
      "Epoch 293/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7729 - loss: 0.5142 \n",
      "Epoch 294/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7528 - loss: 0.5212 \n",
      "Epoch 295/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7567 - loss: 0.5044 \n",
      "Epoch 296/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7710 - loss: 0.5094 \n",
      "Epoch 297/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8340 - loss: 0.4854 \n",
      "Epoch 298/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7763 - loss: 0.5042 \n",
      "Epoch 299/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8145 - loss: 0.4885 \n",
      "Epoch 300/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7932 - loss: 0.5100 \n",
      "Epoch 301/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8041 - loss: 0.5061 \n",
      "Epoch 302/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.7530 - loss: 0.5155 \n",
      "Epoch 303/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7581 - loss: 0.5177 \n",
      "Epoch 304/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7803 - loss: 0.5164 \n",
      "Epoch 305/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7908 - loss: 0.5067 \n",
      "Epoch 306/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8040 - loss: 0.4766 \n",
      "Epoch 307/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7959 - loss: 0.4874 \n",
      "Epoch 308/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8277 - loss: 0.4774 \n",
      "Epoch 309/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7562 - loss: 0.5139 \n",
      "Epoch 310/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7722 - loss: 0.5031 \n",
      "Epoch 311/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7733 - loss: 0.4937 \n",
      "Epoch 312/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7613 - loss: 0.5034 \n",
      "Epoch 313/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7937 - loss: 0.4991 \n",
      "Epoch 314/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8041 - loss: 0.4670 \n",
      "Epoch 315/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7942 - loss: 0.4959 \n",
      "Epoch 316/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8131 - loss: 0.4781 \n",
      "Epoch 317/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7542 - loss: 0.4963 \n",
      "Epoch 318/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7573 - loss: 0.5134 \n",
      "Epoch 319/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7770 - loss: 0.5028 \n",
      "Epoch 320/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7916 - loss: 0.5022 \n",
      "Epoch 321/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7972 - loss: 0.5215 \n",
      "Epoch 322/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8191 - loss: 0.4762 \n",
      "Epoch 323/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7463 - loss: 0.5223 \n",
      "Epoch 324/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8134 - loss: 0.4680 \n",
      "Epoch 325/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8132 - loss: 0.4663 \n",
      "Epoch 326/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7470 - loss: 0.5067 \n",
      "Epoch 327/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7883 - loss: 0.4973 \n",
      "Epoch 328/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7991 - loss: 0.5068 \n",
      "Epoch 329/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7964 - loss: 0.4898 \n",
      "Epoch 330/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8377 - loss: 0.4684 \n",
      "Epoch 331/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7715 - loss: 0.4980 \n",
      "Epoch 332/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7806 - loss: 0.4992 \n",
      "Epoch 333/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8321 - loss: 0.4895 \n",
      "Epoch 334/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8053 - loss: 0.4940 \n",
      "Epoch 335/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7726 - loss: 0.5162 \n",
      "Epoch 336/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7967 - loss: 0.4660 \n",
      "Epoch 337/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7911 - loss: 0.5009 \n",
      "Epoch 338/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7794 - loss: 0.4853 \n",
      "Epoch 339/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7927 - loss: 0.4874 \n",
      "Epoch 340/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7445 - loss: 0.5175 \n",
      "Epoch 341/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7271 - loss: 0.5165 \n",
      "Epoch 342/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8042 - loss: 0.4809 \n",
      "Epoch 343/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7734 - loss: 0.5025 \n",
      "Epoch 344/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8201 - loss: 0.4762 \n",
      "Epoch 345/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7547 - loss: 0.5037 \n",
      "Epoch 346/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7770 - loss: 0.5037 \n",
      "Epoch 347/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7708 - loss: 0.5031 \n",
      "Epoch 348/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7582 - loss: 0.5227 \n",
      "Epoch 349/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7803 - loss: 0.4828 \n",
      "Epoch 350/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8099 - loss: 0.4608 \n",
      "Epoch 351/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8189 - loss: 0.4797 \n",
      "Epoch 352/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7621 - loss: 0.4946 \n",
      "Epoch 353/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7266 - loss: 0.5260 \n",
      "Epoch 354/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7910 - loss: 0.4757 \n",
      "Epoch 355/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7272 - loss: 0.5115 \n",
      "Epoch 356/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8051 - loss: 0.4591 \n",
      "Epoch 357/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7764 - loss: 0.4894 \n",
      "Epoch 358/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7909 - loss: 0.4733 \n",
      "Epoch 359/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8112 - loss: 0.4820 \n",
      "Epoch 360/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7742 - loss: 0.4833  \n",
      "Epoch 361/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8088 - loss: 0.4719 \n",
      "Epoch 362/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7694 - loss: 0.4902 \n",
      "Epoch 363/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7710 - loss: 0.5024 \n",
      "Epoch 364/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7882 - loss: 0.4950 \n",
      "Epoch 365/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8287 - loss: 0.4815 \n",
      "Epoch 366/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8009 - loss: 0.4786 \n",
      "Epoch 367/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7762 - loss: 0.4813 \n",
      "Epoch 368/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7960 - loss: 0.4935 \n",
      "Epoch 369/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7403 - loss: 0.5292 \n",
      "Epoch 370/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7597 - loss: 0.4948 \n",
      "Epoch 371/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7860 - loss: 0.4919 \n",
      "Epoch 372/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7926 - loss: 0.5053 \n",
      "Epoch 373/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7948 - loss: 0.4875 \n",
      "Epoch 374/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7938 - loss: 0.4907 \n",
      "Epoch 375/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7868 - loss: 0.4847 \n",
      "Epoch 376/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7547 - loss: 0.5158 \n",
      "Epoch 377/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7986 - loss: 0.4658 \n",
      "Epoch 378/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7492 - loss: 0.5180 \n",
      "Epoch 379/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7437 - loss: 0.5088 \n",
      "Epoch 380/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7857 - loss: 0.4709 \n",
      "Epoch 381/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7924 - loss: 0.4969 \n",
      "Epoch 382/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7912 - loss: 0.4833 \n",
      "Epoch 383/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7881 - loss: 0.4961 \n",
      "Epoch 384/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8076 - loss: 0.4728 \n",
      "Epoch 385/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7663 - loss: 0.5143 \n",
      "Epoch 386/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8017 - loss: 0.4971 \n",
      "Epoch 387/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7951 - loss: 0.4848 \n",
      "Epoch 388/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7726 - loss: 0.5016 \n",
      "Epoch 389/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8116 - loss: 0.4720 \n",
      "Epoch 390/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7571 - loss: 0.4997 \n",
      "Epoch 391/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7665 - loss: 0.4866 \n",
      "Epoch 392/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7958 - loss: 0.4920 \n",
      "Epoch 393/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8024 - loss: 0.4638 \n",
      "Epoch 394/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7885 - loss: 0.4848 \n",
      "Epoch 395/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7776 - loss: 0.4898 \n",
      "Epoch 396/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8158 - loss: 0.4788 \n",
      "Epoch 397/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7943 - loss: 0.4632 \n",
      "Epoch 398/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7910 - loss: 0.4801 \n",
      "Epoch 399/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7671 - loss: 0.4811 \n",
      "Epoch 400/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7454 - loss: 0.5038 \n",
      "Epoch 401/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7597 - loss: 0.4984 \n",
      "Epoch 402/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7702 - loss: 0.4950 \n",
      "Epoch 403/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7913 - loss: 0.4920 \n",
      "Epoch 404/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8137 - loss: 0.4825 \n",
      "Epoch 405/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7549 - loss: 0.4886 \n",
      "Epoch 406/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7373 - loss: 0.4782 \n",
      "Epoch 407/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8199 - loss: 0.4382 \n",
      "Epoch 408/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8133 - loss: 0.4629 \n",
      "Epoch 409/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7757 - loss: 0.4860 \n",
      "Epoch 410/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7901 - loss: 0.4790 \n",
      "Epoch 411/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7929 - loss: 0.4794 \n",
      "Epoch 412/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7770 - loss: 0.4769 \n",
      "Epoch 413/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7786 - loss: 0.4595 \n",
      "Epoch 414/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7697 - loss: 0.4621 \n",
      "Epoch 415/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7953 - loss: 0.4726 \n",
      "Epoch 416/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7618 - loss: 0.4713 \n",
      "Epoch 417/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7911 - loss: 0.4855 \n",
      "Epoch 418/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8009 - loss: 0.4919 \n",
      "Epoch 419/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8017 - loss: 0.4772 \n",
      "Epoch 420/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8112 - loss: 0.4656 \n",
      "Epoch 421/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7956 - loss: 0.4815 \n",
      "Epoch 422/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7886 - loss: 0.4815 \n",
      "Epoch 423/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7711 - loss: 0.4804 \n",
      "Epoch 424/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7611 - loss: 0.4715 \n",
      "Epoch 425/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7342 - loss: 0.4930 \n",
      "Epoch 426/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7845 - loss: 0.4577 \n",
      "Epoch 427/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7963 - loss: 0.4737 \n",
      "Epoch 428/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7869 - loss: 0.4622 \n",
      "Epoch 429/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7597 - loss: 0.4999 \n",
      "Epoch 430/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7575 - loss: 0.4874 \n",
      "Epoch 431/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7690 - loss: 0.4661 \n",
      "Epoch 432/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8012 - loss: 0.4395 \n",
      "Epoch 433/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7886 - loss: 0.4765 \n",
      "Epoch 434/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7883 - loss: 0.4836 \n",
      "Epoch 435/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7765 - loss: 0.4538 \n",
      "Epoch 436/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7991 - loss: 0.4698 \n",
      "Epoch 437/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8132 - loss: 0.4463 \n",
      "Epoch 438/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7696 - loss: 0.4901 \n",
      "Epoch 439/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7261 - loss: 0.5008 \n",
      "Epoch 440/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7561 - loss: 0.4772 \n",
      "Epoch 441/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7655 - loss: 0.4765 \n",
      "Epoch 442/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8027 - loss: 0.4618 \n",
      "Epoch 443/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7758 - loss: 0.4904 \n",
      "Epoch 444/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7608 - loss: 0.4801 \n",
      "Epoch 445/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7494 - loss: 0.4872 \n",
      "Epoch 446/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7912 - loss: 0.4892 \n",
      "Epoch 447/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7798 - loss: 0.4693 \n",
      "Epoch 448/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7693 - loss: 0.4885 \n",
      "Epoch 449/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7853 - loss: 0.4679 \n",
      "Epoch 450/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7717 - loss: 0.5035 \n",
      "Epoch 451/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7626 - loss: 0.4969 \n",
      "Epoch 452/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7933 - loss: 0.4716 \n",
      "Epoch 453/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7175 - loss: 0.5057 \n",
      "Epoch 454/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7310 - loss: 0.4972 \n",
      "Epoch 455/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7948 - loss: 0.4486 \n",
      "Epoch 456/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7668 - loss: 0.4843 \n",
      "Epoch 457/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8220 - loss: 0.4581 \n",
      "Epoch 458/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7674 - loss: 0.4567 \n",
      "Epoch 459/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7471 - loss: 0.4886 \n",
      "Epoch 460/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7984 - loss: 0.4416 \n",
      "Epoch 461/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7599 - loss: 0.4702 \n",
      "Epoch 462/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7975 - loss: 0.4468 \n",
      "Epoch 463/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7713 - loss: 0.4755 \n",
      "Epoch 464/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7836 - loss: 0.4622 \n",
      "Epoch 465/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7713 - loss: 0.4807 \n",
      "Epoch 466/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8022 - loss: 0.4626 \n",
      "Epoch 467/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7696 - loss: 0.4896 \n",
      "Epoch 468/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7565 - loss: 0.4823 \n",
      "Epoch 469/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8047 - loss: 0.4715 \n",
      "Epoch 470/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7780 - loss: 0.4752 \n",
      "Epoch 471/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7712 - loss: 0.4789 \n",
      "Epoch 472/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7456 - loss: 0.4912 \n",
      "Epoch 473/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8071 - loss: 0.4518 \n",
      "Epoch 474/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7722 - loss: 0.4675 \n",
      "Epoch 475/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7640 - loss: 0.4762 \n",
      "Epoch 476/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7556 - loss: 0.4919 \n",
      "Epoch 477/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7787 - loss: 0.4785 \n",
      "Epoch 478/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7896 - loss: 0.4673 \n",
      "Epoch 479/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7622 - loss: 0.4808 \n",
      "Epoch 480/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7777 - loss: 0.4533 \n",
      "Epoch 481/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.8239 - loss: 0.4575 \n",
      "Epoch 482/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7439 - loss: 0.4935 \n",
      "Epoch 483/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7740 - loss: 0.4738 \n",
      "Epoch 484/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7753 - loss: 0.4598 \n",
      "Epoch 485/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7628 - loss: 0.4706 \n",
      "Epoch 486/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7857 - loss: 0.4675 \n",
      "Epoch 487/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7818 - loss: 0.4693 \n",
      "Epoch 488/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8126 - loss: 0.4682 \n",
      "Epoch 489/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7570 - loss: 0.4873 \n",
      "Epoch 490/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7592 - loss: 0.4757 \n",
      "Epoch 491/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7785 - loss: 0.4777 \n",
      "Epoch 492/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7652 - loss: 0.4788 \n",
      "Epoch 493/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7680 - loss: 0.4693 \n",
      "Epoch 494/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7925 - loss: 0.4579 \n",
      "Epoch 495/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7759 - loss: 0.4593 \n",
      "Epoch 496/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7740 - loss: 0.4686 \n",
      "Epoch 497/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7864 - loss: 0.4611 \n",
      "Epoch 498/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7800 - loss: 0.4736 \n",
      "Epoch 499/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7777 - loss: 0.4786 \n",
      "Epoch 500/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7699 - loss: 0.4834 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7464c5b82120>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=500, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75        31\n",
      "           1       1.00      0.34      0.51        32\n",
      "\n",
      "    accuracy                           0.67        63\n",
      "   macro avg       0.80      0.67      0.63        63\n",
      "weighted avg       0.80      0.67      0.63        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "tmp = np.vectorize(lambda x: helper(x,th=0.75))(y_predict.reshape(-1))\n",
    "print(classification_report(y_test,tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another standard small example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "x_iris = iris['data']\n",
    "y_iris = iris['target']\n",
    "\n",
    "labeler = LabelBinarizer()\n",
    "y = labeler.fit_transform(y_iris)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(4,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_iris, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - binary_accuracy: 0.6164 - loss: 0.6528\n",
      "Epoch 2/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6667 - loss: 0.5885 \n",
      "Epoch 3/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6667 - loss: 0.5568 \n",
      "Epoch 4/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6779 - loss: 0.5193 \n",
      "Epoch 5/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7577 - loss: 0.4923 \n",
      "Epoch 6/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.8667 - loss: 0.4693 \n",
      "Epoch 7/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8983 - loss: 0.4337 \n",
      "Epoch 8/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.8777 - loss: 0.4159 \n",
      "Epoch 9/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - binary_accuracy: 0.8895 - loss: 0.3974\n",
      "Epoch 10/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - binary_accuracy: 0.8975 - loss: 0.3601\n",
      "Epoch 11/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8858 - loss: 0.3556 \n",
      "Epoch 12/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8912 - loss: 0.3282 \n",
      "Epoch 13/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8581 - loss: 0.3447 \n",
      "Epoch 14/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8723 - loss: 0.3165 \n",
      "Epoch 15/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8804 - loss: 0.3036 \n",
      "Epoch 16/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8608 - loss: 0.3094 \n",
      "Epoch 17/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8834 - loss: 0.2774 \n",
      "Epoch 18/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8776 - loss: 0.2804 \n",
      "Epoch 19/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8726 - loss: 0.2715 \n",
      "Epoch 20/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.8808 - loss: 0.2512 \n",
      "Epoch 21/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9035 - loss: 0.2435 \n",
      "Epoch 22/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9091 - loss: 0.2428 \n",
      "Epoch 23/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9531 - loss: 0.2488 \n",
      "Epoch 24/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9539 - loss: 0.2361 \n",
      "Epoch 25/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9538 - loss: 0.2129 \n",
      "Epoch 26/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9664 - loss: 0.2038 \n",
      "Epoch 27/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9827 - loss: 0.2098 \n",
      "Epoch 28/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9891 - loss: 0.2064 \n",
      "Epoch 29/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9751 - loss: 0.2060 \n",
      "Epoch 30/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9598 - loss: 0.2025 \n",
      "Epoch 31/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9802 - loss: 0.1850 \n",
      "Epoch 32/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9926 - loss: 0.1739 \n",
      "Epoch 33/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.9870 - loss: 0.1717 \n",
      "Epoch 34/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9825 - loss: 0.1815 \n",
      "Epoch 35/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9839 - loss: 0.1747 \n",
      "Epoch 36/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9880 - loss: 0.1550 \n",
      "Epoch 37/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9899 - loss: 0.1562 \n",
      "Epoch 38/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9885 - loss: 0.1426 \n",
      "Epoch 39/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9841 - loss: 0.1381 \n",
      "Epoch 40/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9923 - loss: 0.1370 \n",
      "Epoch 41/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9908 - loss: 0.1416 \n",
      "Epoch 42/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.1347 \n",
      "Epoch 43/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9909 - loss: 0.1317 \n",
      "Epoch 44/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.1246 \n",
      "Epoch 45/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9837 - loss: 0.1230 \n",
      "Epoch 46/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9947 - loss: 0.1151 \n",
      "Epoch 47/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9864 - loss: 0.1151 \n",
      "Epoch 48/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.1042 \n",
      "Epoch 49/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9929 - loss: 0.1153 \n",
      "Epoch 50/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9836 - loss: 0.1117 \n",
      "Epoch 51/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9956 - loss: 0.0974 \n",
      "Epoch 52/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.1033 \n",
      "Epoch 53/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9803 - loss: 0.0978 \n",
      "Epoch 54/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9892 - loss: 0.0882 \n",
      "Epoch 55/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9891 - loss: 0.0873 \n",
      "Epoch 56/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9909 - loss: 0.0914 \n",
      "Epoch 57/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9905 - loss: 0.0915 \n",
      "Epoch 58/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9847 - loss: 0.1001 \n",
      "Epoch 59/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9929 - loss: 0.0711 \n",
      "Epoch 60/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9870 - loss: 0.0795 \n",
      "Epoch 61/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9779 - loss: 0.0883 \n",
      "Epoch 62/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9883 - loss: 0.0840 \n",
      "Epoch 63/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9781 - loss: 0.0857 \n",
      "Epoch 64/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9864 - loss: 0.0751 \n",
      "Epoch 65/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9923 - loss: 0.0706 \n",
      "Epoch 66/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9937 - loss: 0.0732 \n",
      "Epoch 67/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9844 - loss: 0.0771 \n",
      "Epoch 68/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9909 - loss: 0.0598 \n",
      "Epoch 69/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9866 - loss: 0.0706 \n",
      "Epoch 70/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9831 - loss: 0.0635 \n",
      "Epoch 71/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9793 - loss: 0.0706 \n",
      "Epoch 72/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9929 - loss: 0.0684 \n",
      "Epoch 73/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9895 - loss: 0.0553 \n",
      "Epoch 74/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9868 - loss: 0.0659 \n",
      "Epoch 75/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9966 - loss: 0.0553 \n",
      "Epoch 76/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9910 - loss: 0.0584 \n",
      "Epoch 77/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9809 - loss: 0.0669 \n",
      "Epoch 78/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9797 - loss: 0.0749 \n",
      "Epoch 79/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9907 - loss: 0.0611 \n",
      "Epoch 80/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0548 \n",
      "Epoch 81/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9945 - loss: 0.0485 \n",
      "Epoch 82/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9847 - loss: 0.0742 \n",
      "Epoch 83/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9914 - loss: 0.0490 \n",
      "Epoch 84/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9796 - loss: 0.0695 \n",
      "Epoch 85/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9906 - loss: 0.0550 \n",
      "Epoch 86/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9803 - loss: 0.0666 \n",
      "Epoch 87/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9796 - loss: 0.0759 \n",
      "Epoch 88/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9896 - loss: 0.0507 \n",
      "Epoch 89/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9740 - loss: 0.0667 \n",
      "Epoch 90/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9851 - loss: 0.0488 \n",
      "Epoch 91/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0447 \n",
      "Epoch 92/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9758 - loss: 0.0707 \n",
      "Epoch 93/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9966 - loss: 0.0402 \n",
      "Epoch 94/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9731 - loss: 0.0609 \n",
      "Epoch 95/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9929 - loss: 0.0447 \n",
      "Epoch 96/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.0483 \n",
      "Epoch 97/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9935 - loss: 0.0443 \n",
      "Epoch 98/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9746 - loss: 0.0478 \n",
      "Epoch 99/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9929 - loss: 0.0401 \n",
      "Epoch 100/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0577 \n",
      "Epoch 101/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9805 - loss: 0.0551 \n",
      "Epoch 102/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9805 - loss: 0.0472 \n",
      "Epoch 103/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0394 \n",
      "Epoch 104/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9878 - loss: 0.0521 \n",
      "Epoch 105/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9919 - loss: 0.0474 \n",
      "Epoch 106/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9866 - loss: 0.0648 \n",
      "Epoch 107/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9882 - loss: 0.0445 \n",
      "Epoch 108/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0492 \n",
      "Epoch 109/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0494 \n",
      "Epoch 110/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9733 - loss: 0.0642 \n",
      "Epoch 111/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9841 - loss: 0.0467 \n",
      "Epoch 112/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9945 - loss: 0.0345 \n",
      "Epoch 113/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9896 - loss: 0.0376 \n",
      "Epoch 114/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9929 - loss: 0.0391 \n",
      "Epoch 115/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0379 \n",
      "Epoch 116/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9836 - loss: 0.0583 \n",
      "Epoch 117/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.0416 \n",
      "Epoch 118/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9844 - loss: 0.0520 \n",
      "Epoch 119/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9782 - loss: 0.0460 \n",
      "Epoch 120/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0452 \n",
      "Epoch 121/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9820 - loss: 0.0571 \n",
      "Epoch 122/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0387 \n",
      "Epoch 123/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9844 - loss: 0.0473 \n",
      "Epoch 124/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9935 - loss: 0.0352 \n",
      "Epoch 125/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9793 - loss: 0.0432 \n",
      "Epoch 126/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9935 - loss: 0.0390 \n",
      "Epoch 127/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9956 - loss: 0.0324 \n",
      "Epoch 128/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9920 - loss: 0.0451 \n",
      "Epoch 129/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9842 - loss: 0.0389 \n",
      "Epoch 130/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9758 - loss: 0.0624 \n",
      "Epoch 131/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9836 - loss: 0.0586 \n",
      "Epoch 132/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9844 - loss: 0.0525 \n",
      "Epoch 133/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9782 - loss: 0.0456 \n",
      "Epoch 134/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9820 - loss: 0.0511 \n",
      "Epoch 135/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9923 - loss: 0.0396 \n",
      "Epoch 136/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9782 - loss: 0.0576 \n",
      "Epoch 137/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9749 - loss: 0.0679 \n",
      "Epoch 138/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9929 - loss: 0.0392 \n",
      "Epoch 139/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9770 - loss: 0.0388 \n",
      "Epoch 140/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9905 - loss: 0.0461 \n",
      "Epoch 141/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9956 - loss: 0.0291 \n",
      "Epoch 142/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9899 - loss: 0.0392 \n",
      "Epoch 143/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9893 - loss: 0.0336 \n",
      "Epoch 144/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9947 - loss: 0.0286 \n",
      "Epoch 145/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9947 - loss: 0.0278 \n",
      "Epoch 146/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9796 - loss: 0.0568 \n",
      "Epoch 147/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9883 - loss: 0.0396 \n",
      "Epoch 148/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9749 - loss: 0.0591 \n",
      "Epoch 149/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9919 - loss: 0.0390 \n",
      "Epoch 150/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9831 - loss: 0.0482 \n",
      "Epoch 151/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9923 - loss: 0.0305 \n",
      "Epoch 152/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9947 - loss: 0.0333 \n",
      "Epoch 153/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.0280 \n",
      "Epoch 154/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9935 - loss: 0.0299 \n",
      "Epoch 155/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9890 - loss: 0.0341 \n",
      "Epoch 156/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9796 - loss: 0.0504 \n",
      "Epoch 157/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9908 - loss: 0.0302 \n",
      "Epoch 158/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9905 - loss: 0.0310 \n",
      "Epoch 159/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9749 - loss: 0.0561 \n",
      "Epoch 160/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9856 - loss: 0.0374 \n",
      "Epoch 161/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9779 - loss: 0.0411 \n",
      "Epoch 162/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0585 \n",
      "Epoch 163/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9771 - loss: 0.0521 \n",
      "Epoch 164/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9769 - loss: 0.0540 \n",
      "Epoch 165/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9923 - loss: 0.0379 \n",
      "Epoch 166/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9852 - loss: 0.0397 \n",
      "Epoch 167/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9945 - loss: 0.0295 \n",
      "Epoch 168/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9919 - loss: 0.0329 \n",
      "Epoch 169/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9794 - loss: 0.0405 \n",
      "Epoch 170/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9656 - loss: 0.0549 \n",
      "Epoch 171/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9923 - loss: 0.0316 \n",
      "Epoch 172/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0347 \n",
      "Epoch 173/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9809 - loss: 0.0560 \n",
      "Epoch 174/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9783 - loss: 0.0524 \n",
      "Epoch 175/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0434 \n",
      "Epoch 176/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9800 - loss: 0.0646 \n",
      "Epoch 177/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0325 \n",
      "Epoch 178/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0517 \n",
      "Epoch 179/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0339 \n",
      "Epoch 180/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9935 - loss: 0.0295 \n",
      "Epoch 181/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0312 \n",
      "Epoch 182/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0327 \n",
      "Epoch 183/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9820 - loss: 0.0528 \n",
      "Epoch 184/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9820 - loss: 0.0522 \n",
      "Epoch 185/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0452 \n",
      "Epoch 186/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9947 - loss: 0.0255 \n",
      "Epoch 187/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0456 \n",
      "Epoch 188/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9797 - loss: 0.0424 \n",
      "Epoch 189/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9742 - loss: 0.0486 \n",
      "Epoch 190/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9883 - loss: 0.0328 \n",
      "Epoch 191/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9809 - loss: 0.0393 \n",
      "Epoch 192/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9856 - loss: 0.0352 \n",
      "Epoch 193/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9820 - loss: 0.0416 \n",
      "Epoch 194/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9905 - loss: 0.0403 \n",
      "Epoch 195/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9934 - loss: 0.0296 \n",
      "Epoch 196/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9732 - loss: 0.0591 \n",
      "Epoch 197/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0453 \n",
      "Epoch 198/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9947 - loss: 0.0331 \n",
      "Epoch 199/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9900 - loss: 0.0333 \n",
      "Epoch 200/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0481 \n",
      "Epoch 201/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0321 \n",
      "Epoch 202/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9919 - loss: 0.0283 \n",
      "Epoch 203/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9820 - loss: 0.0499 \n",
      "Epoch 204/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0365 \n",
      "Epoch 205/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9932 - loss: 0.0391 \n",
      "Epoch 206/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0293 \n",
      "Epoch 207/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9838 - loss: 0.0362 \n",
      "Epoch 208/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9956 - loss: 0.0189 \n",
      "Epoch 209/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9919 - loss: 0.0272 \n",
      "Epoch 210/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9907 - loss: 0.0297 \n",
      "Epoch 211/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9783 - loss: 0.0518 \n",
      "Epoch 212/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9891 - loss: 0.0388 \n",
      "Epoch 213/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0318 \n",
      "Epoch 214/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9848 - loss: 0.0318 \n",
      "Epoch 215/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9935 - loss: 0.0269 \n",
      "Epoch 216/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9851 - loss: 0.0348 \n",
      "Epoch 217/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9861 - loss: 0.0581 \n",
      "Epoch 218/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9935 - loss: 0.0242 \n",
      "Epoch 219/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9895 - loss: 0.0406 \n",
      "Epoch 220/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9856 - loss: 0.0350 \n",
      "Epoch 221/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9888 - loss: 0.0272 \n",
      "Epoch 222/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9947 - loss: 0.0265 \n",
      "Epoch 223/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9778 - loss: 0.0424 \n",
      "Epoch 224/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9844 - loss: 0.0425 \n",
      "Epoch 225/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9966 - loss: 0.0198 \n",
      "Epoch 226/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0355 \n",
      "Epoch 227/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9905 - loss: 0.0354 \n",
      "Epoch 228/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9983 - loss: 0.0324 \n",
      "Epoch 229/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0475 \n",
      "Epoch 230/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9902 - loss: 0.0306 \n",
      "Epoch 231/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9905 - loss: 0.0360 \n",
      "Epoch 232/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9841 - loss: 0.0448 \n",
      "Epoch 233/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.9783 - loss: 0.0452 \n",
      "Epoch 234/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9836 - loss: 0.0424 \n",
      "Epoch 235/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9883 - loss: 0.0358 \n",
      "Epoch 236/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9935 - loss: 0.0265 \n",
      "Epoch 237/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9923 - loss: 0.0324 \n",
      "Epoch 238/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0307 \n",
      "Epoch 239/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9905 - loss: 0.0327 \n",
      "Epoch 240/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9973 - loss: 0.0304 \n",
      "Epoch 241/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9836 - loss: 0.0348 \n",
      "Epoch 242/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9857 - loss: 0.0497 \n",
      "Epoch 243/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9891 - loss: 0.0313 \n",
      "Epoch 244/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0429 \n",
      "Epoch 245/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9956 - loss: 0.0222 \n",
      "Epoch 246/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9868 - loss: 0.0362 \n",
      "Epoch 247/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9796 - loss: 0.0445 \n",
      "Epoch 248/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.9857 - loss: 0.0298 \n",
      "Epoch 249/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9749 - loss: 0.0551 \n",
      "Epoch 250/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9966 - loss: 0.0253 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7464a0377390>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, epochs=250, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "0.9473684210526315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      0.85      0.92        13\n",
      "           2       0.87      1.00      0.93        13\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.96      0.95      0.95        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "yy_pred = np.argmax(y_pred,axis=1)\n",
    "yy_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "print(accuracy_score(yy_test,yy_pred))\n",
    "print(classification_report(yy_test,yy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0. ,  52.5,  53.3],\n",
       "       [  0. ,   0.4,  99.8],\n",
       "       [  0. ,   0.1, 100. ],\n",
       "       [  0. ,  24.7,  77.3],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [  0. ,  99.9,   0. ],\n",
       "       [  0. ,   0.1, 100. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [  0. ,   0. , 100. ],\n",
       "       [  0. ,  96.4,   2.6],\n",
       "       [  0. ,   0.3,  99.8],\n",
       "       [  0. ,  99.9,   0. ],\n",
       "       [  0.2,  99.6,   0.1],\n",
       "       [100. ,   0.1,   0. ],\n",
       "       [  0. ,  43.8,  59.7],\n",
       "       [  0. ,  99.9,   0. ],\n",
       "       [  0. ,   0. , 100. ],\n",
       "       [  0.1,  99.9,   0. ],\n",
       "       [  0. ,  99.8,   0.1],\n",
       "       [100. ,   0.1,   0. ],\n",
       "       [  1.4,  97.8,   0. ],\n",
       "       [  0. ,   0. , 100. ],\n",
       "       [ 99.8,   0.8,   0. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [  0. ,  21.1,  79.9],\n",
       "       [100. ,   0.2,   0. ],\n",
       "       [  0. ,  99.1,   0.5],\n",
       "       [  0. ,  38.6,  64.1],\n",
       "       [  0. ,   2.6,  98.2],\n",
       "       [  0. ,  99.9,   0. ],\n",
       "       [100. ,   0.1,   0. ],\n",
       "       [100. ,   0.1,   0. ],\n",
       "       [  0. ,  99.6,   0.2],\n",
       "       [  0. ,  37.1,  62.7],\n",
       "       [  0. ,   0.1, 100. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [100. ,   0. ,   0. ],\n",
       "       [100. ,   0.1,   0. ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(100*y_pred,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Large Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "x_digits = digits['data']\n",
    "y_digits = digits['target']\n",
    "\n",
    "labeler = LabelBinarizer()\n",
    "\n",
    "yy_digits = labeler.fit_transform(y_digits)\n",
    "xx_digits = x_digits.reshape(1797,8,8,1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(xx_digits, yy_digits)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(8,8,1)))\n",
    "model.add(Conv2D(32, (3, 2), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - binary_accuracy: 0.8844 - loss: 0.4868\n",
      "Epoch 2/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9583 - loss: 0.1474 \n",
      "Epoch 3/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9787 - loss: 0.0881 \n",
      "Epoch 4/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9872 - loss: 0.0635 \n",
      "Epoch 5/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9900 - loss: 0.0495 \n",
      "Epoch 6/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9922 - loss: 0.0436 \n",
      "Epoch 7/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9947 - loss: 0.0343 \n",
      "Epoch 8/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9957 - loss: 0.0303 \n",
      "Epoch 9/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9975 - loss: 0.0245 \n",
      "Epoch 10/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9947 - loss: 0.0285 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7464a0374910>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step  \n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "yy_pred = np.argmax(y_pred,axis=1)\n",
    "yy_test = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        49\n",
      "           1       0.98      1.00      0.99        42\n",
      "           2       1.00      1.00      1.00        47\n",
      "           3       0.93      0.98      0.96        58\n",
      "           4       1.00      0.93      0.96        42\n",
      "           5       0.89      1.00      0.94        42\n",
      "           6       0.96      1.00      0.98        43\n",
      "           7       0.97      0.97      0.97        37\n",
      "           8       0.97      0.85      0.91        41\n",
      "           9       0.98      0.92      0.95        49\n",
      "\n",
      "    accuracy                           0.96       450\n",
      "   macro avg       0.97      0.96      0.96       450\n",
      "weighted avg       0.97      0.96      0.96       450\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[48,  0,  0,  0,  0,  1,  0,  0,  0,  0],\n",
       "       [ 0, 42,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 47,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 57,  0,  1,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 39,  0,  1,  1,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0, 42,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0, 43,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 36,  1,  0],\n",
       "       [ 0,  1,  0,  3,  0,  1,  1,  0, 35,  0],\n",
       "       [ 1,  0,  0,  1,  0,  2,  0,  0,  0, 45]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(yy_test,yy_pred))\n",
    "confusion_matrix(yy_test,yy_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yet Another Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces(data_home='/home/kaygun/local/data/scikit_learn_data/')\n",
    "binarizer = LabelBinarizer()\n",
    "\n",
    "y = binarizer.fit_transform(faces.target.flatten()).reshape(-1,40)\n",
    "X = faces.data.flatten().reshape(-1,4096)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 4.0289 - val_loss: 3.8790\n",
      "Epoch 2/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 3.7811 - val_loss: 3.7523\n",
      "Epoch 3/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3.7033 - val_loss: 3.7278\n",
      "Epoch 4/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3.6722 - val_loss: 3.6784\n",
      "Epoch 5/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.5953 - val_loss: 3.6561\n",
      "Epoch 6/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3.5817 - val_loss: 3.6119\n",
      "Epoch 7/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3.5182 - val_loss: 3.5883\n",
      "Epoch 8/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 3.4487 - val_loss: 3.5165\n",
      "Epoch 9/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3.3786 - val_loss: 3.4967\n",
      "Epoch 10/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3.2945 - val_loss: 3.3687\n",
      "Epoch 11/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 3.1866 - val_loss: 3.4359\n",
      "Epoch 12/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.1126 - val_loss: 3.2586\n",
      "Epoch 13/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 3.0167 - val_loss: 3.1738\n",
      "Epoch 14/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.8414 - val_loss: 3.1266\n",
      "Epoch 15/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.7633 - val_loss: 2.9646\n",
      "Epoch 16/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.6618 - val_loss: 2.9236\n",
      "Epoch 17/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.5159 - val_loss: 2.7583\n",
      "Epoch 18/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.3548 - val_loss: 2.5880\n",
      "Epoch 19/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.2019 - val_loss: 2.5003\n",
      "Epoch 20/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.0640 - val_loss: 2.3568\n",
      "Epoch 21/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.9769 - val_loss: 2.2901\n",
      "Epoch 22/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.9060 - val_loss: 2.2819\n",
      "Epoch 23/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.8511 - val_loss: 2.1037\n",
      "Epoch 24/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.6684 - val_loss: 1.9949\n",
      "Epoch 25/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.6151 - val_loss: 1.9874\n",
      "Epoch 26/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.4938 - val_loss: 2.0463\n",
      "Epoch 27/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.5489 - val_loss: 1.8303\n",
      "Epoch 28/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.3187 - val_loss: 1.7738\n",
      "Epoch 29/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.2409 - val_loss: 1.7675\n",
      "Epoch 30/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.2411 - val_loss: 1.6915\n",
      "Epoch 31/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.1194 - val_loss: 1.6959\n",
      "Epoch 32/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.1283 - val_loss: 1.5523\n",
      "Epoch 33/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.0661 - val_loss: 1.4917\n",
      "Epoch 34/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.9345 - val_loss: 1.5245\n",
      "Epoch 35/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.9631 - val_loss: 1.5592\n",
      "Epoch 36/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.9164 - val_loss: 1.4604\n",
      "Epoch 37/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.9328 - val_loss: 1.2988\n",
      "Epoch 38/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.7781 - val_loss: 1.3004\n",
      "Epoch 39/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.7414 - val_loss: 1.3479\n",
      "Epoch 40/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.7539 - val_loss: 1.2934\n",
      "Epoch 41/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.6712 - val_loss: 1.2650\n",
      "Epoch 42/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.6659 - val_loss: 1.2509\n",
      "Epoch 43/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.6401 - val_loss: 1.3060\n",
      "Epoch 44/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.6174 - val_loss: 1.2980\n",
      "Epoch 45/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.5920 - val_loss: 1.2317\n",
      "Epoch 46/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.6086 - val_loss: 1.1205\n",
      "Epoch 47/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.5938 - val_loss: 1.0841\n",
      "Epoch 48/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.5376 - val_loss: 1.1544\n",
      "Epoch 49/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.4931 - val_loss: 1.0934\n",
      "Epoch 50/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.4262 - val_loss: 1.0105\n",
      "Epoch 51/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4418 - val_loss: 1.1485\n",
      "Epoch 52/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4227 - val_loss: 1.0326\n",
      "Epoch 53/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.4376 - val_loss: 0.9992\n",
      "Epoch 54/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3680 - val_loss: 1.1063\n",
      "Epoch 55/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3764 - val_loss: 0.9660\n",
      "Epoch 56/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3193 - val_loss: 0.9631\n",
      "Epoch 57/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3447 - val_loss: 1.0020\n",
      "Epoch 58/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2952 - val_loss: 1.1421\n",
      "Epoch 59/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3058 - val_loss: 1.0009\n",
      "Epoch 60/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.3124 - val_loss: 0.9826\n",
      "Epoch 61/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2501 - val_loss: 0.9864\n",
      "Epoch 62/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3053 - val_loss: 1.1083\n",
      "Epoch 63/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2860 - val_loss: 1.1851\n",
      "Epoch 64/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2611 - val_loss: 0.9673\n",
      "Epoch 65/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2817 - val_loss: 0.9386\n",
      "Epoch 66/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2423 - val_loss: 0.9990\n",
      "Epoch 67/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2085 - val_loss: 0.9387\n",
      "Epoch 68/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2031 - val_loss: 0.9708\n",
      "Epoch 69/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2003 - val_loss: 0.9631\n",
      "Epoch 70/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1830 - val_loss: 1.0738\n",
      "Epoch 71/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1790 - val_loss: 0.9979\n",
      "Epoch 72/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1768 - val_loss: 1.0378\n",
      "Epoch 73/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2076 - val_loss: 0.9061\n",
      "Epoch 74/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1626 - val_loss: 1.1610\n",
      "Epoch 75/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1821 - val_loss: 0.9545\n",
      "Epoch 76/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1535 - val_loss: 0.9327\n",
      "Epoch 77/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1523 - val_loss: 0.9861\n",
      "Epoch 78/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1291 - val_loss: 0.9410\n",
      "Epoch 79/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1334 - val_loss: 1.0410\n",
      "Epoch 80/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1256 - val_loss: 0.9460\n",
      "Epoch 81/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1263 - val_loss: 0.9187\n",
      "Epoch 82/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1322 - val_loss: 1.1017\n",
      "Epoch 83/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1233 - val_loss: 0.8634\n",
      "Epoch 84/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1297 - val_loss: 0.9314\n",
      "Epoch 85/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1189 - val_loss: 1.1021\n",
      "Epoch 86/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1105 - val_loss: 0.9012\n",
      "Epoch 87/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1052 - val_loss: 1.0247\n",
      "Epoch 88/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0943 - val_loss: 0.9565\n",
      "Epoch 89/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0920 - val_loss: 0.9486\n",
      "Epoch 90/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0874 - val_loss: 0.9552\n",
      "Epoch 91/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0786 - val_loss: 0.9927\n",
      "Epoch 92/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0689 - val_loss: 0.9358\n",
      "Epoch 93/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0818 - val_loss: 1.0015\n",
      "Epoch 94/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0690 - val_loss: 0.8937\n",
      "Epoch 95/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0668 - val_loss: 1.0078\n",
      "Epoch 96/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0747 - val_loss: 0.9484\n",
      "Epoch 97/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0623 - val_loss: 0.9513\n",
      "Epoch 98/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0639 - val_loss: 0.9590\n",
      "Epoch 99/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0602 - val_loss: 1.0277\n",
      "Epoch 100/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0700 - val_loss: 1.0188\n",
      "Epoch 101/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0619 - val_loss: 0.9429\n",
      "Epoch 102/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0528 - val_loss: 1.0330\n",
      "Epoch 103/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0560 - val_loss: 0.9411\n",
      "Epoch 104/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0536 - val_loss: 0.9592\n",
      "Epoch 105/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0549 - val_loss: 1.0440\n",
      "Epoch 106/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0577 - val_loss: 0.8918\n",
      "Epoch 107/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0659 - val_loss: 1.2313\n",
      "Epoch 108/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0676 - val_loss: 0.8645\n",
      "Epoch 109/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0537 - val_loss: 1.0060\n",
      "Epoch 110/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0470 - val_loss: 1.0006\n",
      "Epoch 111/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0476 - val_loss: 0.9345\n",
      "Epoch 112/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0446 - val_loss: 0.9798\n",
      "Epoch 113/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0377 - val_loss: 0.9693\n",
      "Epoch 114/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0370 - val_loss: 0.9902\n",
      "Epoch 115/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0385 - val_loss: 0.9576\n",
      "Epoch 116/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0379 - val_loss: 1.0323\n",
      "Epoch 117/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0347 - val_loss: 0.9843\n",
      "Epoch 118/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0346 - val_loss: 1.0002\n",
      "Epoch 119/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0322 - val_loss: 1.0010\n",
      "Epoch 120/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0349 - val_loss: 0.9634\n",
      "Epoch 121/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0331 - val_loss: 1.0065\n",
      "Epoch 122/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0276 - val_loss: 0.9385\n",
      "Epoch 123/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0284 - val_loss: 1.0483\n",
      "Epoch 124/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0301 - val_loss: 0.9551\n",
      "Epoch 125/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0273 - val_loss: 1.0405\n",
      "Epoch 126/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0275 - val_loss: 0.9734\n",
      "Epoch 127/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0251 - val_loss: 1.0215\n",
      "Epoch 128/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0247 - val_loss: 0.9994\n",
      "Epoch 129/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0249 - val_loss: 0.9830\n",
      "Epoch 130/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0230 - val_loss: 0.9631\n",
      "Epoch 131/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0221 - val_loss: 1.0261\n",
      "Epoch 132/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0208 - val_loss: 0.9643\n",
      "Epoch 133/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0211 - val_loss: 0.9991\n",
      "Epoch 134/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0184 - val_loss: 0.9918\n",
      "Epoch 135/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0210 - val_loss: 0.9970\n",
      "Epoch 136/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0215 - val_loss: 1.0148\n",
      "Epoch 137/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0192 - val_loss: 0.9813\n",
      "Epoch 138/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0190 - val_loss: 1.0025\n",
      "Epoch 139/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0187 - val_loss: 0.9795\n",
      "Epoch 140/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0184 - val_loss: 0.9974\n",
      "Epoch 141/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0176 - val_loss: 1.0041\n",
      "Epoch 142/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0163 - val_loss: 0.9876\n",
      "Epoch 143/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0179 - val_loss: 1.0086\n",
      "Epoch 144/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0158 - val_loss: 1.0096\n",
      "Epoch 145/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0153 - val_loss: 0.9909\n",
      "Epoch 146/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0157 - val_loss: 0.9900\n",
      "Epoch 147/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0159 - val_loss: 1.0231\n",
      "Epoch 148/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0168 - val_loss: 0.9865\n",
      "Epoch 149/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0141 - val_loss: 1.0130\n",
      "Epoch 150/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0150 - val_loss: 1.0131\n",
      "Epoch 151/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0142 - val_loss: 1.0001\n",
      "Epoch 152/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0142 - val_loss: 0.9967\n",
      "Epoch 153/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0140 - val_loss: 1.0116\n",
      "Epoch 154/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0135 - val_loss: 1.0228\n",
      "Epoch 155/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0129 - val_loss: 1.0008\n",
      "Epoch 156/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0110 - val_loss: 1.0167\n",
      "Epoch 157/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0134 - val_loss: 0.9821\n",
      "Epoch 158/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0129 - val_loss: 1.0283\n",
      "Epoch 159/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0127 - val_loss: 1.0256\n",
      "Epoch 160/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0123 - val_loss: 1.0089\n",
      "Epoch 161/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0117 - val_loss: 1.0116\n",
      "Epoch 162/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0119 - val_loss: 0.9905\n",
      "Epoch 163/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0116 - val_loss: 1.0439\n",
      "Epoch 164/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0115 - val_loss: 0.9953\n",
      "Epoch 165/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0110 - val_loss: 0.9917\n",
      "Epoch 166/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0113 - val_loss: 1.0476\n",
      "Epoch 167/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0108 - val_loss: 0.9746\n",
      "Epoch 168/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0107 - val_loss: 1.0564\n",
      "Epoch 169/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0114 - val_loss: 1.0096\n",
      "Epoch 170/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0104 - val_loss: 0.9995\n",
      "Epoch 171/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0101 - val_loss: 1.0268\n",
      "Epoch 172/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0098 - val_loss: 1.0247\n",
      "Epoch 173/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0097 - val_loss: 1.0170\n",
      "Epoch 174/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0096 - val_loss: 1.0184\n",
      "Epoch 175/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0092 - val_loss: 1.0265\n",
      "Epoch 176/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0089 - val_loss: 1.0384\n",
      "Epoch 177/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0093 - val_loss: 1.0072\n",
      "Epoch 178/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0092 - val_loss: 1.0243\n",
      "Epoch 179/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0093 - val_loss: 1.0435\n",
      "Epoch 180/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0086 - val_loss: 0.9901\n",
      "Epoch 181/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0085 - val_loss: 1.0532\n",
      "Epoch 182/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0093 - val_loss: 1.0274\n",
      "Epoch 183/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0083 - val_loss: 1.0021\n",
      "Epoch 184/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0085 - val_loss: 1.0398\n",
      "Epoch 185/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0078 - val_loss: 1.0391\n",
      "Epoch 186/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0074 - val_loss: 1.0359\n",
      "Epoch 187/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0080 - val_loss: 1.0379\n",
      "Epoch 188/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0075 - val_loss: 1.0055\n",
      "Epoch 189/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0077 - val_loss: 1.0300\n",
      "Epoch 190/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0074 - val_loss: 1.0382\n",
      "Epoch 191/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0078 - val_loss: 1.0387\n",
      "Epoch 192/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0072 - val_loss: 1.0312\n",
      "Epoch 193/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0071 - val_loss: 1.0297\n",
      "Epoch 194/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0072 - val_loss: 1.0453\n",
      "Epoch 195/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0068 - val_loss: 1.0256\n",
      "Epoch 196/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0068 - val_loss: 1.0401\n",
      "Epoch 197/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0066 - val_loss: 1.0342\n",
      "Epoch 198/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0067 - val_loss: 1.0434\n",
      "Epoch 199/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0066 - val_loss: 1.0463\n",
      "Epoch 200/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0064 - val_loss: 1.0382\n",
      "Epoch 201/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0064 - val_loss: 1.0527\n",
      "Epoch 202/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0065 - val_loss: 1.0475\n",
      "Epoch 203/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0065 - val_loss: 1.0240\n",
      "Epoch 204/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0060 - val_loss: 1.0543\n",
      "Epoch 205/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0062 - val_loss: 1.0487\n",
      "Epoch 206/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0058 - val_loss: 1.0425\n",
      "Epoch 207/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0058 - val_loss: 1.0471\n",
      "Epoch 208/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0063 - val_loss: 1.0627\n",
      "Epoch 209/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0057 - val_loss: 1.0463\n",
      "Epoch 210/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0055 - val_loss: 1.0486\n",
      "Epoch 211/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0057 - val_loss: 1.0480\n",
      "Epoch 212/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0054 - val_loss: 1.0528\n",
      "Epoch 213/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0053 - val_loss: 1.0663\n",
      "Epoch 214/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0056 - val_loss: 1.0498\n",
      "Epoch 215/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0054 - val_loss: 1.0473\n",
      "Epoch 216/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0055 - val_loss: 1.0539\n",
      "Epoch 217/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0052 - val_loss: 1.0705\n",
      "Epoch 218/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0053 - val_loss: 1.0380\n",
      "Epoch 219/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0049 - val_loss: 1.0628\n",
      "Epoch 220/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0052 - val_loss: 1.0573\n",
      "Epoch 221/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0052 - val_loss: 1.0401\n",
      "Epoch 222/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0049 - val_loss: 1.0590\n",
      "Epoch 223/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0051 - val_loss: 1.0746\n",
      "Epoch 224/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0050 - val_loss: 1.0489\n",
      "Epoch 225/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0046 - val_loss: 1.0514\n",
      "Epoch 226/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0048 - val_loss: 1.0611\n",
      "Epoch 227/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0048 - val_loss: 1.0541\n",
      "Epoch 228/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0048 - val_loss: 1.0545\n",
      "Epoch 229/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0044 - val_loss: 1.0595\n",
      "Epoch 230/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0041 - val_loss: 1.0649\n",
      "Epoch 231/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0045 - val_loss: 1.0588\n",
      "Epoch 232/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0045 - val_loss: 1.0699\n",
      "Epoch 233/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0044 - val_loss: 1.0605\n",
      "Epoch 234/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0039 - val_loss: 1.0662\n",
      "Epoch 235/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0046 - val_loss: 1.0734\n",
      "Epoch 236/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0043 - val_loss: 1.0660\n",
      "Epoch 237/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0040 - val_loss: 1.0649\n",
      "Epoch 238/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0042 - val_loss: 1.0650\n",
      "Epoch 239/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0042 - val_loss: 1.0711\n",
      "Epoch 240/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0042 - val_loss: 1.0709\n",
      "Epoch 241/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0040 - val_loss: 1.0728\n",
      "Epoch 242/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0040 - val_loss: 1.0570\n",
      "Epoch 243/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0034 - val_loss: 1.0698\n",
      "Epoch 244/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0042 - val_loss: 1.0847\n",
      "Epoch 245/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0040 - val_loss: 1.0681\n",
      "Epoch 246/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0037 - val_loss: 1.0819\n",
      "Epoch 247/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0038 - val_loss: 1.0627\n",
      "Epoch 248/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0036 - val_loss: 1.0789\n",
      "Epoch 249/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0037 - val_loss: 1.0788\n",
      "Epoch 250/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0036 - val_loss: 1.0757\n",
      "Epoch 251/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0035 - val_loss: 1.0861\n",
      "Epoch 252/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0035 - val_loss: 1.0722\n",
      "Epoch 253/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0039 - val_loss: 1.0751\n",
      "Epoch 254/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0037 - val_loss: 1.0796\n",
      "Epoch 255/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0033 - val_loss: 1.0833\n",
      "Epoch 256/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0034 - val_loss: 1.0719\n",
      "Epoch 257/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0035 - val_loss: 1.0930\n",
      "Epoch 258/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0032 - val_loss: 1.0791\n",
      "Epoch 259/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0032 - val_loss: 1.0704\n",
      "Epoch 260/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0033 - val_loss: 1.0862\n",
      "Epoch 261/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0034 - val_loss: 1.0885\n",
      "Epoch 262/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0032 - val_loss: 1.0814\n",
      "Epoch 263/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0033 - val_loss: 1.0868\n",
      "Epoch 264/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0031 - val_loss: 1.0840\n",
      "Epoch 265/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0032 - val_loss: 1.0813\n",
      "Epoch 266/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0030 - val_loss: 1.0930\n",
      "Epoch 267/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0030 - val_loss: 1.0893\n",
      "Epoch 268/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0029 - val_loss: 1.0791\n",
      "Epoch 269/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0030 - val_loss: 1.0852\n",
      "Epoch 270/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0030 - val_loss: 1.0882\n",
      "Epoch 271/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0031 - val_loss: 1.0897\n",
      "Epoch 272/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0028 - val_loss: 1.0915\n",
      "Epoch 273/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0028 - val_loss: 1.0869\n",
      "Epoch 274/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0028 - val_loss: 1.0960\n",
      "Epoch 275/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0028 - val_loss: 1.0846\n",
      "Epoch 276/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0027 - val_loss: 1.0961\n",
      "Epoch 277/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0027 - val_loss: 1.0960\n",
      "Epoch 278/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0028 - val_loss: 1.0957\n",
      "Epoch 279/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0028 - val_loss: 1.0957\n",
      "Epoch 280/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0026 - val_loss: 1.0871\n",
      "Epoch 281/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0027 - val_loss: 1.0918\n",
      "Epoch 282/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0027 - val_loss: 1.0960\n",
      "Epoch 283/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0026 - val_loss: 1.0990\n",
      "Epoch 284/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0027 - val_loss: 1.0975\n",
      "Epoch 285/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0025 - val_loss: 1.0828\n",
      "Epoch 286/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0026 - val_loss: 1.0992\n",
      "Epoch 287/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0024 - val_loss: 1.1158\n",
      "Epoch 288/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0026 - val_loss: 1.0849\n",
      "Epoch 289/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0024 - val_loss: 1.0856\n",
      "Epoch 290/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0026 - val_loss: 1.0931\n",
      "Epoch 291/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0025 - val_loss: 1.1079\n",
      "Epoch 292/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0025 - val_loss: 1.0954\n",
      "Epoch 293/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0024 - val_loss: 1.0896\n",
      "Epoch 294/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0025 - val_loss: 1.1048\n",
      "Epoch 295/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0024 - val_loss: 1.1053\n",
      "Epoch 296/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0025 - val_loss: 1.1065\n",
      "Epoch 297/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0022 - val_loss: 1.0974\n",
      "Epoch 298/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0024 - val_loss: 1.1083\n",
      "Epoch 299/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0023 - val_loss: 1.0976\n",
      "Epoch 300/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0024 - val_loss: 1.1058\n",
      "Epoch 301/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0022 - val_loss: 1.1054\n",
      "Epoch 302/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0022 - val_loss: 1.0983\n",
      "Epoch 303/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0025 - val_loss: 1.1079\n",
      "Epoch 304/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0022 - val_loss: 1.1077\n",
      "Epoch 305/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0021 - val_loss: 1.0934\n",
      "Epoch 306/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0023 - val_loss: 1.1015\n",
      "Epoch 307/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0022 - val_loss: 1.1084\n",
      "Epoch 308/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0021 - val_loss: 1.1124\n",
      "Epoch 309/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 1.1077\n",
      "Epoch 310/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0022 - val_loss: 1.1058\n",
      "Epoch 311/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0019 - val_loss: 1.1155\n",
      "Epoch 312/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0022 - val_loss: 1.1115\n",
      "Epoch 313/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0020 - val_loss: 1.1155\n",
      "Epoch 314/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0020 - val_loss: 1.0969\n",
      "Epoch 315/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0021 - val_loss: 1.1091\n",
      "Epoch 316/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 1.1251\n",
      "Epoch 317/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0020 - val_loss: 1.1094\n",
      "Epoch 318/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0021 - val_loss: 1.1072\n",
      "Epoch 319/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0021 - val_loss: 1.1099\n",
      "Epoch 320/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0020 - val_loss: 1.1099\n",
      "Epoch 321/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0019 - val_loss: 1.1109\n",
      "Epoch 322/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0018 - val_loss: 1.1135\n",
      "Epoch 323/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0018 - val_loss: 1.1090\n",
      "Epoch 324/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0018 - val_loss: 1.1124\n",
      "Epoch 325/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0017 - val_loss: 1.1217\n",
      "Epoch 326/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0017 - val_loss: 1.1205\n",
      "Epoch 327/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0018 - val_loss: 1.1119\n",
      "Epoch 328/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0019 - val_loss: 1.1177\n",
      "Epoch 329/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0018 - val_loss: 1.1191\n",
      "Epoch 330/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0018 - val_loss: 1.1151\n",
      "Epoch 331/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0017 - val_loss: 1.1202\n",
      "Epoch 332/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0017 - val_loss: 1.1112\n",
      "Epoch 333/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0017 - val_loss: 1.1229\n",
      "Epoch 334/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0017 - val_loss: 1.1288\n",
      "Epoch 335/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 1.1167\n",
      "Epoch 336/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0018 - val_loss: 1.1044\n",
      "Epoch 337/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0018 - val_loss: 1.1173\n",
      "Epoch 338/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0017 - val_loss: 1.1312\n",
      "Epoch 339/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0017 - val_loss: 1.1205\n",
      "Epoch 340/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0016 - val_loss: 1.1120\n",
      "Epoch 341/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0017 - val_loss: 1.1154\n",
      "Epoch 342/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0018 - val_loss: 1.1345\n",
      "Epoch 343/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0017 - val_loss: 1.1236\n",
      "Epoch 344/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0015 - val_loss: 1.1092\n",
      "Epoch 345/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0015 - val_loss: 1.1171\n",
      "Epoch 346/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0016 - val_loss: 1.1454\n",
      "Epoch 347/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0015 - val_loss: 1.1247\n",
      "Epoch 348/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0016 - val_loss: 1.1038\n",
      "Epoch 349/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0016 - val_loss: 1.1223\n",
      "Epoch 350/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0015 - val_loss: 1.1409\n",
      "Epoch 351/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0015 - val_loss: 1.1282\n",
      "Epoch 352/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 1.1156\n",
      "Epoch 353/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 1.1307\n",
      "Epoch 354/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0015 - val_loss: 1.1356\n",
      "Epoch 355/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0015 - val_loss: 1.1231\n",
      "Epoch 356/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0015 - val_loss: 1.1224\n",
      "Epoch 357/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0014 - val_loss: 1.1343\n",
      "Epoch 358/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0015 - val_loss: 1.1343\n",
      "Epoch 359/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0015 - val_loss: 1.1310\n",
      "Epoch 360/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0015 - val_loss: 1.1268\n",
      "Epoch 361/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0015 - val_loss: 1.1297\n",
      "Epoch 362/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0014 - val_loss: 1.1226\n",
      "Epoch 363/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0014 - val_loss: 1.1347\n",
      "Epoch 364/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0014 - val_loss: 1.1313\n",
      "Epoch 365/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0014 - val_loss: 1.1374\n",
      "Epoch 366/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0013 - val_loss: 1.1363\n",
      "Epoch 367/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0013 - val_loss: 1.1241\n",
      "Epoch 368/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0014 - val_loss: 1.1289\n",
      "Epoch 369/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0014 - val_loss: 1.1385\n",
      "Epoch 370/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0014 - val_loss: 1.1235\n",
      "Epoch 371/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0013 - val_loss: 1.1281\n",
      "Epoch 372/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0013 - val_loss: 1.1447\n",
      "Epoch 373/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 1.1373\n",
      "Epoch 374/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0014 - val_loss: 1.1329\n",
      "Epoch 375/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0014 - val_loss: 1.1287\n",
      "Epoch 376/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0013 - val_loss: 1.1311\n",
      "Epoch 377/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 1.1337\n",
      "Epoch 378/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0013 - val_loss: 1.1368\n",
      "Epoch 379/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0013 - val_loss: 1.1308\n",
      "Epoch 380/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0012 - val_loss: 1.1444\n",
      "Epoch 381/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 1.1386\n",
      "Epoch 382/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0013 - val_loss: 1.1320\n",
      "Epoch 383/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 1.1369\n",
      "Epoch 384/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0012 - val_loss: 1.1449\n",
      "Epoch 385/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0012 - val_loss: 1.1383\n",
      "Epoch 386/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0012 - val_loss: 1.1343\n",
      "Epoch 387/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0012 - val_loss: 1.1439\n",
      "Epoch 388/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0011 - val_loss: 1.1454\n",
      "Epoch 389/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0011 - val_loss: 1.1463\n",
      "Epoch 390/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0011 - val_loss: 1.1372\n",
      "Epoch 391/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0012 - val_loss: 1.1379\n",
      "Epoch 392/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0011 - val_loss: 1.1392\n",
      "Epoch 393/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0012 - val_loss: 1.1414\n",
      "Epoch 394/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0012 - val_loss: 1.1517\n",
      "Epoch 395/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0011 - val_loss: 1.1420\n",
      "Epoch 396/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0011 - val_loss: 1.1357\n",
      "Epoch 397/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0011 - val_loss: 1.1420\n",
      "Epoch 398/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0011 - val_loss: 1.1416\n",
      "Epoch 399/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0011 - val_loss: 1.1497\n",
      "Epoch 400/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0011 - val_loss: 1.1511\n",
      "Epoch 401/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0011 - val_loss: 1.1444\n",
      "Epoch 402/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0011 - val_loss: 1.1445\n",
      "Epoch 403/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0010 - val_loss: 1.1429\n",
      "Epoch 404/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0011 - val_loss: 1.1517\n",
      "Epoch 405/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0012 - val_loss: 1.1493\n",
      "Epoch 406/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0012 - val_loss: 1.1451\n",
      "Epoch 407/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0011 - val_loss: 1.1443\n",
      "Epoch 408/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0010 - val_loss: 1.1421\n",
      "Epoch 409/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0011 - val_loss: 1.1488\n",
      "Epoch 410/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0010 - val_loss: 1.1520\n",
      "Epoch 411/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0010 - val_loss: 1.1484\n",
      "Epoch 412/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.6637e-04 - val_loss: 1.1472\n",
      "Epoch 413/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0010 - val_loss: 1.1451\n",
      "Epoch 414/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0010 - val_loss: 1.1440\n",
      "Epoch 415/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 9.7271e-04 - val_loss: 1.1494\n",
      "Epoch 416/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0010 - val_loss: 1.1506\n",
      "Epoch 417/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0010 - val_loss: 1.1599\n",
      "Epoch 418/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.6157e-04 - val_loss: 1.1550\n",
      "Epoch 419/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0010 - val_loss: 1.1565\n",
      "Epoch 420/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0010 - val_loss: 1.1463\n",
      "Epoch 421/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.6319e-04 - val_loss: 1.1453\n",
      "Epoch 422/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.8691e-04 - val_loss: 1.1647\n",
      "Epoch 423/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0010 - val_loss: 1.1661\n",
      "Epoch 424/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.8144e-04 - val_loss: 1.1437\n",
      "Epoch 425/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.0811e-04 - val_loss: 1.1471\n",
      "Epoch 426/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.7089e-04 - val_loss: 1.1622\n",
      "Epoch 427/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.0050e-04 - val_loss: 1.1639\n",
      "Epoch 428/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.9482e-04 - val_loss: 1.1498\n",
      "Epoch 429/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.1459e-04 - val_loss: 1.1588\n",
      "Epoch 430/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.9272e-04 - val_loss: 1.1612\n",
      "Epoch 431/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 9.1131e-04 - val_loss: 1.1513\n",
      "Epoch 432/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.0270e-04 - val_loss: 1.1567\n",
      "Epoch 433/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.4690e-04 - val_loss: 1.1664\n",
      "Epoch 434/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.0933e-04 - val_loss: 1.1549\n",
      "Epoch 435/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.4645e-04 - val_loss: 1.1546\n",
      "Epoch 436/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.8330e-04 - val_loss: 1.1550\n",
      "Epoch 437/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.6276e-04 - val_loss: 1.1547\n",
      "Epoch 438/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.0035e-04 - val_loss: 1.1534\n",
      "Epoch 439/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.0493e-04 - val_loss: 1.1656\n",
      "Epoch 440/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.1534e-04 - val_loss: 1.1644\n",
      "Epoch 441/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.6926e-04 - val_loss: 1.1569\n",
      "Epoch 442/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.2874e-04 - val_loss: 1.1567\n",
      "Epoch 443/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.4354e-04 - val_loss: 1.1659\n",
      "Epoch 444/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.0768e-04 - val_loss: 1.1696\n",
      "Epoch 445/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.1267e-04 - val_loss: 1.1645\n",
      "Epoch 446/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.2074e-04 - val_loss: 1.1576\n",
      "Epoch 447/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.3245e-04 - val_loss: 1.1637\n",
      "Epoch 448/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.0651e-04 - val_loss: 1.1592\n",
      "Epoch 449/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.0456e-04 - val_loss: 1.1686\n",
      "Epoch 450/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.0496e-04 - val_loss: 1.1652\n",
      "Epoch 451/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.0207e-04 - val_loss: 1.1588\n",
      "Epoch 452/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.2677e-04 - val_loss: 1.1706\n",
      "Epoch 453/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.7871e-04 - val_loss: 1.1613\n",
      "Epoch 454/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.1879e-04 - val_loss: 1.1628\n",
      "Epoch 455/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.3598e-04 - val_loss: 1.1617\n",
      "Epoch 456/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.2073e-04 - val_loss: 1.1682\n",
      "Epoch 457/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.1260e-04 - val_loss: 1.1700\n",
      "Epoch 458/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 7.0902e-04 - val_loss: 1.1634\n",
      "Epoch 459/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 7.7906e-04 - val_loss: 1.1636\n",
      "Epoch 460/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.4527e-04 - val_loss: 1.1650\n",
      "Epoch 461/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.7494e-04 - val_loss: 1.1595\n",
      "Epoch 462/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.6221e-04 - val_loss: 1.1643\n",
      "Epoch 463/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.6632e-04 - val_loss: 1.1663\n",
      "Epoch 464/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.3828e-04 - val_loss: 1.1726\n",
      "Epoch 465/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.2774e-04 - val_loss: 1.1712\n",
      "Epoch 466/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.4438e-04 - val_loss: 1.1630\n",
      "Epoch 467/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.5795e-04 - val_loss: 1.1698\n",
      "Epoch 468/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.1561e-04 - val_loss: 1.1713\n",
      "Epoch 469/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.4421e-04 - val_loss: 1.1651\n",
      "Epoch 470/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.3876e-04 - val_loss: 1.1653\n",
      "Epoch 471/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.4658e-04 - val_loss: 1.1710\n",
      "Epoch 472/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.7139e-04 - val_loss: 1.1750\n",
      "Epoch 473/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.0824e-04 - val_loss: 1.1750\n",
      "Epoch 474/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.1358e-04 - val_loss: 1.1682\n",
      "Epoch 475/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.2123e-04 - val_loss: 1.1716\n",
      "Epoch 476/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.2485e-04 - val_loss: 1.1715\n",
      "Epoch 477/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.1840e-04 - val_loss: 1.1737\n",
      "Epoch 478/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.4963e-04 - val_loss: 1.1729\n",
      "Epoch 479/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.2653e-04 - val_loss: 1.1682\n",
      "Epoch 480/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.1704e-04 - val_loss: 1.1688\n",
      "Epoch 481/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.1930e-04 - val_loss: 1.1771\n",
      "Epoch 482/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.7614e-04 - val_loss: 1.1742\n",
      "Epoch 483/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.7697e-04 - val_loss: 1.1687\n",
      "Epoch 484/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.0365e-04 - val_loss: 1.1737\n",
      "Epoch 485/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5799e-04 - val_loss: 1.1759\n",
      "Epoch 486/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.6484e-04 - val_loss: 1.1794\n",
      "Epoch 487/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.1595e-04 - val_loss: 1.1696\n",
      "Epoch 488/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.8796e-04 - val_loss: 1.1708\n",
      "Epoch 489/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.4248e-04 - val_loss: 1.1741\n",
      "Epoch 490/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.6957e-04 - val_loss: 1.1817\n",
      "Epoch 491/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.3689e-04 - val_loss: 1.1800\n",
      "Epoch 492/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.6442e-04 - val_loss: 1.1729\n",
      "Epoch 493/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 6.5249e-04 - val_loss: 1.1826\n",
      "Epoch 494/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.2069e-04 - val_loss: 1.1848\n",
      "Epoch 495/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.4532e-04 - val_loss: 1.1733\n",
      "Epoch 496/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.5804e-04 - val_loss: 1.1808\n",
      "Epoch 497/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.8682e-04 - val_loss: 1.1849\n",
      "Epoch 498/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 5.9296e-04 - val_loss: 1.1865\n",
      "Epoch 499/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.2054e-04 - val_loss: 1.1776\n",
      "Epoch 500/500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.6041e-04 - val_loss: 1.1753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7464a0378e90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(4096,)))\n",
    "model.add(Dense(128, activation=\"relu\",))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(40, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=500, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           3       0.50      1.00      0.67         1\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       0.67      1.00      0.80         2\n",
      "           7       0.00      0.00      0.00         5\n",
      "           9       0.17      1.00      0.29         1\n",
      "          10       1.00      0.67      0.80         3\n",
      "          11       1.00      1.00      1.00         1\n",
      "          12       1.00      0.33      0.50         3\n",
      "          13       1.00      1.00      1.00         1\n",
      "          14       0.50      1.00      0.67         2\n",
      "          15       1.00      1.00      1.00         2\n",
      "          16       1.00      1.00      1.00         4\n",
      "          17       1.00      1.00      1.00         2\n",
      "          18       1.00      1.00      1.00         3\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       0.50      0.50      0.50         4\n",
      "          23       1.00      1.00      1.00         2\n",
      "          24       0.67      0.67      0.67         3\n",
      "          25       1.00      0.50      0.67         2\n",
      "          26       0.75      1.00      0.86         3\n",
      "          27       0.50      0.50      0.50         2\n",
      "          28       1.00      1.00      1.00         2\n",
      "          29       1.00      1.00      1.00         2\n",
      "          30       1.00      0.67      0.80         3\n",
      "          31       1.00      0.80      0.89         5\n",
      "          32       1.00      1.00      1.00         4\n",
      "          33       0.75      1.00      0.86         3\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      0.67      0.80         3\n",
      "          36       1.00      1.00      1.00         4\n",
      "          37       1.00      1.00      1.00         4\n",
      "          38       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.86      0.87      0.84       100\n",
      "weighted avg       0.86      0.84      0.83       100\n",
      "\n",
      "[[2 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 4 0 0]\n",
      " [0 0 0 ... 0 4 0]\n",
      " [0 0 0 ... 0 0 3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kaygun/.local/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "yy_pred = np.argmax(y_pred,axis=1)\n",
    "yy_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "print(classification_report(yy_test,yy_pred))\n",
    "print(confusion_matrix(yy_test,yy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
